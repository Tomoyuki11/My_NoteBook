# 生成モデル [generative model]
機械学習を生成モデル手法（GAN、VAE等）について勉強したことをまとめたノート（忘備録）です。現在執筆中<br>

## 目次 [Contents]

1. 概要
1. [変分オートエンコーダー [VAE : Variational AutoEncoder]](#VAE)
    1. [VAE のアーキテクチャ](#VAEのアーキテクチャ)
    1. [VAE の学習とKLダイバージェンス](#VAEの学習とKLダイバージェンス)
    <!--
    1. [潜在変数の空間と生成画像の分布の関係](#潜在変数の空間と生成画像の分布の関係)
    -->
1. [GAN [Generative Adversarial Networks]](#GAN)
    1. [GAN のアーキテクチャ](#GANのアーキテクチャ)
    1. [識別器の動作と損失関数](#識別器の動作と損失関数)
    1. [生成器の動作と損失関数](#生成器の動作と損失関数)
    1. [密度比推定による識別器の役割の再解釈](#密度比推定による識別器の役割の再解釈)
    1. [JSダイバージェンスによる識別器の損失関数の再解釈](#JSダイバージェンスによる識別器の損失関数の再解釈)
    1. [GANの学習の困難さ](#GANの学習の困難さ)
        1. [GANの収束性](#GANの収束性)
        1. [モード崩壊](#モード崩壊)
        1. [勾配損失問題](#勾配損失問題)
1. [DCGAN [Deep Convolutional GAN]](#DCGAN)
    1. [DCGAN のアーキテクチャ](#DCGANのアーキテクチャ)
    1. [DCGAN の適用例](#DCGANの適用例)
1. [Conditional GAN（cGAN）](#ConditionalGAN（cGAN）)
    1. [cGAN のアーキテクチャ](#cGANのアーキテクチャ)
    1. [cGAN の損失関数](#cGANの損失関数)
    1. [cGAN の適用例](#cGANの適用例)
1. [WGAN [Wasserstein GAN]](#WGAN)
    1. [JSダイバージェンスと Earth-Mover 距離の収束性と勾配消失問題](#JSダイバージェンスとEarth-Mover距離の収束性の違いと勾配消失問題)
    1. [WGAN のアーキテクチャと損失関数](#WGANのアーキテクチャと損失関数)
    1. [WGANでのその他の工夫](#WGANでのその他の工夫)
    1. [WGAN のアルゴリズム](#WGANのアルゴリズム)
    1. [WGANのその他の利点](#WGANのその他の利点)
    1. [WGAN の適用例](#WGANの適用例)
1. WGAN-gp
1. [pix2pix](#pix2pix)
    1. [pix2pix のアーキテクチャ](#pix2pixのアーキテクチャ)
    1. [pix2pix の損失関数](#pix2pixの損失関数)
    1. [pix2pix の適用例](#pix2pixの適用例)
1. ProgressiveGAN
1. [StyleGAN](#StyleGAN)
    1. [StyleGAN のアーキテクチャ](#StyleGANのアーキテクチャ)
    1. 潜在空間における entanglement（もつれ）の disentanglement （解きほぐし）の評価
1. 論文翻訳
    1. [【論文翻訳（非公開）】Generative Adversarial Networks](https://github.com/Yagami360/MachineLearning-Papers_Survey/blob/master/papers/Generative_Adversarial_Networks/GenerativeAdversarialNetworks.md)
    1. [【論文翻訳（非公開）】Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://github.com/Yagami360/MachineLearning-Papers_Survey/blob/master/papers/Unsupervised_Representation_Learning_with_Deep_Convolutional_Generative_Adversarial_Networks/DeepConvolutionalGAN.md)
    1. [【論文翻訳（非公開）】Conditional Generative Adversarial Nets](https://github.com/Yagami360/MachineLearning-Papers_Survey/blob/master/papers/Conditional_Generative_Adversarial_Nets/ConditionalGAN.md)
    1. [【論文翻訳（非公開）】Wasserstein GAN](https://github.com/Yagami360/MachineLearning-Papers_Survey/blob/master/papers/Wasserstein_GAN/WassersteinGAN.md)
    1. [【論文翻訳（非公開）】Image-to-Image Translation with Conditional Adversarial Networks](https://github.com/Yagami360/MachineLearning-Papers_Survey/blob/master/papers/Image-to-Image_Translation_with_Conditional_Adversarial_Networks/pix2pix.md)
    1. [【論文翻訳（非公開）】U-Net: Convolutional Networks for Biomedical Image Segmentation](https://github.com/Yagami360/MachineLearning-Papers_Survey/blob/master/papers/U-Net_Convolutional_Networks_for_Biomedical/UNet.md)
    1. [【論文翻訳（非公開）】A Style-Based Generator Architecture for Generative Adversarial Networks](https://github.com/Yagami360/MachineLearning-Papers_Survey/blob/master/papers/A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks/StyleGAN.md)
1. [補足事項](#補足事項)
    1. [【補足】KLダイバージェンス [Kullback-Leibler(KL) diviergence]](#KLダイバージェンス)
    1. [【補足】JSダイバージェンス [Jensen-Shannon(JS) divergence]](#JSダイバージェンス)
    1. [【補足】Earth-Mover 距離（Wassertein距離）](#EMD)
    1. [【補足】U-Net](#UNet)
        1. [U-Net のアーキテクチャ](#UNetのアーキテクチャ)
        1. U-Net の学習方法
        1. [U-Net の適用例](#UNetの適用例)
    1. [【補足】 Style Transfer における正規化手法（INとAdaIN）](#StyleTransferにおける正規化手法（INとAdaIN）)
    1. [【補足（外部リンク）】情報理論 / 情報数理](http://yagami12.hatenablog.com/entry/2017/09/17/103228)
    1. [【補足（外部リンク）】ゲーム理論](http://yagami12.hatenablog.com/entry/2018/03/02/171939)
1. [参考サイト](#参考)

---

<a id="VAE"></a>

## ■ 変分オートエンコーダー [VAE : Variational AutoEncoder]

<a id="VAEのアーキテクチャ"></a>

### ◎ VAE のアーキテクチャ
（生成モデルの分野における）オートエンコーダーとは、教師なし学習のもとで、データを表現するための特徴を獲得するために、以下のようなアーキテクチャと処理手順を持った手法である。<br>

![image](https://user-images.githubusercontent.com/25688193/56261188-05248e00-6115-11e9-9ee8-c4ebe45b67dc.png)<br>

【オートエンコーダーの処理手順】<br>
1. 入力データ（上図では手書き数字画像データ）x を、エンコーダー（ニューラルネットワーク）で潜在変数 z に変換する。<br>
	この様子は、見方を変えると入力データの符号化しているようにみなせるため、この入力データを変換するニューラルネットワークをエンコーダーと呼ぶ。<br>
    又、潜在変数 z の次元が、入力データ x より小さい場合、このエンコーダーでの処理は次元削除になっているとみなすことも出来る。<br>
1. 潜在変数 z を、デコーダー（ニューラルネットワーク）に入力し、元の画像に再変換する。<br>
    この様子は、見方を変えるとエンコーダーで符号化した潜在変数を、再度（入力データである画像に）復号化しているようにみなせるため、この潜在変数を変換するニューラルネットワークをデコーダーと呼ぶ。<br>

<br>

このようなオートエンコーダーの内、以下のようなアーキテクチャ図のように、潜在変数 z を生成する分布が、正規分布 N(0,1) であるように設定した（ z~N(0,1)  ）オートエンコーダーを、VAE [Variational AutoEncoder] という。<br>
（これに対して、通常の AutoEncoder は潜在変数 z を生成する分布を仮定していない。）<br>

![image](https://user-images.githubusercontent.com/25688193/57172796-dff78580-6e5f-11e9-91cf-a71af9ab692a.png)<br>

※ ここで、VAE におけるエンコーダーは、正規分布に従う潜在変数 z を直接出力するのではなく、潜在変数 z が従う正規分布の平均値 μ(x) と分散値 σ(x) を生成していることに注意。<br>


<a id="VAEの学習とKLダイバージェンス"></a>

### ◎ VAE の学習とKLダイバージェンス
VAE の学習の目的は、潜在変数 z から画像を生成する確率分布 p_θ (x) の最大化である。<br>
但し、確率分布のままでは扱いづらいため、その対数尤度 ![image](https://user-images.githubusercontent.com/25688193/56268166-b1716f00-612b-11e9-96f6-e219ce7af871.png) の最大化を考える。<br>

結論から述べると、この対数尤度 ![image](https://user-images.githubusercontent.com/25688193/56268166-b1716f00-612b-11e9-96f6-e219ce7af871.png) は、以下の式ように、変分下限 [ELBO : Evidence Lower BOund] なるものととKLダイバージェンスの和で表現できる。（詳細計算略）<br>
![image](https://user-images.githubusercontent.com/25688193/56268264-dd8cf000-612b-11e9-88a3-52169bd47f44.png)<br>

ここで、KLダイバージェンスの項 ![image](https://user-images.githubusercontent.com/25688193/56268329-07dead80-612c-11e9-8a6c-49949d8b5470.png) はその定義より、常に０以上の値となるが、VAE の学習の目的である、![image](https://user-images.githubusercontent.com/25688193/56268673-e03c1500-612c-11e9-8022-a4c2974fd409.png) のときは０の値となるので、対数尤度 ![image](https://user-images.githubusercontent.com/25688193/56268166-b1716f00-612b-11e9-96f6-e219ce7af871.png) を最大化したければ、変分下限 L(θ,ϕ,x) を最大化すれば良いことが分かる。<br>

次に、この変分下限 L(θ,ϕ,x) は、以下の式に変形出来る。（詳細計算略）<br>
![image](https://user-images.githubusercontent.com/25688193/56268734-0cf02c80-612d-11e9-89b8-92d02bad08f3.png)<br>

従って、変分下限 L(θ,ϕ,x)  を最大化するためには、KLダイバージェンスを最小化し、復元誤差を最大化すれば良いことが分かる。<br>

まとめると、VAE の学習の目的である対数尤度 ![image](https://user-images.githubusercontent.com/25688193/56268166-b1716f00-612b-11e9-96f6-e219ce7af871.png) の最大化したければ、変分下限 L(θ,ϕ,x) を最大化すれば良いが、<br>
そのためには、２つの確率分布 ![image](https://user-images.githubusercontent.com/25688193/56268861-550f4f00-612d-11e9-96bd-fc7fe83cc895.png) のKLダイバージェンス<br>
![image](https://user-images.githubusercontent.com/25688193/56268970-8d169200-612d-11e9-82fc-b09556914cd9.png)<br>
を最小化し、<br>
復元誤差<br>
![image](https://user-images.githubusercontent.com/25688193/56268998-a15a8f00-612d-11e9-9935-a448a70220c8.png)<br>
を最大化すれば良いこととになる。<br>


<!--
> 図を自作のものに要修正

<a id="潜在変数の空間と生成画像の分布の関係"></a>

### ◎ 潜在変数の空間と生成画像の分布の関係
以下の図は、潜在変数 z の次元を２次元として、エンコーダー側から正規分布に従って出力される潜在変数 z の値と、実際にデコーダーから出力される画像の対応関係を示した図である。<br>

![image](https://user-images.githubusercontent.com/25688193/56263478-0908de00-611e-11e9-890a-d34fbc180164.png)<br>

エンコーダーから出力される潜在変数 z の空間において、正規分布 N(0,1) の密度が大きい領域（＝中央部分）では、3,5,8,9,2 の画像などの似た形状の画像の種類が多く分布していることがわかる。反対に、1,7,0 などの似ていない形状の画像の種類は、正規分布の密度が小さい領域に分布していることが分かる。<br>

<br>

更に、以下の左下図は、潜在変数 z の次元を２次元として、潜在変数 z の値を動かした場合（赤矢印）の、デコーダー側から出力される出力画像の変化を表した図である。<br>
![image](https://user-images.githubusercontent.com/25688193/56262854-7e26e400-611b-11e9-88d6-05e974c449a6.png)<br>
先のエンコーダー側から出力される潜在変数の値の分布図（右上図）と、概ね一致していることが分かる。<br>
-->

<a id="GAN"></a>

## ■ GAN [Generative Adversarial Networks]

> VAE からの話（真の分布とモデルの分布の一致化等）の延長で、議論を展開した文章に修正すること。

GAN は、Generator（生成器）と Discriminator（識別器）という２つの機構から構成される生成モデルである。<br>

生成器は、学習用データと同じようなデータ（偽物画像）を生成する。<br>
一方、識別器は、（この学習用データと、生成器が出力した偽物画像を入力とし）、これらのデータが、学習用データから来たものであるのか、或いは、生成器から来た偽物画像であるのかを識別する。<br>

そして、この生成器と識別器の双方が、敵対的に学習していくにつれ、次第に、識別器が本物の学習用データと見分けがつかないデータを生成することが出来るようになるという流れとなる。<br>
※ このとき、完全に学習が進むと、生成器は、データが本物画像なのか偽物画像なのかを、完全に見分けがつかなくなるので、識別率が 50% となる。<br>

尚、この生成器と識別器による処理は、一種の関数とみなせるが、この表現として、一般的に、多層パーセプトロン（MLP）が用いられる。<br>
※ ニューラルネットワークを用いると、既存の最尤推定による生成モデルでは爆発的に増加する計算量を誤差逆伝搬法で解決できる。<br>


<a id="GANのアーキテクチャ"></a>

### ◎ GAN のアーキテクチャ
![image](https://user-images.githubusercontent.com/25688193/57116628-5bd6cc80-6d91-11e9-9fa6-2089ffd7bbbc.png)<br>
![image](https://user-images.githubusercontent.com/25688193/57116750-22eb2780-6d92-11e9-9644-9d56420e743d.png)<br>

<!--
![image](https://user-images.githubusercontent.com/25688193/56490862-a0f13800-6521-11e9-9e51-77b579de22db.png)<br>
![image](https://user-images.githubusercontent.com/25688193/56491160-85d2f800-6522-11e9-8d67-bf2eabb2fe91.png)<br>
-->

<a id="識別器の動作と損失関数"></a>

### ◎ 識別器の動作と損失関数
識別器の損失関数は、以下のような式で与えられる。<br>
![image](https://user-images.githubusercontent.com/25688193/57116895-0ef3f580-6d93-11e9-8b5c-0e36bf0a92e3.png)<br>
※ この式の、![image](https://user-images.githubusercontent.com/25688193/57117890-529e2d80-6d9a-11e9-8cb6-9f8a696b62a4.png) の部分は、ゲーム理論で言うところの、min-max 法（想定される最小の利益が最大になるように最適化）に相当する式になっている。（ゲームは、２プレイヤーのゼロサムゲーム）<br>

ここで、この損失関数の式は、以下のような識別器の動作の意味に対応している。<br>
① 識別器は、本物画像 x が入力されたとき、本物画像 x を（正しく）本物に判定するように ![image](https://user-images.githubusercontent.com/25688193/57117139-d8b77580-6d94-11e9-8cfd-e396d0eaa99a.png) を出力しようとする。<br>
⇒ 第１項 ![image](https://user-images.githubusercontent.com/25688193/57116786-4f9f3f00-6d92-11e9-9d03-0a2d9f2b2acb.png) の最大化に対応<br>

![image](https://user-images.githubusercontent.com/25688193/57117538-b2470980-6d97-11e9-8adf-b0ceb7eb2fac.png)<br>

② 識別器は、偽物画像 ![image](https://user-images.githubusercontent.com/25688193/57116920-36e35900-6d93-11e9-89d9-040c8a126c56.png) が入力されたとき、偽物画像 ![image](https://user-images.githubusercontent.com/25688193/57116920-36e35900-6d93-11e9-89d9-040c8a126c56.png) を（正しく）偽物に判定するように ![image](https://user-images.githubusercontent.com/25688193/57116877-e966ec00-6d92-11e9-8fe9-48cab2627791.png) を出力しようとする。<br>
⇒ 第２項 ![image](https://user-images.githubusercontent.com/25688193/57117395-a9a20380-6d96-11e9-9140-97bb8f91e868.png) の最大化に対応<br>

![image](https://user-images.githubusercontent.com/25688193/57117512-790e9980-6d97-11e9-8bda-d6c37db14c02.png)<br>

<a id="生成器の動作と損失関数"></a>

### ◎ 生成器の動作と損失関数
![image](https://user-images.githubusercontent.com/25688193/56271987-b38bfb80-6134-11e9-9053-092bc8d798db.png)<br>

> 記載中...

<a id="密度比推定による識別器の役割の再解釈"></a>

### ◎ 密度比推定による識別器の役割の再解釈
ここでは、密度比推定の観点から、GANにおける識別器が果たす役割を、理論的に再解釈する。

先の VAE では、デコーダー側での潜在変数 z から画像を生成する確率分布 p_θ (x) が、正規分布やベルヌーイ分布の形になると仮定した上で、その確率分布の対数尤度を最大化するように学習していた。<br>

一方、GANでは、このような生成器の確率分布 ![image](https://user-images.githubusercontent.com/25688193/56272299-62c8d280-6135-11e9-8467-89448e51e0d0.png) の具体的な形を仮定することなしに、暗黙的な生成モデルとして考えている。<br>
従って、VAEのときのように、確率分布の対数尤度 ![image](https://user-images.githubusercontent.com/25688193/56272169-1f6e6400-6135-11e9-8ad7-c4ff1eb0c28e.png) を直接評価できないので、モデルの分布 ![image](https://user-images.githubusercontent.com/25688193/56272299-62c8d280-6135-11e9-8467-89448e51e0d0.png) と真の分布 ![image](https://user-images.githubusercontent.com/25688193/56272229-3ca33280-6135-11e9-8682-7ab994fc852f.png) の密度比<br>
![image](https://user-images.githubusercontent.com/25688193/56272333-77a56600-6135-11e9-8e19-728221fc907b.png)<br>
でモデルの分布がどのくらい真の分布っぽいかの尤度を評価することになる。<br>

ここで、データ集合 X のうち、<br>
半分のデータがモデルの分布 ![image](https://user-images.githubusercontent.com/25688193/56272299-62c8d280-6135-11e9-8467-89448e51e0d0.png) から生成されてものと考え、そのラベルを y=0 とし、<br>
もう半分のデータが真の分布 ![image](https://user-images.githubusercontent.com/25688193/56272229-3ca33280-6135-11e9-8682-7ab994fc852f.png) から生成されてものと考え、そのラベルを y=1 とすると、<br>
モデルの分布と真の分布は、ラベルが設定された際の条件付き確率分布<br>
![image](https://user-images.githubusercontent.com/25688193/56272437-a9b6c800-6135-11e9-8d73-9695403240ea.png)<br>
で表現できる。<br>

従って、密度比の式は、<br>
![image](https://user-images.githubusercontent.com/25688193/56272551-e2ef3800-6135-11e9-85a0-36c6526f3cd0.png)<br>
つまり、観測データ x が、モデルの分布 ![image](https://user-images.githubusercontent.com/25688193/56272299-62c8d280-6135-11e9-8467-89448e51e0d0.png) から来たのか？真の分布 ![image](https://user-images.githubusercontent.com/25688193/56272229-3ca33280-6135-11e9-8682-7ab994fc852f.png) から来たのか？を予想するような**識別モデル**<br>
![image](https://user-images.githubusercontent.com/25688193/56272572-fbf7e900-6135-11e9-8825-6d826fa79631.png)<br>
を学習出来れば、密度比を推定することが出来る。<br>
この識別モデルこれはまさに、先に述べた識別器 D そのもの役割であり、従って、識別器 D は、密度比を推定しているともみなすことも出来る。<br>
※ 識別器によって、GAN における尤度推定を、NNが得意としている分類問題に置き換えている点に注目。<br>


<a id="JSダイバージェンスによる識別器の損失関数の再解釈"></a>

### ◎ JSダイバージェンスによる識別器の損失関数の再解釈
GAN の学習の目的は、モデルの分布と真の分布を近づけ ![image](https://user-images.githubusercontent.com/25688193/56332861-ea155500-61cc-11e9-8032-98293e456e86.png) とすることである。このことは、以下で見るようにモデルの分布と真の分布間のJSダイバージェンスを最小化していることに一致する。<br>

真の分布 ![image](https://user-images.githubusercontent.com/25688193/56272229-3ca33280-6135-11e9-8682-7ab994fc852f.png) とモデルの分布 ![image](https://user-images.githubusercontent.com/25688193/56272299-62c8d280-6135-11e9-8467-89448e51e0d0.png) との間のJSダイバージェンスは、<br>

![image](https://user-images.githubusercontent.com/25688193/56332894-02856f80-61cd-11e9-9b0d-1e8993fbae3e.png)<br>

ここで、上式の赤字の項 ![image](https://user-images.githubusercontent.com/25688193/56332952-3cef0c80-61cd-11e9-8cc2-78d1f05581a7.png) は、識別器の損失関数そのものであるので、損失関数を最小化することは、JSダイバージェンスを最小化することと等価であることが分かる。<br>
そして、このJSダイバージェンスを最小化することは、モデルの分布と真の分布の２つの分布を一致させることであるので、<br>
結局のところ、<br>
「GANの学習の目的であるモデルの分布と真の分布を近づけ ![image](https://user-images.githubusercontent.com/25688193/56332861-ea155500-61cc-11e9-8032-98293e456e86.png) とすること。」<br>
⇔「（真の分布とモデルの分布の）JSダイバージェンスを最小化すること。」<br>
⇔「（識別器の）損失関数を最小化すること。」<br>
の関係が成り立つことが分かる。<br>

<a id="GANの収束性"></a>

### ◎ GANの学習の困難さ
GANの学習の難しさの要因には、大きく分けて以下のような要因がある。

- 損失関数の収束性の問題
- モード崩壊
- 勾配損失問題
- 生成画像のクオリティーの評価が、損失関数から判断し難い


<a id="GANの収束性"></a>

#### ☆ GANの収束性
GANの学習の難しさの１つの要因に、損失関数の収束性の問題がある。<br>

- GANは、識別器と生成器の２人プレイヤーゼロサムゲームになっているが、このゲームの最適解は、ナッシュ均衡点になる。<br>

- ２人プレイヤーゼロサムゲームのナッシュ均衡点は、鞍点になる。<br>
    ![image](https://user-images.githubusercontent.com/25688193/56336649-213f3280-61dc-11e9-8b55-9360b88fcc82.png)<br>

- 識別器の損失関数<br>
    ![image](https://user-images.githubusercontent.com/25688193/56336686-3f0c9780-61dc-11e9-9719-49f1faf7b2d5.png)<br>
    の形状が、上図のような凸関数であれば、SGDによってナッシュ均衡点（＝鞍点）に収束されることが保証されるが、非凸関数の場合は、保証されない。（GANではニューラルネットワークにより損失関数を表現するので、関数の形状は非凸関数）<br>

- このような非凸関数に対して、SGDで最適化（＝鞍点の探査）を行っていくと、ナッシュ均衡点（＝鞍点）に行かず、振動する可能性がある。<br>
    例えば、損失関数が L(a,b)=a×b という単純な形であった場合にも、SGDで生成器と識別器を交互に最適化していくと、以下の図のように、損失関数の値が発散する。<br>
    ![image](https://user-images.githubusercontent.com/25688193/56336712-4f247700-61dc-11e9-9557-f4f661c2d747.png)<br>

<a id="モード崩壊"></a>

#### ☆ モード崩壊 [Mode collapse]
学習が不十分な識別器に対して、生成器を最適化した場合や、生成器への入力ノイズ z の潜在変数としての次元が足りたていない場合などにおいて、生成器による生成画像が、ある特定の画像（例えば、MNISTでは数字の３などの特定の画像）に集中してしまい、学習用データが本来持っている多様な種類の画像（例えば、MNISTでは数字の0~9の画像）を生成できなくなってしまう問題がある。<br>
GANにおいて発生するこのような問題を、モード崩壊といい、GANの学習の困難さの要因の１つになっている。<br>
※ ここでいうモード（流行）とは、最頻出値のこと。<br>

<!--
> 自作の図を要追加
![image](https://user-images.githubusercontent.com/25688193/56337604-10dd8680-61e1-11e9-8c81-fdb1090c67e2.png)<br>
-->

<a id="勾配損失問題"></a>

#### ☆ 勾配損失問題
先に見たように、学習が十分でない識別器に対して、生成器を最適化すると、モード崩壊が発生してしまう。<br>
それを防ぐために、ある生成器 G の状態に対しての識別器を完全に学習すると、今度は勾配損失問題が発生してしまう。<br>
このように、GANでは、モード崩壊と勾配損失問題が互いに反して発生してしまうというジレンマを抱えている。<br>


<a id="DCGAN"></a>

## ■ DCGAN [Deep Convolutional GAN]
DCGAN は、GAN における生成器と識別器の内部を、「”プーリング層なし”の全て畳み込み層で構成される CNN 」に置き換えたものである。<br>

CNN は、画像認識タスクにおいて、優れたモデルになっているが、DCGAN もまた、画像生成において、（通常の GAN や他の生成モデルに比べて）優れたモデルになっている。<br>


<a id="DCGANのアーキテクチャ"></a>

### ◎ DCGAN のアーキテクチャ
![image](https://user-images.githubusercontent.com/25688193/57172563-c0129280-6e5c-11e9-941a-dc29501e74a3.png)<br>

![image](https://user-images.githubusercontent.com/25688193/57172588-226b9300-6e5d-11e9-8c40-d7e348a3debe.png)<br>

上図は、DCGAN の全体のアーキテクチャ（ネットワーク構成は略式表記）と、生成器・識別器内部の詳細なネットワーク構成を示したアーキテクチャ図である。<br>
DCGAN における生成器と識別器の構造は、以下のような特徴を持つ。<br>

➀ 生成器：Generator<br>

- 通常の CNN とは異なり、プーリング処理を行わない。（プーリング層は存在しない）<br>
- deconvolution（逆畳み込み）を用いて、アップサンプリングする。<br>
    ※ deconvolution という言葉は別の他の手法で既出のため、厳密には、deconvolution ではなく fractionally-strided convolution 或いは transposed convolution である。<br>
- 活性化関数として、Relu を使用。但し、出力層の活性化関数は tanh（※通常の GAN の generator と同じ）<br>
- batch normalization で学習させる。<br>
    但し、全ての層に適用されると学習が不安定となってしまうため、generator の出力層には適用しないようにする。<br>

➁ 識別器：Discriminator<br>

- 通常の CNN とは異なり、プーリング処理を行わない。（プーリング層は存在しない）<br>
- その代わりに、ストライド幅 2 の畳み込みで、ダウンサンプリングする。<br>
- 全結合層 [fully connected layer] を取り除き、代わりに、GAP : global average pooling で置き換える。<br>
    即ち、１つの特徴マップに対し、それらの平均をとって、１つの出力ノードに対応させる。（下図）<br>
    ![image](https://user-images.githubusercontent.com/25688193/57119115-c3494800-6da2-11e9-89cb-1e6c8640683c.png)<br>

- 通常の GAN の discriminator とは異なり、全ての層の活性化関数として、<br>
    Leaky ReLU (![image](https://user-images.githubusercontent.com/25688193/57119196-436fad80-6da3-11e9-9870-7db07a0f43e6.png)) を使用する。<br>
    これは、通常の Relu では、x≦0 の領域で、勾配が 0 になるために、学習を進めることが出来なくなってしまうのに対し、Leaky Relu では、x≦0 の領域でも、勾配が 0 とはならないので、学習を進めることが出来るようになることに起因する。<br>
    ![image](https://user-images.githubusercontent.com/25688193/57119284-e3c5d200-6da3-11e9-9c5f-c304098cd6ef.png)<br>

- batch normalization で学習させる。<br>
    但し、全ての層に適用されると学習が不安定となってしまうため、discriminator の入力層には適用しないようにする。<br>


<a id="DCGANの適用例"></a>

### ◎ DCGAN の適用例

- [DCGAN を利用した手書き文字（MNIST）の自動生成](https://github.com/Yagami360/MachineLearning_Exercises_Python_PyTorch/tree/master/GAN_DCGAN_PyTorch)


<a id="ConditionalGAN（cGAN）"></a>

## ■ Conditional GAN（cGAN）
従来の GAN では、潜在変数としての入力ノイズ z の値を動かすことで、様々な種類の画像を生成することができた。<br>
しかしながら、画像の生成過程において、今生成している画像がどのような種類（例えば、MNISTでは数字の番号）の画像であるのかを、GAN自身が知ることはできないかった。<br>

Conditional GAN（cGAN）は、従来の GAN の入力に、生成する画像のクラスラベルなどの条件 y を付与することにより、GAN自身に、今どのような種類の画像を生成しているのか？といった、データの生成過程を指示することを可能にし、結果として、条件 y で指定した特定の画像のみを生成することが出来るようにする。<br>


<a id="cGANのアーキテクチャ"></a>

### ◎ cGAN のアーキテクチャ
![image](https://user-images.githubusercontent.com/25688193/57116464-42815080-6d90-11e9-8d21-db015d449e10.png)<br>

上図は、cGAN のアーキテクチャを示した図である。<br>
cGAN では、従来の GAN に対して、生成器の入力と識別器の入力の双方に、正解となるクラスラベルなど条件 y を入力している。<br>
※ このアーキテクチャ図では、代表的な条件 y として、クラスラベルとしているが、他にも様々な条件（アノテーションなど）を入力することが出来る。<br>

但し、この例では、GANのアーキテクチャに対応できるように、<br>

- 識別器へのクラスラベル y の入力は、one-hot エンコードされた画像データのフォーマット（正解画像を白画像、それ以外を黒画像）で入力し、
- 生成器へのクラスラベル y の入力は one-hot エンコードされたバイナリデータの系列のフォーマット（正解を0、それ以外を０）で入力している。

※ cGAN のアルゴリズム自体には、このようなエンコード手法などの具体的な含まれないことに注意。<br>
（従来のGANのアーキテクチャに組み込める形であれば、どのような方法でもよい。）<br>


<a id="cGANの損失関数"></a>

### ◎ cGAN の損失関数
cGAN の生成器と識別器の損失関数は、従来のGANの損失関数に対して、クラスラベルなどの追加条件 y の制約を加えた式（＝条件付き確率）で表現できる。<br>
即ち、識別器の損失関数 L_D  は、<br>
![image](https://user-images.githubusercontent.com/25688193/56723413-dd23d300-6783-11e9-86e3-50c8aaff09ea.png)<br>

生成器の損失関数 L_G  は、<br>
![image](https://user-images.githubusercontent.com/25688193/56723450-f0cf3980-6783-11e9-9362-302766cc6114.png)<br>

そして、先のアーキテクチャ図で示しているように、従来の GAN と同じく、これらの損失関数を誤差逆伝搬法で逆伝搬することで、生成器と識別器の学習を逐次行う。<br>


<a id="cGANの適用例"></a>

### ◎ cGAN の適用例

- [cGAN を利用した手書き文字（MNIST）の自動生成](https://github.com/Yagami360/MachineLearning_Exercises_Python_PyTorch/tree/master/GAN_cGAN_PyTorch)


<a id="WGAN"></a>

## ■ WGAN [Wasserstein GAN]
既存のGANにおける（識別器の）学習は、先で見たように、JSダイバージェンスを最小化していることと同値であった。<br>
しかしながら、このJSダイバージェンスの最小化に基づく学習では、以下のような問題点が存在する。（詳細は後述）<br>

- 真の分布 ![image](https://user-images.githubusercontent.com/25688193/56492380-9a18f400-6526-11e9-8d35-7687a906526f.png) とモデルの分布 ![image](https://user-images.githubusercontent.com/25688193/56492448-d0ef0a00-6526-11e9-8c9b-79fb36a75af0.png) の台（Supp）が重ならない場合において、勾配損失問題が発生する。<br>

- 学習回数に対する loss 値の変化の様子が、生成画像のクオリティに比例しない。（＝生成画像のクオリティが損失関数の値から判断しずらい）<br>

- モード崩壊が発生することがある。

- 損失関数の収束性に問題があり、学習が不安定

そこで、Wasserein GAN では、JSダイバージェンスではなく、Earth-Mover 距離（EMD）（＝Wassertein距離ともいう）と呼ばれる別の距離指標を使用する。
（※厳密には、このWassertein距離の双対表現の式を近似した式）<br>
これにより、既存の GAN（＝JSダイバージェンス最小化）で発生する上記の問題を回避することが出来る。<br>


<a id="JSダイバージェンスとEarth-Mover距離の収束性の違いと勾配消失問題"></a>

### ◎ JSダイバージェンスとEarth-Mover距離の収束性の違いと勾配消失問題
概略で挙げたように、従来のGANにおける問題点の１つは、<br>

- 真の分布とモデルの分布の台が重ならない場合において、勾配消失問題が発生する。<br>

という問題である。<br>
ここでは、この問題点の詳細と、WGANにおける解決策を見ていく。<br>

![image](https://user-images.githubusercontent.com/25688193/56465595-60b78a00-643b-11e9-841d-e4e704f29b57.png)<br>

GANでの学習の目的は、真の分布 ![image](https://user-images.githubusercontent.com/25688193/56492380-9a18f400-6526-11e9-8d35-7687a906526f.png) とモデルの分布 ![image](https://user-images.githubusercontent.com/25688193/56492448-d0ef0a00-6526-11e9-8c9b-79fb36a75af0.png) の２つの確率分布を互いに一致させることであるが、この２つの確率分布の台（Suup）が、上図のように互いに重ならない場合を考える。<br>
※ θ は、生成器のニューラルネットワークの重みパラメーター<br>

このような２つの確率分布の台が交わらない極端なケースの例として、以下のような図のようなケースで、JSダイバージェンスを計算する。<br>

![image](https://user-images.githubusercontent.com/25688193/56465614-f18e6580-643b-11e9-9ab3-5d67522c4e75.png)<br>

![image](https://user-images.githubusercontent.com/25688193/56465620-04089f00-643c-11e9-8eeb-be4c269fd7ce.png)<br>

この計算結果を図示すると、以下のような図となる。<br>

![image](https://user-images.githubusercontent.com/25688193/56465640-4c27c180-643c-11e9-8103-8dcc5f8afaba.png)<br>

真の分布とモデルの分布が重なる θ=0 の部分で、JSダイバージェンスの値が不連続となっている。一方、真の分布とモデルの分布が重ならない θ≠0 の部分では、JSダイバージェンスの値が一定値となっていることが分かる。<br>
※ この様子を確率分布の系列 ![image](https://user-images.githubusercontent.com/25688193/56465651-842f0480-643c-11e9-97f1-4947a7f7399a.png) で考えると、この系列は、JSダイバージェンスのもとで、θ_t→0 で収束していないとも言える。<br>

ここで、GANの目的は、真の分布とモデルの分布を一致させるでことあり、これをJSダイバージェンスの最小化で行っていた。<br>
しかしながら、上図では、学習の余地がある、真の分布とモデルの分布が重ならない θ≠0 の部分において、JSダイバージェンスの値が一定値となっているために、その勾配の値が０になっていまうために、結果として、JSダイバージェンスを最小化して、誤差逆伝搬で学習するのに必要な勾配値が得られないという、いわゆる勾配消失問題が発生していることが分かる。<br>

このように、真の分布とモデルの分布の台（Supp）が互いに重ならない場合において、従来のGANのJSダイバージェンスの最小化による学習では、勾配消失問題が発生してしまう。<br>

<br>

Wassertein GAN では、この勾配消失問題を回避するために、Earth-Mover距離（Wassertein距離）を使用するが、このEarth-Mover距離（Wassertein距離）は、以下のように定義される。<br>

![image](https://user-images.githubusercontent.com/25688193/56477963-17704480-64e6-11e9-9396-a72c7cae56af.png)<br>

このEarth-Mover距離を、先のJSダイバージェンスと同様にして、２つの確率分布の台が交わらない極端なケースで計算すると、<br>
![image](https://user-images.githubusercontent.com/25688193/56480315-7b016e80-64f4-11e9-860e-fd3963c90c7a.png)<br>
となる。<br>
この計算結果を図示すると、以下のような図となる。<br>
![image](https://user-images.githubusercontent.com/25688193/56480337-8eacd500-64f4-11e9-908a-cde46f6e8743.png)<br>

真の分布とモデルの分布が重なる θ=0 の部分で、EMDの値が０なっている。
一方、真の分布とモデルの分布が重ならない θ≠0 の部分では、EMDの勾配が一定値となっていることが分かる。<br>
つまり、学習が完了していない、真の分布とモデルの分布が重ならない領域において、誤差逆伝搬での学習に必要な、EMDの勾配値が得られているので、勾配消失問題は発生していないことが分かる。<br>


<a id="WGANのアーキテクチャと損失関数"></a>

### ◎  WGANのアーキテクチャと損失関数
先でみたように、Wassertein GAN では、以下のように定義される、Earth-Mover距離（Wassertein距離）を使用する。<br>

![image](https://user-images.githubusercontent.com/25688193/56480424-f82ce380-64f4-11e9-9952-d4f72fbd4aaa.png)<br>

但し、この式では、GANで扱うような画像の次元が大きい場合に、現実的に計算可能ではないので、この式の双対表現の式を利用する。<br>
※ このEarth-Mover距離を計算することは、線形計画法を解くことに一致するので、元の式（主問題）に対する双対表現が得られる。<br>

即ち、<br>
![image](https://user-images.githubusercontent.com/25688193/56480447-11359480-64f5-11e9-9ff8-71541144ff8d.png)<br>

ここで、関数 ![image](https://user-images.githubusercontent.com/25688193/56480469-30342680-64f5-11e9-8519-a93d37cfc1ba.png) は、K-リプシッツ連続な関数であることが、元のEarth-Mover距離の制約条件から要請される。<br>

![image](https://user-images.githubusercontent.com/25688193/56483115-c28ef700-6502-11e9-9d55-67132cc24464.png)<br>

また、このリプシッツ連続性の条件は、先の２つの確率分布の台が互いに重ならない場合で発生する勾配消失問題でみたように、一定の勾配が得られて学習がうまくいくための要件になっているとみなすことも出来る。（上図参照）<br>

![image](https://user-images.githubusercontent.com/25688193/57116586-10242300-6d91-11e9-9d96-d2e4683c6c6d.png)<br>

WGANでは、上図のアーキテクチャ図のように、このリプシッツ連続な関数 f を、
ニューラルネットワークで構成されたクリティック（＝従来の識別器）で表現する。<br>
（※ このクリティックは、従来のGANの識別器のように、入力データが本物か偽物かの 0 or 1 の２値を sigmoid 活性化関数で活性出力するのではなく、リプシッツ連続な関数 f の出力（＝連続値になる）をそのまま出力する。）<br>

そのため、もはや従来の識別器とは異なる機能となっているため、識別器ではなくクリティックという。<br>

<br>

このとき、クリティックのニューラルネットワークの重みパラメーターを w とし、この関数 f をパラメーター付きの関数![image](https://user-images.githubusercontent.com/25688193/56492322-5de59380-6526-11e9-87de-75ae8e5916db.png)で表記すると、先の Earth-Mover 距離の双対表現の式は、以下の式で近似できる。（導出略）<br>

![image](https://user-images.githubusercontent.com/25688193/56483339-4d242600-6504-11e9-9523-f94d78cbe79f.png)<br>

そしてWGANでは、クリティックの損失関数 ![image](https://user-images.githubusercontent.com/25688193/56492511-fd0a8b00-6526-11e9-8ed3-38e305f10c56.png) として、このEarth-Mover 距離の双対表現の近似式で採用する。<br>
そして、上記のアーキテクチャ図で示したように、この勾配を誤差逆伝搬することで、学習を行う。<br>


<a id="WGANでのその他の工夫"></a>

### ◎ WGAN でのその他の工夫

- クリティックの学習と生成器の学習の更新間隔：<br>
    従来のGANでは、損失関数（＝JSダイバージェンス）が途中で勾配消失するために、ある生成器の出力に対して、最適途中までしか識別器を学習させることしかできず、そのために、<br>
    「識別器の損失関数を更新（１回）→生成器の損失関数の更新（１回）」 を、各々１回ずつ繰り返すことで、生成器と識別器の学習を逐次行っていた。<br>

    一方、先に述べたように、WGANの損失関数（＝EMD）では、ほとんどいたるところで連続で微分可能あり、誤差逆伝搬での学習のための勾配が得られ続けるので、ある生成器の出力に対して、クリティックの最適状態まで、学習を続けることが可能である。<br>

    従って、WGANでは、正しい損失関数を計算するために<br>
    「識別器の損失関数を更新（n_critic=5 回）→生成器の損失関数の更新（１回）」というように繰り返すことで、生成器と識別器の学習を逐次行なう。<br>
    これにより、安定した学習や、モード崩壊の防止を実現できる。<br>

- リプシッツ連続な関数 f の実現と重みクリッピング：<br>
    ここで、何度も述べているように、クリティックの出力となる関数![image](https://user-images.githubusercontent.com/25688193/56492322-5de59380-6526-11e9-87de-75ae8e5916db.png)は、∀w∈W に対して、リプシッツ連続な関数である必要があるが、WGANでは、このことをニューラルネットワークで実現するために、単純に、−c≤w≤c (例えば c=0.01)  の範囲で重みクリッピングを行うことで実現する。<br>

    ※ 但し、この重みクリッピングによる方法では、勾配爆発や、低い値にクリッピングしすぎることによる勾配消失の問題が発生する可能性がある。<br>
    ※ 後述の WGAN-gp では、このような問題が起らないように、重みクリッピングによるリプシッツ連続性の実現ではなく、勾配ノルムに制限項を加えることで、リプシッツ連続性を実現する。（詳細は後述）

- 最適化アルゴリズムの選択：<br>
	最適化アルゴリズムとして、Adam のようなモーメンタムベースの最適化アルゴリズムを使用すると、ときどき学習が不安定になるという実験的結果がある。（※ この理論的根拠はまだない？）v
	そのため論文では、最適化アルゴリズムとして、モーメンタムベースのアルゴリズムではない RMSProp を採用している。<br>


<a id="WGANのアルゴリズム"></a>

### ◎ WGAN のアルゴリズム
以上のことを踏まえて、最終的に WGAN のアルゴリズムは、以下のようになる。<br>

![image](https://user-images.githubusercontent.com/25688193/56484127-f40ac100-6508-11e9-92c7-e1581e138ae0.png)<br>


<a id="WGANのその他の利点"></a>

### ◎ WGAN のその他の利点
先に述べたように、WGANでは、従来の GAN で問題になっていた、（真の分布とモデルの分布の台が互いに重ならない場合に発生する）勾配消失問題を回避することが出来るが、その他にも以下に上げるようないくつかの利点が存在する。<br>

※ 勾配消失問題の回避を含めたこれらの利点の原因は、互いに独立したものではなく、いづれもWGANの損失関数（＝EMDの近似）の連続性やクリティックの出力のリプシッツ連続性に起因していることに注目。<br>

- 学習が安定している：<br>
	先に述べたように、WGANの損失関数（＝EMD）では、ほとんどいたるところで連続で微分可能あり、誤差逆伝搬での学習のための勾配が得られ続けるので、ある生成器の出力に対して、クリティックの最適状態まで、学習を続けることが可能である。<br>
	従って、WGANでは、正しい損失関数を計算するために、<br>
	「識別器の損失関数を更新（n_critic  回）→生成器の損失関数の更新（１回）」　というように繰り返すことで、生成器と識別器の学習を逐次行うことが可能となり、<br>
	従来のGANでの、「識別器の損失関数を更新（１回）→生成器の損失関数の更新（１回）」 での方式より、学習が安定化する。<br>

- モード崩壊が発生しない：<br>
	モード崩壊は、十分に最適化されたいない識別器に対して、生成器を最適化し、画像を生成することで発生する。<br>
	先に述べたように、WGANの損失関数（＝EMDの近似）では、ほとんどいたるところで連続で微分可能あり、誤差逆伝搬での学習のための勾配が得られ続けるので、ある生成器の出力に対して、クリティック（＝識別器）の最適状態まで、学習を続けることが可能である。<br>
	従って、モード崩壊の発生原因である、十分に最適化されたいないクリティック（＝識別器）という状況が発生しないため、モード崩壊も発生しない。<br>

- 生成画像のクオリティを loss値から判断できる：<br>
    従来のGANNいおいては、学習の経過において、loss 値が減少したからといって、必ずしも生成画像のクオリティが良くなるとは限らない。<br>
	そのため、良いクオリティの生成画像を得るために、学習をいつ止めるべきなのかを目視していなくてはいけないとう問題が存在した。<br>
    <br>
    一方 WGAN では、学習回数に対する loss 値の変化の様子が、生成画像のクオリティに比例することが、実験的に示されている。（理論的には？）<br>
    そのため、良いクオリティの生成画像を得るために、生成画像が都度目視せずとも、学習の完了（＝loss値の０付近への収束）まで待つだけでよい。<br>
    このことを示したのが、以下の図である。<br>
    ※ この性質は、理論的には、クリティックの出力が、リプシッツ連続で、ほとんどいたるところで線形な関数で表現できていることに起因？

    ![image](https://user-images.githubusercontent.com/25688193/56484634-84e29c00-650b-11e9-8174-9c5167e1010c.png)<br>


<a id="WGANの適用例"></a>

### ◎ WGAN の適用例

- [WGAN を利用した手書き文字（MNIST）の自動生成](https://github.com/Yagami360/MachineLearning_Exercises_Python_PyTorch/tree/master/GAN_WGAN_PyTorch)


<a id="pix2pix"></a>

## ■ pix2pix
従来の image-to-image 変換タスク（＝１対１に対応している画像間を変換すること）では、タスクの種類に応じて個別にアルゴリズムを構築していた。<br>
pix2pix は、image-to-image 変換タスクを、cGAN を用いて実現することにより、様々な種類の image-to-image 変換タスクを、汎用的なフレームワークで実現出来る。<br>
※ 通常の GAN では、生成タスクが主な用途であるが、pix2pix はこのような image-to-image の変換タスクが主な用途である。<br>

具体的には、以下の例のような、image-to-image 変換タスクを共通のフレームワークで実現できる。<br>

- 航空写真から地図を生成<br>
    ![image](https://user-images.githubusercontent.com/25688193/57005997-df6cae00-6c17-11e9-88c7-e664288a75a8.png)<br>

- 輪郭線を画像で埋める<br>
    ![image](https://user-images.githubusercontent.com/25688193/57005999-e7c4e900-6c17-11e9-95f0-2d5d362318d6.png)<br>

- 画像をカラー化する<br>
    ![image](https://user-images.githubusercontent.com/25688193/57006008-fa3f2280-6c17-11e9-906c-f6ba9fe55d17.png)<br>

- 道路や建物の塗り分けから画像生成<br>
    ![image](https://user-images.githubusercontent.com/25688193/57006021-1a6ee180-6c18-11e9-805f-a4fe8ee47473.png)<br>


<a id="pix2pixのアーキテクチャ"></a>

### ◎ pix2pix のアーキテクチャ
![image](https://user-images.githubusercontent.com/25688193/57006356-11cbda80-6c1b-11e9-9f92-98c5ee2b175c.png)<br>

上図は、例えば、航空写真を地図に変換する image-to-image 変換タスクにおける、pix2pix のアーキテクチャ図を示した図である。<br>
この pix2pix のアーキテクチャには、主に、以下のような特徴がある。<br>

- アーキテクチャのベースとしては、cGAN を採用：<br>
    cGAN のアルゴリズムにおいて、入力画像 x を変換前の画像（航空写真）とし、追加条件 y として、変換後の画像（地図画像）とすることで、cGAN での image-to-image 変換タスクを実現している。<br>

- 生成器 G のネットワークとしては、U-Net を採用：<br>
    - U-Net は、セマンティックセグメンテーションのタスクにおいて重要となる「局所的な特徴量と、画像全体の特徴の両方を捉えること」 を同時に実現できる。
    - より詳細には、Encoder 側の浅い層から、画像全体の大域的な特徴量を skip connection 経由で、Decoder 側に送り、Encoder 側の深い層からの、画像の局所的な特徴量を skip connection 経由で、Decoder 側に送る。<br>
        そして、Decoder 側で、これら skip connection で送られてきた大域的特徴量と局所的特徴量を保持したたま、アップサンプリングを行い、変換前と同じ解像度の画像を出力する。<br>
        結果として、「局所的な特徴量と、画像全体の特徴の両方を捉えること」 を同時に実現できる。（詳細は、[【補足】U-Net](https://github.com/Yagami360/My_NoteBook/blob/master/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6_%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92_%E7%94%9F%E6%88%90%E3%83%A2%E3%83%87%E3%83%AB.md#UNet) の項目参照）
    - pix2pix では、生成器のネットワーク構成として、U-Net を採用することで、このセマンティックセグメンテーションのタスクにおいて重要となる「局所的な特徴量と、画像全体の特徴の両方を捉えること」 を同時に実現できるようにしている。<br>

- 識別器 D には、Patch GAN を採用：<br>
    - Patch GAN では、識別器に入力された画像をより小さな（＝局所的な）複数の小領域（＝パッチ）に分解した上で、これら各バッチに対して、本物か偽物かの判定を行い、最後に、全ての応答を平均化して、識別器の最終的な出力とする。<br>
    - この Patch GAN の仕組みにより、識別器がある程度大域的な判定を残しつつ、局所的な特徴量でのみ判定に専念できるので、学習パラメーター数を大幅に減らすことができ、結果として、学習を効率的に進めることができる。<br>
    - 尚、Patch GAN という名前がついているが、この PatchGAN 自体は、GAN 全体のアルゴリズムではなく、単に、識別器のみに対してのアルゴリズムであることに注意。<br>

- 生成器 G に入力する入力ノイズ z は、dropout で実現：<br>
    - 生成器 G に入力する入力ノイズ z は、従来の GAN のように、確率分布 U(0,1)  or N(0,1)  から直接サンプリングして実現するのではなく、生成器のネットワークの複数の中間層に、直接 dropout を施すという意味でのノイズとして実現する。<br>

このようなアーキテクチャのもとで、識別器 D は、入力されたデータのペアが、学習データに含まれている｛変換前の画像（本物の航空写真）, 変換後の画像（地図）｝というペアなのか、或いは、｛ 変換元画像から生成器が生成した画像（偽物の航空写真）、変換後の画像（地図）＞ のペアなのかという判断を行う。<br>

そして、従来の GAN と同じように、この識別器による判定結果が、50% - 50% になるように、生成器と識別器の学習を、交互に実施していく。


<a id="pix2pixの損失関数"></a>

### ◎ pix2pix の損失関数
pix2pix は、cGAN をベースに構築されているが、cGAN の識別器の損失関数 L_D と生成器の損失関数 L_G は、以下のように定義された。<br>

![image](https://user-images.githubusercontent.com/25688193/57006110-0d062700-6c19-11e9-908c-39d457fe628a.png)<br>

pix2pix でのアーキテクチャにそうように書き換えると、<br>

![image](https://user-images.githubusercontent.com/25688193/57006122-21e2ba80-6c19-11e9-9709-e40b49281733.png)<br>

pix2pix では更に、生成器での損失関数 L_G に対して、この cGAN の損失関数をベースにして、L1正則化の項<br>

![image](https://user-images.githubusercontent.com/25688193/57006135-42ab1000-6c19-11e9-9092-821fde5aca8a.png)<br>
を追加する。<br>
これにより、生成器での損失関数は、以下のようになる。<br>

![image](https://user-images.githubusercontent.com/25688193/57010909-1784e880-6c3a-11e9-9403-08c3ac3490f8.png)<br>

この L1正則化の項目は、変換後の本物画像 x と生成器が生成した偽物画像 ![image](https://user-images.githubusercontent.com/25688193/57006148-6d956400-6c19-11e9-9af3-d6860b30f3dc.png) が、”ピクセル単位” でどの程度異なるのか（＝局所的な特徴量）を表している。<br>
セマンティックセグメンテーションのタスクにおいては、「局所的な特徴量と、画像全体の特徴の両方を捉えること」 が重要となる。<br>
pix2pix では、既存の cGAN の損失関数に、L1正則化項を追加することにより、
局所的な特徴量をL1正則化で捉え、全体的な情報の正しさを識別器で捉え判定することを可能にしている。<br>
※ 同じような理由で、L2正則化も考えられるが、L2よりL1のほうが、生成画像のぼやけが少ない傾向があるので、pix2pix では、L1を採用している。<br>


<a id="pix2pixの適用例"></a>

### ◎ pix2pix の適用例

- [pix2pix を利用して、衛星画像から地図を生成する。](https://github.com/Yagami360/MachineLearning_Exercises_Python_PyTorch/tree/master/Pix2Pix_PyTorch)


<a id="StyleGAN"></a>

## ■ StyleGAN
StyleGAN は、高解像度の画像を生成可能な ProgressiveGAN をベースラインとして、画像の画風変換（Style Transfer）の分野の知見を取り入れたアルゴリズムである。<br>

この StyleGAN のアーキテクチャは、従来の GAN の生成器とは、かなり異なるアーキテクチャになっているが、非常にクオリティの高い画像を、高解像度で生成出来る。<br>
又、単に高解像の画像を生成出来るだけでなく、人物画像の高レベルで大域的な属性（顔の輪郭、眼鏡の有無など）から局所的な属性（しわや肌質など）までを、アルゴリズムで制御出来るようになっている。<br>

<a id="StyleGANのアーキテクチャ"></a>

### ◎ StyleGAN のアーキテクチャ
![image](https://user-images.githubusercontent.com/25688193/57349935-00dd1500-7197-11e9-9e03-08d12a33febf.png)<br>

上図は、StyleGAN の全体のアーキテクチャ（上段）と、生成器内部の詳細なアーキテクチャ（下段）を示した図である。<br>
この StyleGAN のアーキテクチャには、主に、以下のような特徴がある。<br>

- Style-Based Generator：<br>
    StyleGAN の生成器（Style-Based Generator）は、ProgressiveGAN の生成器をベースラインとして、画風変換（Style Transfer）の分野の知見を取り入れた、従来の GAN の生成器とは、かなり異なるアーキテクチャになっている。<br>
    具体的には、以下のような構造や特徴をもつ。（詳細は後述）<br>
    - Mapping network f
    - Adaptive Instance Normalization (AdaIN)
    - pixel-wise Noise Input と Stochastic variation
    - ProgressiveGAN から入力層を排除
    - Style Mixing

- Mapping network f（潜在空間 Z から潜在空間 W への写像と潜在表現の獲得）：<br>
    従来の GAN では、入力ノイズ z∈Z（Z:潜在空間）を、そのまま生成器に入力していた。
    しかしながら、この方法では、下図で示すように、画像の特徴量の一部の組み合わせが存在しないようなケースにおいて、入力ノイズ z が存在する潜在空間 Z は、線形ではなく entanglement（もつれ）のある歪んだ空間となる。<br>
    ![image](https://user-images.githubusercontent.com/25688193/57284435-1fd29d00-70ec-11e9-9fdd-a818745d4fa4.png)<br>

    そのため、下図のように、潜在空間 Z のある端点（１）から別の端点（９）までの線形補間を実施した場合に、途中の潜在空間での値 z で生される生成画像に、非線形な変化（４，５，６）が生じてしまう。<br>
    ![image](https://user-images.githubusercontent.com/25688193/57284523-51e3ff00-70ec-11e9-813d-e2d17b43e28a.png)<br>

    これに対して、StyleGAN では、この潜在変数である入力ノイズ z をまず、mapping network f という８層のMLPから構成されるネットワークに入力し、中間的な潜在空間 W へと写像して、別の潜在変数 w∈W を獲得した上で、後段のネットワークに入力する。<br>
    ここで、mapping network は、学習されるネットワークであるので、これにより、この mapping network で写像された潜在変数 w は、下図で示すように、画像の特徴量の entanglement（もつれ）の少ない状態で最適化されていることが期待できる。<br>
    ![image](https://user-images.githubusercontent.com/25688193/57284625-8061da00-70ec-11e9-9cd9-780f2ebbeb6e.png)<br>

    尚、StyleGAN では、この mapping network f による潜在空間の entanglement（もつれ）の disentanglement （解きほぐし）の程度を、定量的に評価する指標として、新しい２つの指標（Perceptual path length と Linear separability）を提案している。（詳細は後述）<br>

    > ※ disentanglement と潜在表現の獲得：<br>
    > 直訳すると、（複雑に絡み合った問題の）”解きほぐし” の意味であるが、ここでの意味は、データのもつれを解く最適な表現方法を獲得する（disentanglement）こと。

- 各解像度スケールでの Adaptive Instance Normalization (AdaIN) の採用：<br>
    AdaIN は、Style Transfer の文脈では、Instance Normalization　の発展版で、1つのモデルであらゆるスタイル（画風）への変換を可能にした正規化手法である。<br>
    ※ 詳細は、「[【補足】 Style Transfer における正規化手法（INとAdaIN）](https://github.com/Yagami360/My_NoteBook/blob/master/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6_%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92_%E7%94%9F%E6%88%90%E3%83%A2%E3%83%87%E3%83%AB.md#StyleTransfer%E3%81%AB%E3%81%8A%E3%81%91%E3%82%8B%E6%AD%A3%E8%A6%8F%E5%8C%96%E6%89%8B%E6%B3%95%EF%BC%88IN%E3%81%A8AdaIN%EF%BC%89)」 の項目を参照）<br>

    ![image](https://user-images.githubusercontent.com/25688193/57350788-67affd80-719a-11e9-8c32-60bca8dcc128.png)<br>

    StyleGAN において、この AdaIn は、上図のように、Mapping Network によって得られた中間潜在変数 w に対して、アフィン変換（平行移動変換）を施した後に、このアフィン変換によるスケール値とバイアス値を、それぞれAdaIN の制御パラメーターであるスケール項 ![image](https://user-images.githubusercontent.com/25688193/57304365-28da6300-711a-11e9-8f25-19f6a89fd8ac.png) とバイアス項 ![image](https://user-images.githubusercontent.com/25688193/57304393-37287f00-711a-11e9-94bf-4b78ca958831.png) として用いるという用途で適用される。<br>
    ここで、AdaIN の式は、以下のような式で与えられる。<br>

    ![image](https://user-images.githubusercontent.com/25688193/57304514-722ab280-711a-11e9-8c90-bbbe3b0d973c.png)<br>

	この AdaIN の処理は、特徴マップ単位（＝チャンネル単位）での、正規化処理になっているが、この AdaIN による変換を、各解像度スケール（4×4、8×8、16×16、・・・、1024×1024）の畳み込みの後に行うことで、各解像度スケール（4×4、8×8、16×16、・・・、1024×1024）毎に、”画像全体に渡って大域的に”、スタイル（画風）を変化させることが出来るようになる。<br>

- ピクセル単位の Noise Input と Stochastic variation（確率的変動）：<br>
    人物画像の髪やシワなどは、確率的とみなせる細部の局所的な特徴生成として扱うことが出来る。<br>
	従来の GAN では、このような局所的な特徴生成も（大域的な特徴生成と同様にして）、潜在変数としての入力ノイズを直接生成器に入力することで実現する構造になっている。しかしこの方法では、上記の潜在空間の歪みにより、ノイズの影響を制御することが困難である。<br>

    ![image](https://user-images.githubusercontent.com/25688193/57350819-8615f900-719a-11e9-93c7-2584dc4b592e.png)<br>

    これに対して、StyleGAN では、上図のアーキテクチャ図のように、Synthesis Network g での各畳み込みの直後に、ノイズマップで、ピクセル単位のノイズを直接加えることでこれを実現する。<br>
    このノイズはピクセル単位で適用されるので、髪やシワなどの確率的な変動を、ピクセル単位で局所的に直接制御することが可能になる。<br>
    （※ これに対して、AdaIN は、特徴マップ単位で適用されるので、画風（スタイル）といった画像全体に渡っての大域的な属性を制御している。）<br>


- ProgressiveGAN から入力層を除外し、代わりに学習済み定数マップを入力：<br>
    StyleGAN のベースアルゴリズムである  ProgressiveGAN では、乱数入力で生成器への初期入力（4×4）を入力している。<br>

    ![image](https://user-images.githubusercontent.com/25688193/57351073-8793f100-719b-11e9-8eaa-93ceadb81aea.png)<br>

    一方、StyleGAN の生成器では、上図のアーキテクチャ図の Synthesis Network g の開始部分で示しているように、入力層を完全に排除し、そのかわりに、学習済み定数マップ（4×4×512）を入力の開始としている。<br>
    これは、StyleGAN が、生成画像を、前述の潜在変数 w と AdaIN、及びノイズマップによって制御しているために、入力が不要であることによるものである。<br>

- Style Mixing：<br>
    ![image](https://user-images.githubusercontent.com/25688193/57352094-3d147380-719f-11e9-827c-53b9ed64c873.png)<br>

    Style Mixing は、上図のように潜在空間よりサンプリングした２つの潜在変数 ![image](https://user-images.githubusercontent.com/25688193/57352230-b1e7ad80-719f-11e9-9d7d-ca5763974256.png)、及び２つの中間潜在空間 ![image](https://user-images.githubusercontent.com/25688193/57352252-c4fa7d80-719f-11e9-9bab-768bace02642.png) に関して、Synthesis Network g に AdaIN のパラメーターとして入力する際に、ある解像度スケールまでは、z1,w1 を入力し、それ以降の解像度スケールには、z2, w2 を入力するように切り替えるという正規化手法である。<br>
    これにより、Synthesis Network g は、隣接した解像度スケールの層間で画風（スタイル）に相関があるという前提を置くことができなくなるため、画風（スタイル）の影響を各解像度スケールの層に局在化させることが可能となる。<br>

> 記載中...


<a id="補足事項"></a>

## ■ 補足事項

<a id="KLダイバージェンス"></a>

### ◎ KLダイバージェンス [Kullback-Leibler(KL) diviergence]
２つの確率分布 P,Q 間の距離を表す指標として、KLダイバージェンスと呼ばれる以下のような指標が考えられる。<br>

![image](https://user-images.githubusercontent.com/25688193/56255740-ee743c00-6100-11e9-9e09-3574daab5b09.png)<br>

※ ここで、定義から分かるように、KLダイバージェンスは、２つの確率分布 P,Q に対して、対称ではない。（![image](https://user-images.githubusercontent.com/25688193/56255798-25e2e880-6101-11e9-9ff6-5abfdd6e8b82.png)）これは、例えば、A地点からB地点までの距離と、その反対のB地点からA地点までの距離が異なることを意味している。従って、距離の公理を満たしていないことになり、厳密には距離ではないことになるが、便宜上、統計的距離という。<br>

以下の図は、２つの確率分布 P,Q の具体的な確率分布の形状として、いくつかの正規分布でKLダイバージェンスを求めた図である。<br>

![image](https://user-images.githubusercontent.com/25688193/57173202-12f14780-6e67-11e9-9b26-9d036487fa6a.png)<br>

２つの確率分布 P,Q が完全に一致するとき、０の値となっており、<br>
２つの確率分布 P,Q が重ならない部分が大きくなるにつれ、大きな値となっていることが分かる。<br>

#### ☆ KLダイバージェンスとエントロピーの関係
KLダイバージェンスは、クロスエントロピーとエントロピーの差で記述することも出来る。<br>
即ち、エントロピーは<br>
![image](https://user-images.githubusercontent.com/25688193/56255899-95f16e80-6101-11e9-9791-95c012937999.png)<br>
クロスエントロピーは、<br>
![image](https://user-images.githubusercontent.com/25688193/56255930-ab669880-6101-11e9-9e78-d06bfecee5c4.png)<br>
であることより、<br>
![image](https://user-images.githubusercontent.com/25688193/56255946-bb7e7800-6101-11e9-9633-beb2a3426628.png)<br>
となり、従って、<br>
![image](https://user-images.githubusercontent.com/25688193/56255976-d6e98300-6101-11e9-9b53-a2304f892bc7.png)<br>
の関係が成り立つことがわかる。<br>


<a id="JSダイバージェンス"></a>

### ◎ JSダイバージェンス [Jensen-Shannon(JS) divergence]
KLダイバージェンスは、距離の公理の一つである対称性の性質を満たさない（![image](https://user-images.githubusercontent.com/25688193/56255798-25e2e880-6101-11e9-9ff6-5abfdd6e8b82.png)）ので、扱いづらいという問題があった。<br>
（※この対称性の性質を満たさないことは、例えば、A地点からB地点までの距離と、その反対のB地点からA地点までの距離が異なることを意味している。　）

JSダイバージェンスは、対称性の性質を満たすように、KLダイバージェンスを使って、以下のように定義される２つの確率分布 P,Q 間の距離指標である。<br>
![image](https://user-images.githubusercontent.com/25688193/56256830-3d23d500-6105-11e9-8d7d-4f101841b809.png)<br>

以下の図は、２つの確率分布 P,Q の具体的な確率分布の形状として、いくつかの正規分布でKLダイバージェンスとJSダイバージェンスを求めた図である。<br>

![image](https://user-images.githubusercontent.com/25688193/57173174-92cae200-6e66-11e9-929f-00a4bd409a50.png)<br>

黄色枠で示した確率分布は、２つの確率分布 P,Q の平均値である M である。<br>
この平均分布 M は、各々の確率分布 P,Q から見て対称であるが、<br>
JSダイバージェンスでは、その定義式より、（対称である）平均分布 M と各々の確率分布 P, Q とのKLダイバージェンスを計算することで、対称性の性質を満たすようにしていることがわかる。<br>
２つの確率分布 P,Q が重ならない部分が大きくなるにつれ、JSダイバージェンスは小さな値となっていることが分かる。<br>

<a id="EMD"></a>

### ◎ Earth-Mover距離（Wassertein距離）
Earth-Mover距離（Wassertein距離）は、２つの確率分布間の距離指標の１つであるが、下図のように、確率分布を１つの山とみなすと、片方の確率分布の山をもう片方の確率分布の山に，土を輸送して変形させるための最小の労力であるという最適輸送問題ともみなせる。<br>
（※それ故、Earth-Mover という名前がついている。）<br>

![image](https://user-images.githubusercontent.com/25688193/56466456-69b15700-644d-11e9-869d-6c6ebf66279d.png)<br>

![image](https://user-images.githubusercontent.com/25688193/56466469-88afe900-644d-11e9-9a29-4f37e14e1b4b.png)<br>

![image](https://user-images.githubusercontent.com/25688193/56466479-bdbc3b80-644d-11e9-9a23-c143b4f06f8d.png)<br>

そして、Earth-Mover距離（EMD,Wassertein距離）は、この 「（輸送する労力）＝（輸送する土の量）×（輸送する距離）」 の関係において、全ての組み合わせで総和した以下のような式で定義できる。<br>

![image](https://user-images.githubusercontent.com/25688193/56477963-17704480-64e6-11e9-9396-a72c7cae56af.png)<br>

２つの確率分布の山を一致させるさせるためには、無数の方法が考えられるが、そららの方法の中で最小の労力（＝最小のEMD）で済むような、最適な輸送方法を考えるのが、考えるべき最適輸送問題となる。<br>

そして、この最適輸送問題は、一般的な線形計画法を用いて解くことが出来る。<br>

しかし、結論から述べると、この最適輸送問題を、単純に線形計画法の主形式で解く方法では、GANのように確率変数 x,y の次元が大きい場合には、その計算量が現実的ではなくなる。<br>
一方、この線形計画問題の双対形式で与える式では、現実的に計算可能な式になっている。<br>

そして、この線形計画法の双対形式での、
Earth-Mover距離（EMD,Wassertein距離）の式は、以下のようになる。（式の導出略）<br>

![image](https://user-images.githubusercontent.com/25688193/56466815-c151c180-6451-11e9-8474-4fb0e160df4e.png)<br>

※ 関数 f のリプシッツ連続性の要件は、線形計画問題おける制約条件から発生している。<br>


<a id="UNet"></a>

### ◎ U-Net
セマンティックセグメンテーション（領域抽出）のタスクでは、局所的な特徴と、画像全体の特徴の両方を、元の画像上で捉えることが重要となる。<br>
CNN では、畳み込み層が物体の局所的な特徴を抽出し、プーリング層が物体の全体的な位置情報をぼかしている役割を持っていた。<br>
そのため、層が深くなるほど、抽出される特徴は、より局所的になり、物体の全体的な位置情報はより曖昧になるので、従来のCNNベースの手法（VGG16など）では、局所的な特徴量と、画像全体の特徴の両方を捉えることが困難となる。<br>

U-Net は、セマンティックセグメンテーションのタスクにおけるこの問題を解決するために、全結合層を利用してしない Fully Convolutional Network（FCN）のアーキテクチャをベースとして、ダウンサンプリングのパス（contractiong path）とアップサンプリングのパス（extractiong path）、及び、skip connection の構造を加えることで、この 「（セマンティックセグメンテーションで重要となる）局所的な特徴量と、画像全体の特徴の両方を捉えること」 を実現する。<br>

※ U-Net は、このように、画像のセマンティックセグメンテーションでの用途で考察された手法であり、医療画像セグメンテーションのコンペで当時の SOTA を達成している。<br>
※ 尚、この U-Net は、オートエンコーダー（AE）としての側面もある。<br>
※ 又、この U-Net は、pix2pix の生成器側のネットワークとして採用されている。<br>


<a id="UNetのアーキテクチャ"></a>

#### ☆ U-Net のアーキテクチャ
![image](https://user-images.githubusercontent.com/25688193/56942254-969ff100-6b54-11e9-81e2-f9ffe31eef12.png)<br>

上図は、U-Net のアーキテクチャ図である。<br>
この U-Net のアーキテクチャには、主に、以下のような特徴がある。<br>

- ダウンサンプリングとアップサンプリング（Encoder と Decoder）
    - 概要で述べたように、セマンティックセグメンテーションのタスクにおいては、物体の局所的な特徴と、物体の全体的な位置情報を同時に捉えることが重要となるが、U-Net では、max pooling によるダウンサンプリング側（Encoder側）の深い層で局所的な特徴を捉え、浅い層で、全体的な位置情報を捉えること出来るようにしている。
    - そして、up-conv によるアップサンプリング側（Decoder側）で、特徴を保持したまま、画像の解像度を復元し、出力する。
    - これらの一連の処理は、Encoder（符号器）と Decoder（復号器）に相当する処理となっているため、U-Net は、一種のオートエンコーダー（AE）ともみなせる。

- skip connections
    - U-Net の主な特徴の１つは、Encoder と Decoder との間に、skip connections と呼ばれる構造を持つことである。（※ この構造の効果は、ResNet に似ている。）
    - これは、CNNでの、Encoder-Decoder のように全ての情報をボトルネックまでダウンサンプリングさせるのではなく、共通の特徴量（＝局所的な特徴と物体の全体的な位置情報）は Encoder-Decoder 間を、skip connections 経由でスキップさせて、上図のアーキテクチャ図の下側のボトルネック部分を回避することで、共通の特徴量におけるデータ欠損を回避させるというものである。
    - より詳細には、Encoder 側の深い層ほど局所的特徴が強く、浅い層ほど全体的な位置情報が強くなっているので、深い層での skip connections は、局所的特徴が強い特徴マップをスキップさせ、浅い層での skip connections は、全体的な位置情報が強い特徴マップをスキップさせることになるが、Decoder 側では、特徴を保持したままアップサンプリングが可能であるので、結果として、「（セマンティックセグメンテーションで重要となる）局所的な特徴量と、画像全体の特徴の両方を捉えること」 を実現できるというものである。
    - 尚、Encoder 側からのスキップした特徴マップと、Decoder側からのアップサンプリングされた特徴マップをマージする際には、Encoder 側の特徴マップの中央部分を crop（切り出し）して、両者のサイズを一致させた上でマージを行う。これは、Encoder 側では、padding=0 の畳み込みで特徴サイズが（ダウンサンプリングとは違う意図で）小さくなっているために、Decoder 側で２倍間隔でアップサンプリングされた特徴マップと、サイズが一致しないくなってしまうためである。

- 全結合層を利用していない。
    - U-Net では、全結合層は使用されていない。
    - このような畳み込みのみからなるネットワークは、 Fully Convolutional Network（FCN）と呼ばれるが、U-Net は、この Fully Convolutional Network（FCN）のアーキテクチャをベースとして設計されている。
    - U-Net で全結合層が使用されていない理由としては、CNN での画像分類のアーキテクチャの出力（この場合は最後の層が全結合層になる必要がある）とは異なり、ネットワークの出力が画像であることが１つの理由として考えられる。
    - <font color="Pink">別の理由としては、overlap-tile strategy によって、任意の大きな画像のシームレスなセグメンテーションを許可するため？（元論文での記載より）</font>

<a id="UNetの学習方法">

#### ☆ U-Net の学習方法

> 記載中...


<a id="UNetの適用例">

#### ☆ U-Net の適用例

- [UNet によるセマンティックセグメンテーションを利用して、衛星画像から地図を生成する。](https://github.com/Yagami360/MachineLearning_Exercises_Python_PyTorch/tree/master/UNet_PyTorch)


<a id="StyleTransferにおける正規化手法（INとAdaIN）"></a>

### ◎ Style Transfer における正規化手法（INとAdaIN）

- Instance Normalization（IN）
    BatchNorm のように、バッチサイズ単位で正規化するのではなく、特徴マップ単位（＝画像の縦横幅単位）で正規化する手法。<br>

    ![image](https://user-images.githubusercontent.com/25688193/57304779-ed8c6400-711a-11e9-9b8a-d1e88e6ecb23.png)<br>

    具体的には、以下の式で与えられる。<br>
    ![image](https://user-images.githubusercontent.com/25688193/57304837-009f3400-711b-11e9-9756-04a87e1e97cd.png)<br>
    Style Transfer のタスクにおいて、Batch Normalization をこの Instance Normalization に変えることで、Style Transfer の質において大きな向上が達成されることが報告されている。

- Adaptive Instance Normalization (AdaIN)

> 記載中...


<a id="参考"></a>

## ■ 参考

### ◎ 参考サイト

**強調文字**付きは、特に参考にさせたもらったサイトです。<br>

- 全般
    - [**GAN（と強化学習との関係）**](https://www.slideshare.net/masa_s/gan-83975514)
    - [IIBMP2016 深層生成モデルによる表現学習](https://www.slideshare.net/pfi/iibmp2016-okanohara-deep-generative-models-for-representation-learning)
    - [タカハシ春の GAN 祭り！〜 一日一GAN(๑•̀ㅂ•́)و✧ 〜- ABEJA Arts Blog](https://tech-blog.abeja.asia/entry/everyday_gan)

- VAE
    - [**Variational Autoencoder徹底解説**](https://qiita.com/kenmatsu4/items/b029d697e9995d93aa24)
    - [**猫でも分かるVariational AutoEncoder**](https://www.slideshare.net/ssusere55c63/variational-autoencoder-64515581)
    - 実装
        - - [PyTorch (11) Variational Autoencoder - 人工知能に関する断創録](http://aidiary.hatenablog.com/entry/20180228/1519828344)

- GAN
    - [**【元論文】[1406.2661] Generative Adversarial Networks**](https://arxiv.org/abs/1406.2661)
    - [**今さら聞けないGAN（1）　基本構造の理解**](https://qiita.com/triwave33/items/1890ccc71fab6cbca87e)
    - [**Generative Adversarial Networks(GAN)を勉強して、kerasで手書き文字生成する - 緑茶思考ブログ**](http://yusuke-ujitoko.hatenablog.com/entry/2017/05/08/010314)
    - [GANの論文を読んだ自分なりの理解とTensorflowでのGANの実装メモ](http://owatank.hatenablog.com/entry/2018/04/20/180151)

- DCGAN
    - [**【元論文】[1511.06434] Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks**](https://arxiv.org/abs/1511.06434)
    - 実装
        - [**DCGAN Tutorial — PyTorch Tutorials 1.0.0.dev20190327 documentation**](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html?highlight=dataloader)
        - [GitHub - znxlwm/pytorch-MNIST-CelebA-GAN-DCGAN: Pytorch implementation of Generative Adversarial Networks (GAN) and Deep Convolutional Generative Adversarial Networks (DCGAN) for MNIST and CelebA datasets](https://github.com/znxlwm/pytorch-MNIST-CelebA-GAN-DCGAN)
        - [**PyTorch (12) Generative Adversarial Networks (MNIST) - 人工知能に関する断創録**](http://aidiary.hatenablog.com/entry/20180304/1520172429)

- Conditional GAN（cGAN）
    - [**【元論文】[1411.1784] Conditional Generative Adversarial Nets**](https://arxiv.org/abs/1411.1784)
    - [**今さら聞けないGAN（6） Conditional GANの実装**](https://qiita.com/triwave33/items/f6352a40bcfbfdea0476)
    - 実装
        - [GitHub/znxlwm/pytorch-MNIST-CelebA-cGAN-cDCGAN](https://github.com/znxlwm/pytorch-MNIST-CelebA-cGAN-cDCGAN)
        - [**PyTorch で Conditional GAN をやってみる**](http://cedro3.com/ai/pytorch-conditional-gan/)

- Wasserstein GAN
    - [**【元論文】[1701.07875] Wasserstein GAN**](https://arxiv.org/abs/1701.07875)
    - [**Read-through: Wasserstein GAN**](https://www.alexirpan.com/2017/02/22/wasserstein-gan.html)
    - [**From GAN to WGAN**](https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html)
    - [GAN — Wasserstein GAN & WGAN-GP](https://medium.com/@jonathan_hui/gan-wasserstein-gan-wgan-gp-6a1a2aa1b490)
    - [**Wasserstein GAN と Kantorovich-Rubinstein 双対性 - Qiita**](https://qiita.com/mittyantest/items/0fdc9ce7624dbd2ee134)
    - [**[DL輪読会]Wasserstein GAN/Towards Principled Methods for Training Generative Adversarial Networks**](https://www.slideshare.net/DeepLearningJP2016/dlwasserstein-gantowards-principled-methods-for-training-generative-adversarial-networks)
    - [Wasserstein GAN（WGAN）でいらすとや画像を生成してみる - 緑茶思考ブログ](http://yusuke-ujitoko.hatenablog.com/entry/2017/05/20/145924)
    - [Wasserstein GAN [arXiv:1701.07875] – ご注文は機械学習ですか？](http://musyoku.github.io/2017/02/06/Wasserstein-GAN/)
    - [Wasserstein GAN (WGAN) - DeepLearningを勉強する人](http://wanwannodao.hatenablog.com/entry/2017/02/28/051353)
    - 実装
        - [**GitHub - martinarjovsky/WassersteinGAN**](https://github.com/martinarjovsky/WassersteinGAN)

- pix2pix
    - [**【元論文】Image-to-Image Translation with Conditional Adversarial Networks**](https://arxiv.org/abs/1611.07004)
    - [**pix2pixでポップアートから写真を復元してみた (追記あり)**](https://qiita.com/hiromu1996/items/38f1bd5a78336fa8ca25)
    - [**深層学習を利用した食事画像変換で飯テロ**](https://qiita.com/negi111111/items/6d6f19edc060a91662ec)
    - [**[DL輪読会]Image-to-Image Translation with Conditional Adversarial Networks**](https://www.slideshare.net/DeepLearningJP2016/dlimagetoimage-translation-with-conditional-adversarial-networks)
    - [Image-to-Image Translation with Conditional Adversarial Nets](https://phillipi.github.io/pix2pix/)
    - [Image-to-Image Translation in Tensorflow](https://affinelayer.com/pix2pix/)
    - 実装
        - [GitHub/junyanz/pytorch-CycleGAN-and-pix2pix](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)
        - [**GitHub/eriklindernoren/PyTorch-GAN**](https://github.com/eriklindernoren/PyTorch-GAN#pix2pix)

- U-Net
    - [**【元論文】U-Net: Convolutional Networks for Biomedical Image Segmentation**](https://arxiv.org/abs/1505.04597)
    - [**Deep learningで画像認識⑨〜Kerasで畳み込みニューラルネットワーク vol.5〜**](https://lp-tech.net/articles/5MIeh)
    - [U-Net：ディープラーニングによるSemantic Segmentation手法](https://blog.negativemind.com/2019/03/15/semantic-segmentation-by-u-net/)
    - [U-Net: Convolutional Networks for Biomedical Image Segmentationの紹介](https://www.slideshare.net/KCSKeioComputerSocie/unet-convolutional-networks-for-biomedicalimage-segmentation?ref=https://blog.negativemind.com/2019/03/15/semantic-segmentation-by-u-net/)
    - [オートエンコーダーとしてのU-Net（自己符号化から白黒画像のカラー化まで）](https://qiita.com/koshian2/items/603106c228ac6b7d8356)
    - [Semantic Segmentation — U-Net (Part 1)](https://medium.com/@keremturgutlu/semantic-segmentation-u-net-part-1-d8d6f6005066)
    - 実装
        - [**GitHub/GunhoChoi/Kind-PyTorch-Tutorial12_Semantic_Segmentation/**](https://github.com/GunhoChoi/Kind-PyTorch-Tutorial/tree/master/12_Semantic_Segmentation)

- ProgressiveGAN

- StyleGAN
    - [**【元論文】[1812.04948] A Style-Based Generator Architecture for Generative Adversarial Networks**](https://arxiv.org/abs/1812.04948)
    - [**StyleGAN「写真が証拠になる時代は終わった。」 - Qiita**](https://qiita.com/Phoeboooo/items/7be15acb960837adab21)
    - [**StyleGAN: A Style-Based Generator Architecture for Generative Adversarial Networks - 機械学習奮闘記**](https://sakuma-dayo.hatenablog.com/entry/2019/03/06/201429)
    - [**ニューラルネットワークでStyle Transferを行う論文の紹介 - Qiita**](https://qiita.com/kidach1/items/0e7af5981e39955f33d6)
    - [ニューラルネットワークでStyle Transferを行う論文の紹介](https://qiita.com/kidach1/items/0e7af5981e39955f33d6)
    - [**Style-based GANs – Generating and Tuning Realistic Artificial Faces | Lyrn.AI**](https://www.lyrn.ai/2018/12/26/a-style-based-generator-architecture-for-generative-adversarial-networks/)
    - [Arbitrary Style Transfer in Real-Time With Adaptive Instance Normalization の紹介](https://www.slideshare.net/HHiroto/arbitrary-style-transfer-in-realtime-with-adaptive-instance-normalization-84174047)
    - 実装
        - [PyTorchでStyleGAN - Qiita](https://qiita.com/t-ae/items/afc969c48450507dc421)

- その他
    - [**KL divergenceとJS divergenceの可視化**](http://yusuke-ujitoko.hatenablog.com/entry/2017/05/07/200022)
    - [正規分布間のKLダイバージェンス](https://qiita.com/ceptree/items/9a473b5163d5655420e8)

