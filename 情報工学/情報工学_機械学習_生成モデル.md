# 生成モデル [generative model]
機械学習を生成モデル手法（GAN、VAE等）について勉強したことをまとめたノート（忘備録）です。現在執筆中<br>

## 目次 [Contents]

1. 概要
1. [変分オートエンコーダー [VAE : Variational AutoEncoder]](#VAE)
    1. [VAE のアーキテクチャ](#VAEのアーキテクチャ)
    1. [VAE の学習とKLダイバージェンス](#VAEの学習とKLダイバージェンス)
    <!--
    1. [潜在変数の空間と生成画像の分布の関係](#潜在変数の空間と生成画像の分布の関係)
    -->
1. [GAN [Generative Adversarial Networks]](#GAN)
    1. [GAN のアーキテクチャ](#GANのアーキテクチャ)
    1. [識別器の動作と損失関数](#識別器の動作と損失関数)
    1. [生成器の動作と損失関数](#生成器の動作と損失関数)
    1. [密度比推定による識別器の役割の再解釈](#密度比推定による識別器の役割の再解釈)
    1. [JSダイバージェンスによる識別器の損失関数の再解釈](#JSダイバージェンスによる識別器の損失関数の再解釈)
    1. [GANの学習の困難さ](#GANの学習の困難さ)
        1. [GANの収束性](#GANの収束性)
        1. [モード崩壊](#モード崩壊)
        1. [勾配損失問題](#勾配損失問題)
1. [DCGAN [Deep Convolutional GAN]](#DCGAN)
    1. [DCGAN のアーキテクチャ](#DCGANのアーキテクチャ)
    1. [DCGAN の適用例](#DCGANの適用例)
1. [Conditional GAN（cGAN）](#ConditionalGAN（cGAN）)
    1. [cGAN のアーキテクチャ](#cGANのアーキテクチャ)
    1. [cGAN の損失関数](#cGANの損失関数)
    1. [cGAN の適用例](#cGANの適用例)
1. [WGAN [Wasserstein GAN]](#WGAN)
    1. [JSダイバージェンスと Earth-Mover 距離の収束性と勾配消失問題](#JSダイバージェンスとEarth-Mover距離の収束性の違いと勾配消失問題)
    1. [WGAN のアーキテクチャと損失関数](#WGANのアーキテクチャと損失関数)
    1. [WGANでのその他の工夫](#WGANでのその他の工夫)
    1. [WGAN のアルゴリズム](#WGANのアルゴリズム)
    1. [WGANのその他の利点](#WGANのその他の利点)
    1. [WGAN の適用例](#WGANの適用例)
1. WGAN-gp
1. [pix2pix](#pix2pix)
    1. [pix2pix のアーキテクチャ](#pix2pixのアーキテクチャ)
    1. [pix2pix の損失関数](#pix2pixの損失関数)
    1. [pix2pix の適用例](#pix2pixの適用例)
1. [ProgressiveGAN（PGGAN）](#ProgressiveGAN（PGGAN）)
    1. [ProgressiveGAN のアーキテクチャ](#ProgressiveGANのアーキテクチャ)
    1. [生成画像の多様性向上とモード崩壊防止のための工夫](#生成画像の多様性向上とモード崩壊防止のための工夫)
    1. [学習の安定化のための工夫](#学習の安定化のための工夫)
    1. [GAN の生成画像の評価](#GANの生成画像の評価)
1. [StyleGAN](#StyleGAN)
    1. [StyleGAN のアーキテクチャ](#StyleGANのアーキテクチャ)
    1. [潜在空間における entanglement（もつれ）の disentanglement （解きほぐし）の評価](#潜在空間におけるentanglement（もつれ）のdisentanglement（解きほぐし）の評価)
1. [補足事項](#補足事項)
    1. [【補足】KLダイバージェンス [Kullback-Leibler(KL) diviergence]](#KLダイバージェンス)
    1. [【補足】JSダイバージェンス [Jensen-Shannon(JS) divergence]](#JSダイバージェンス)
    1. [【補足】Inception score](#Inceptionscore)
    1. [【補足】Earth-Mover 距離（Wassertein距離）](#EMD)
    1. [【補足】U-Net](#UNet)
        1. [U-Net のアーキテクチャ](#UNetのアーキテクチャ)
        1. U-Net の学習方法
        1. [U-Net の適用例](#UNetの適用例)
    1. [【補足】Minibatch discrimination](#Minibatchdiscrimination)
    1. [【補足】 Style Transfer における正規化手法（INとAdaIN）](#StyleTransferにおける正規化手法（INとAdaIN）)
    <!--
    1. [【補足（外部リンク）】情報理論 / 情報数理](http://yagami12.hatenablog.com/entry/2017/09/17/103228)
    1. [【補足（外部リンク）】ゲーム理論](http://yagami12.hatenablog.com/entry/2018/03/02/171939)
    -->
1. [参考サイト](#参考)
<!--
1. 論文翻訳
    1. [【論文翻訳（非公開）】Generative Adversarial Networks](https://github.com/Yagami360/MachineLearning-Papers_Survey/blob/master/papers/Generative_Adversarial_Networks/GenerativeAdversarialNetworks.md)
    1. [【論文翻訳（非公開）】Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://github.com/Yagami360/MachineLearning-Papers_Survey/blob/master/papers/Unsupervised_Representation_Learning_with_Deep_Convolutional_Generative_Adversarial_Networks/DeepConvolutionalGAN.md)
    1. [【論文翻訳（非公開）】Conditional Generative Adversarial Nets](https://github.com/Yagami360/MachineLearning-Papers_Survey/blob/master/papers/Conditional_Generative_Adversarial_Nets/ConditionalGAN.md)
    1. [【論文翻訳（非公開）】Wasserstein GAN](https://github.com/Yagami360/MachineLearning-Papers_Survey/blob/master/papers/Wasserstein_GAN/WassersteinGAN.md)
    1. [【論文翻訳（非公開）】Image-to-Image Translation with Conditional Adversarial Networks](https://github.com/Yagami360/MachineLearning-Papers_Survey/blob/master/papers/Image-to-Image_Translation_with_Conditional_Adversarial_Networks/pix2pix.md)
    1. [【論文翻訳（非公開）】U-Net: Convolutional Networks for Biomedical Image Segmentation](https://github.com/Yagami360/MachineLearning-Papers_Survey/blob/master/papers/U-Net_Convolutional_Networks_for_Biomedical/UNet.md)
    1. [【論文翻訳（非公開）】A Style-Based Generator Architecture for Generative Adversarial Networks](https://github.com/Yagami360/MachineLearning-Papers_Survey/blob/master/papers/A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks/StyleGAN.md)
-->

---

<a id="VAE"></a>

## ■ 変分オートエンコーダー [VAE : Variational AutoEncoder]

<a id="VAEのアーキテクチャ"></a>

### ◎ VAE のアーキテクチャ
（生成モデルの分野における）オートエンコーダーとは、教師なし学習のもとで、データを表現するための特徴を獲得するために、以下のようなアーキテクチャと処理手順を持った手法である。<br>

![image](https://user-images.githubusercontent.com/25688193/56261188-05248e00-6115-11e9-9ee8-c4ebe45b67dc.png)<br>

【オートエンコーダーの処理手順】<br>
1. 入力データ（上図では手書き数字画像データ）x を、エンコーダー（ニューラルネットワーク）で潜在変数 z に変換する。<br>
	この様子は、見方を変えると入力データの符号化しているようにみなせるため、この入力データを変換するニューラルネットワークをエンコーダーと呼ぶ。<br>
    又、潜在変数 z の次元が、入力データ x より小さい場合、このエンコーダーでの処理は次元削除になっているとみなすことも出来る。<br>
1. 潜在変数 z を、デコーダー（ニューラルネットワーク）に入力し、元の画像に再変換する。<br>
    この様子は、見方を変えるとエンコーダーで符号化した潜在変数を、再度（入力データである画像に）復号化しているようにみなせるため、この潜在変数を変換するニューラルネットワークをデコーダーと呼ぶ。<br>

<br>

このようなオートエンコーダーの内、以下のようなアーキテクチャ図のように、潜在変数 z を生成する分布が、正規分布 N(0,1) であるように設定した（ z~N(0,1)  ）オートエンコーダーを、VAE [Variational AutoEncoder] という。<br>
（これに対して、通常の AutoEncoder は潜在変数 z を生成する分布を仮定していない。）<br>

![image](https://user-images.githubusercontent.com/25688193/57172796-dff78580-6e5f-11e9-91cf-a71af9ab692a.png)<br>

※ ここで、VAE におけるエンコーダーは、正規分布に従う潜在変数 z を直接出力するのではなく、潜在変数 z が従う正規分布の平均値 μ(x) と分散値 σ(x) を生成していることに注意。<br>


<a id="VAEの学習とKLダイバージェンス"></a>

### ◎ VAE の学習とKLダイバージェンス
VAE の学習の目的は、潜在変数 z から画像を生成する確率分布 p_θ (x) の最大化である。<br>
但し、確率分布のままでは扱いづらいため、その対数尤度 ![image](https://user-images.githubusercontent.com/25688193/56268166-b1716f00-612b-11e9-96f6-e219ce7af871.png) の最大化を考える。<br>

結論から述べると、この対数尤度 ![image](https://user-images.githubusercontent.com/25688193/56268166-b1716f00-612b-11e9-96f6-e219ce7af871.png) は、以下の式ように、変分下限 [ELBO : Evidence Lower BOund] なるものととKLダイバージェンスの和で表現できる。（詳細計算略）<br>
![image](https://user-images.githubusercontent.com/25688193/56268264-dd8cf000-612b-11e9-88a3-52169bd47f44.png)<br>

ここで、KLダイバージェンスの項 ![image](https://user-images.githubusercontent.com/25688193/56268329-07dead80-612c-11e9-8a6c-49949d8b5470.png) はその定義より、常に０以上の値となるが、VAE の学習の目的である、![image](https://user-images.githubusercontent.com/25688193/56268673-e03c1500-612c-11e9-8022-a4c2974fd409.png) のときは０の値となるので、対数尤度 ![image](https://user-images.githubusercontent.com/25688193/56268166-b1716f00-612b-11e9-96f6-e219ce7af871.png) を最大化したければ、変分下限 L(θ,ϕ,x) を最大化すれば良いことが分かる。<br>

次に、この変分下限 L(θ,ϕ,x) は、以下の式に変形出来る。（詳細計算略）<br>
![image](https://user-images.githubusercontent.com/25688193/56268734-0cf02c80-612d-11e9-89b8-92d02bad08f3.png)<br>

従って、変分下限 L(θ,ϕ,x)  を最大化するためには、KLダイバージェンスを最小化し、復元誤差を最大化すれば良いことが分かる。<br>

まとめると、VAE の学習の目的である対数尤度 ![image](https://user-images.githubusercontent.com/25688193/56268166-b1716f00-612b-11e9-96f6-e219ce7af871.png) の最大化したければ、変分下限 L(θ,ϕ,x) を最大化すれば良いが、<br>
そのためには、２つの確率分布 ![image](https://user-images.githubusercontent.com/25688193/56268861-550f4f00-612d-11e9-96bd-fc7fe83cc895.png) のKLダイバージェンス<br>
![image](https://user-images.githubusercontent.com/25688193/56268970-8d169200-612d-11e9-82fc-b09556914cd9.png)<br>
を最小化し、<br>
復元誤差<br>
![image](https://user-images.githubusercontent.com/25688193/56268998-a15a8f00-612d-11e9-9935-a448a70220c8.png)<br>
を最大化すれば良いこととになる。<br>


<!--
> 図を自作のものに要修正

<a id="潜在変数の空間と生成画像の分布の関係"></a>

### ◎ 潜在変数の空間と生成画像の分布の関係
以下の図は、潜在変数 z の次元を２次元として、エンコーダー側から正規分布に従って出力される潜在変数 z の値と、実際にデコーダーから出力される画像の対応関係を示した図である。<br>

![image](https://user-images.githubusercontent.com/25688193/56263478-0908de00-611e-11e9-890a-d34fbc180164.png)<br>

エンコーダーから出力される潜在変数 z の空間において、正規分布 N(0,1) の密度が大きい領域（＝中央部分）では、3,5,8,9,2 の画像などの似た形状の画像の種類が多く分布していることがわかる。反対に、1,7,0 などの似ていない形状の画像の種類は、正規分布の密度が小さい領域に分布していることが分かる。<br>

<br>

更に、以下の左下図は、潜在変数 z の次元を２次元として、潜在変数 z の値を動かした場合（赤矢印）の、デコーダー側から出力される出力画像の変化を表した図である。<br>
![image](https://user-images.githubusercontent.com/25688193/56262854-7e26e400-611b-11e9-88d6-05e974c449a6.png)<br>
先のエンコーダー側から出力される潜在変数の値の分布図（右上図）と、概ね一致していることが分かる。<br>
-->

<a id="GAN"></a>

## ■ GAN [Generative Adversarial Networks]

<!--
> VAE からの話（真の分布とモデルの分布の一致化等）の延長で、議論を展開した文章に修正すること。
-->

GAN は、Generator（生成器）と Discriminator（識別器）という２つの機構から構成される生成モデルである。<br>

生成器は、学習用データと同じようなデータ（偽物画像）を生成する。<br>
一方、識別器は、（この学習用データと、生成器が出力した偽物画像を入力とし）、これらのデータが、学習用データから来たものであるのか、或いは、生成器から来た偽物画像であるのかを識別する。<br>

そして、この生成器と識別器の双方が、敵対的に学習していくにつれ、次第に、識別器が本物の学習用データと見分けがつかないデータを生成することが出来るようになるという流れとなる。<br>
※ このとき、完全に学習が進むと、生成器は、データが本物画像なのか偽物画像なのかを、完全に見分けがつかなくなるので、識別率が 50% となる。<br>

尚、この生成器と識別器による処理は、一種の関数とみなせるが、この表現として、一般的に、多層パーセプトロン（MLP）が用いられる。<br>
※ ニューラルネットワークを用いると、既存の最尤推定による生成モデルでは爆発的に増加する計算量を誤差逆伝搬法で解決できる。<br>


<a id="GANのアーキテクチャ"></a>

### ◎ GAN のアーキテクチャ
![image](https://user-images.githubusercontent.com/25688193/57116628-5bd6cc80-6d91-11e9-9fa6-2089ffd7bbbc.png)<br>
![image](https://user-images.githubusercontent.com/25688193/57116750-22eb2780-6d92-11e9-9644-9d56420e743d.png)<br>

<!--
![image](https://user-images.githubusercontent.com/25688193/56490862-a0f13800-6521-11e9-9e51-77b579de22db.png)<br>
![image](https://user-images.githubusercontent.com/25688193/56491160-85d2f800-6522-11e9-8d67-bf2eabb2fe91.png)<br>
-->

<a id="識別器の動作と損失関数"></a>

### ◎ 識別器の動作と損失関数
識別器の損失関数は、以下のような式で与えられる。<br>
![image](https://user-images.githubusercontent.com/25688193/57116895-0ef3f580-6d93-11e9-8b5c-0e36bf0a92e3.png)<br>
※ この式の、![image](https://user-images.githubusercontent.com/25688193/57117890-529e2d80-6d9a-11e9-8cb6-9f8a696b62a4.png) の部分は、ゲーム理論で言うところの、min-max 法（想定される最小の利益が最大になるように最適化）に相当する式になっている。（ゲームは、２プレイヤーのゼロサムゲーム）<br>

ここで、この損失関数の式は、以下のような識別器の動作の意味に対応している。<br>
① 識別器は、本物画像 x が入力されたとき、本物画像 x を（正しく）本物に判定するように ![image](https://user-images.githubusercontent.com/25688193/57117139-d8b77580-6d94-11e9-8cfd-e396d0eaa99a.png) を出力しようとする。<br>
⇒ 第１項 ![image](https://user-images.githubusercontent.com/25688193/57116786-4f9f3f00-6d92-11e9-9d03-0a2d9f2b2acb.png) の最大化に対応<br>

![image](https://user-images.githubusercontent.com/25688193/57117538-b2470980-6d97-11e9-8adf-b0ceb7eb2fac.png)<br>

② 識別器は、偽物画像 ![image](https://user-images.githubusercontent.com/25688193/57116920-36e35900-6d93-11e9-89d9-040c8a126c56.png) が入力されたとき、偽物画像 ![image](https://user-images.githubusercontent.com/25688193/57116920-36e35900-6d93-11e9-89d9-040c8a126c56.png) を（正しく）偽物に判定するように ![image](https://user-images.githubusercontent.com/25688193/57116877-e966ec00-6d92-11e9-8fe9-48cab2627791.png) を出力しようとする。<br>
⇒ 第２項 ![image](https://user-images.githubusercontent.com/25688193/57117395-a9a20380-6d96-11e9-9140-97bb8f91e868.png) の最大化に対応<br>

![image](https://user-images.githubusercontent.com/25688193/57117512-790e9980-6d97-11e9-8bda-d6c37db14c02.png)<br>

<a id="生成器の動作と損失関数"></a>

### ◎ 生成器の動作と損失関数
![image](https://user-images.githubusercontent.com/25688193/56271987-b38bfb80-6134-11e9-9053-092bc8d798db.png)<br>

ここで、この損失関数の式は、以下のような生成器の動作の意味に対応している。<br>
➀ 識別器が、偽物画像 ![image](https://user-images.githubusercontent.com/25688193/57116920-36e35900-6d93-11e9-89d9-040c8a126c56.png) を（正しく）偽物だと判断し、![image](https://user-images.githubusercontent.com/25688193/57116877-e966ec00-6d92-11e9-8fe9-48cab2627791.png) を出力する <br>
⇒ 生成器は識別器を騙すことに失敗しているので、損失関数 L_G の値は大きくなる。（＝大きくなるように定義している）<br>

➁ 識別器が、偽物画像 ![image](https://user-images.githubusercontent.com/25688193/57116920-36e35900-6d93-11e9-89d9-040c8a126c56.png) を（誤って）偽物だと判断し、![image](https://user-images.githubusercontent.com/25688193/57117139-d8b77580-6d94-11e9-8cfd-e396d0eaa99a.png) を出力する。
⇒ 生成器は識別器を騙すことに成功しているので、損失関数 L_G の値は小さくなる。（＝小さくなるように定義している）<br>


<a id="密度比推定による識別器の役割の再解釈"></a>

### ◎ 密度比推定による識別器の役割の再解釈
ここでは、密度比推定の観点から、GANにおける識別器が果たす役割を、理論的に再解釈する。

先の VAE では、デコーダー側での潜在変数 z から画像を生成する確率分布 p_θ (x) が、正規分布やベルヌーイ分布の形になると仮定した上で、その確率分布の対数尤度を最大化するように学習していた。<br>

一方、GANでは、このような生成器の確率分布 ![image](https://user-images.githubusercontent.com/25688193/56272299-62c8d280-6135-11e9-8467-89448e51e0d0.png) の具体的な形を仮定することなしに、暗黙的な生成モデルとして考えている。<br>
従って、VAEのときのように、確率分布の対数尤度 ![image](https://user-images.githubusercontent.com/25688193/56272169-1f6e6400-6135-11e9-8ad7-c4ff1eb0c28e.png) を直接評価できないので、モデルの分布 ![image](https://user-images.githubusercontent.com/25688193/56272299-62c8d280-6135-11e9-8467-89448e51e0d0.png) と真の分布 ![image](https://user-images.githubusercontent.com/25688193/56272229-3ca33280-6135-11e9-8682-7ab994fc852f.png) の密度比<br>
![image](https://user-images.githubusercontent.com/25688193/56272333-77a56600-6135-11e9-8e19-728221fc907b.png)<br>
でモデルの分布がどのくらい真の分布っぽいかの尤度を評価することになる。<br>

ここで、データ集合 X のうち、<br>
半分のデータがモデルの分布 ![image](https://user-images.githubusercontent.com/25688193/56272299-62c8d280-6135-11e9-8467-89448e51e0d0.png) から生成されてものと考え、そのラベルを y=0 とし、<br>
もう半分のデータが真の分布 ![image](https://user-images.githubusercontent.com/25688193/56272229-3ca33280-6135-11e9-8682-7ab994fc852f.png) から生成されてものと考え、そのラベルを y=1 とすると、<br>
モデルの分布と真の分布は、ラベルが設定された際の条件付き確率分布<br>
![image](https://user-images.githubusercontent.com/25688193/56272437-a9b6c800-6135-11e9-8d73-9695403240ea.png)<br>
で表現できる。<br>

従って、密度比の式は、<br>
![image](https://user-images.githubusercontent.com/25688193/56272551-e2ef3800-6135-11e9-85a0-36c6526f3cd0.png)<br>
つまり、観測データ x が、モデルの分布 ![image](https://user-images.githubusercontent.com/25688193/56272299-62c8d280-6135-11e9-8467-89448e51e0d0.png) から来たのか？真の分布 ![image](https://user-images.githubusercontent.com/25688193/56272229-3ca33280-6135-11e9-8682-7ab994fc852f.png) から来たのか？を予想するような識別モデル<br>
![image](https://user-images.githubusercontent.com/25688193/56272572-fbf7e900-6135-11e9-8825-6d826fa79631.png)<br>
を学習出来れば、密度比を推定することが出来る。<br>
この識別モデルこれはまさに、先に述べた識別器 D そのもの役割であり、従って、識別器 D は、密度比を推定しているともみなすことも出来る。<br>
※ 識別器によって、GAN における尤度推定を、NNが得意としている分類問題に置き換えている点に注目。<br>


<a id="JSダイバージェンスによる識別器の損失関数の再解釈"></a>

### ◎ JSダイバージェンスによる識別器の損失関数の再解釈
GAN の学習の目的は、モデルの分布と真の分布を近づけ ![image](https://user-images.githubusercontent.com/25688193/56332861-ea155500-61cc-11e9-8032-98293e456e86.png) とすることである。このことは、以下で見るようにモデルの分布と真の分布間のJSダイバージェンスを最小化していることに一致する。<br>

真の分布 ![image](https://user-images.githubusercontent.com/25688193/56272229-3ca33280-6135-11e9-8682-7ab994fc852f.png) とモデルの分布 ![image](https://user-images.githubusercontent.com/25688193/56272299-62c8d280-6135-11e9-8467-89448e51e0d0.png) との間のJSダイバージェンスは、<br>

![image](https://user-images.githubusercontent.com/25688193/56332894-02856f80-61cd-11e9-9b0d-1e8993fbae3e.png)<br>

ここで、上式の赤字の項 ![image](https://user-images.githubusercontent.com/25688193/56332952-3cef0c80-61cd-11e9-8cc2-78d1f05581a7.png) は、識別器の損失関数そのものであるので、損失関数を最小化することは、JSダイバージェンスを最小化することと等価であることが分かる。<br>
そして、このJSダイバージェンスを最小化することは、モデルの分布と真の分布の２つの分布を一致させることであるので、<br>
結局のところ、<br>
「GANの学習の目的であるモデルの分布と真の分布を近づけ ![image](https://user-images.githubusercontent.com/25688193/56332861-ea155500-61cc-11e9-8032-98293e456e86.png) とすること。」<br>
⇔「（真の分布とモデルの分布の）JSダイバージェンスを最小化すること。」<br>
⇔「（識別器の）損失関数を最小化すること。」<br>
の関係が成り立つことが分かる。<br>

<a id="GANの収束性"></a>

### ◎ GANの学習の困難さ
GANの学習の難しさの要因には、大きく分けて以下のような要因がある。

- 損失関数の収束性の問題
- モード崩壊
- 勾配損失問題
- 生成画像のクオリティーの評価が、損失関数から判断し難い


<a id="GANの収束性"></a>

#### ☆ GANの収束性
GANの学習の難しさの１つの要因に、損失関数の収束性の問題がある。<br>

- GANは、識別器と生成器の２人プレイヤーゼロサムゲームになっているが、このゲームの最適解は、ナッシュ均衡点になる。<br>

- ２人プレイヤーゼロサムゲームのナッシュ均衡点は、鞍点になる。<br>
    ![image](https://user-images.githubusercontent.com/25688193/56336649-213f3280-61dc-11e9-8b55-9360b88fcc82.png)<br>

- 識別器の損失関数<br>
    ![image](https://user-images.githubusercontent.com/25688193/56336686-3f0c9780-61dc-11e9-9719-49f1faf7b2d5.png)<br>
    の形状が、上図のような凸関数であれば、SGDによってナッシュ均衡点（＝鞍点）に収束されることが保証されるが、非凸関数の場合は、保証されない。（GANではニューラルネットワークにより損失関数を表現するので、関数の形状は非凸関数）<br>

- このような非凸関数に対して、SGDで最適化（＝鞍点の探査）を行っていくと、ナッシュ均衡点（＝鞍点）に行かず、振動する可能性がある。<br>
    例えば、損失関数が L(a,b)=a×b という単純な形であった場合にも、SGDで生成器と識別器を交互に最適化していくと、以下の図のように、損失関数の値が発散する。<br>
    ![image](https://user-images.githubusercontent.com/25688193/56336712-4f247700-61dc-11e9-9557-f4f661c2d747.png)<br>

<a id="モード崩壊"></a>

#### ☆ モード崩壊 [Mode collapse]
学習が不十分な識別器に対して、生成器を最適化した場合や、生成器への入力ノイズ z の潜在変数としての次元が足りたていない場合などにおいて、生成器による生成画像が、ある特定の画像（例えば、MNISTでは数字の３などの特定の画像）に集中してしまい、学習用データが本来持っている多様な種類の画像（例えば、MNISTでは数字の0~9の画像）を生成できなくなってしまう問題がある。<br>
GANにおいて発生するこのような問題を、モード崩壊といい、GANの学習の困難さの要因の１つになっている。<br>
※ ここでいうモード（流行）とは、最頻出値のこと。<br>

<!--
> 自作の図を要追加
![image](https://user-images.githubusercontent.com/25688193/56337604-10dd8680-61e1-11e9-8c81-fdb1090c67e2.png)<br>
-->

<a id="勾配損失問題"></a>

#### ☆ 勾配損失問題
先に見たように、学習が十分でない識別器に対して、生成器を最適化すると、モード崩壊が発生してしまう。<br>
それを防ぐために、ある生成器 G の状態に対しての識別器を完全に学習すると、今度は勾配損失問題が発生してしまう。<br>
このように、GANでは、モード崩壊と勾配損失問題が互いに反して発生してしまうというジレンマを抱えている。<br>


<a id="DCGAN"></a>

## ■ DCGAN [Deep Convolutional GAN]
DCGAN は、GAN における生成器と識別器の内部を、「”プーリング層なし”の全て畳み込み層で構成される CNN 」に置き換えたものである。<br>

CNN は、画像認識タスクにおいて、優れたモデルになっているが、DCGAN もまた、画像生成において、（通常の GAN や他の生成モデルに比べて）優れたモデルになっている。<br>


<a id="DCGANのアーキテクチャ"></a>

### ◎ DCGAN のアーキテクチャ
![image](https://user-images.githubusercontent.com/25688193/57172563-c0129280-6e5c-11e9-941a-dc29501e74a3.png)<br>

![image](https://user-images.githubusercontent.com/25688193/57172588-226b9300-6e5d-11e9-8c40-d7e348a3debe.png)<br>

上図は、DCGAN の全体のアーキテクチャ（ネットワーク構成は略式表記）と、生成器・識別器内部の詳細なネットワーク構成を示したアーキテクチャ図である。<br>
DCGAN における生成器と識別器の構造は、以下のような特徴を持つ。<br>

➀ 生成器：Generator<br>

- 通常の CNN とは異なり、プーリング処理を行わない。（プーリング層は存在しない）<br>
- deconvolution（逆畳み込み）を用いて、アップサンプリングする。<br>
    ※ deconvolution という言葉は別の他の手法で既出のため、厳密には、deconvolution ではなく fractionally-strided convolution 或いは transposed convolution である。<br>
- 活性化関数として、Relu を使用。但し、出力層の活性化関数は tanh（※通常の GAN の generator と同じ）<br>
- batch normalization で学習させる。<br>
    但し、全ての層に適用されると学習が不安定となってしまうため、generator の出力層には適用しないようにする。<br>

➁ 識別器：Discriminator<br>

- 通常の CNN とは異なり、プーリング処理を行わない。（プーリング層は存在しない）<br>
- その代わりに、ストライド幅 2 の畳み込みで、ダウンサンプリングする。<br>
- 全結合層 [fully connected layer] を取り除き、代わりに、GAP : global average pooling で置き換える。<br>
    即ち、１つの特徴マップに対し、それらの平均をとって、１つの出力ノードに対応させる。（下図）<br>
    ![image](https://user-images.githubusercontent.com/25688193/57119115-c3494800-6da2-11e9-89cb-1e6c8640683c.png)<br>

- 通常の GAN の discriminator とは異なり、全ての層の活性化関数として、<br>
    Leaky ReLU (![image](https://user-images.githubusercontent.com/25688193/57119196-436fad80-6da3-11e9-9870-7db07a0f43e6.png)) を使用する。<br>
    これは、通常の Relu では、x≦0 の領域で、勾配が 0 になるために、学習を進めることが出来なくなってしまうのに対し、Leaky Relu では、x≦0 の領域でも、勾配が 0 とはならないので、学習を進めることが出来るようになることに起因する。<br>
    ![image](https://user-images.githubusercontent.com/25688193/57119284-e3c5d200-6da3-11e9-9c5f-c304098cd6ef.png)<br>

- batch normalization で学習させる。<br>
    但し、全ての層に適用されると学習が不安定となってしまうため、discriminator の入力層には適用しないようにする。<br>


<a id="DCGANの適用例"></a>

### ◎ DCGAN の適用例

- [DCGAN を利用した手書き文字（MNIST）の自動生成](https://github.com/Yagami360/MachineLearning_Exercises_Python_PyTorch/tree/master/GAN_DCGAN_PyTorch)


<a id="ConditionalGAN（cGAN）"></a>

## ■ Conditional GAN（cGAN）
従来の GAN では、潜在変数としての入力ノイズ z の値を動かすことで、様々な種類の画像を生成することができた。<br>
しかしながら、画像の生成過程において、今生成している画像がどのような種類（例えば、MNISTでは数字の番号）の画像であるのかを、GAN自身が知ることはできないかった。<br>

Conditional GAN（cGAN）は、従来の GAN の入力に、生成する画像のクラスラベルなどの条件 y を付与することにより、GAN自身に、今どのような種類の画像を生成しているのか？といった、データの生成過程を指示することを可能にし、結果として、条件 y で指定した特定の画像のみを生成することが出来るようにする。<br>


<a id="cGANのアーキテクチャ"></a>

### ◎ cGAN のアーキテクチャ
![image](https://user-images.githubusercontent.com/25688193/57116464-42815080-6d90-11e9-8d21-db015d449e10.png)<br>

上図は、cGAN のアーキテクチャを示した図である。<br>
cGAN では、従来の GAN に対して、生成器の入力と識別器の入力の双方に、正解となるクラスラベルなど条件 y を入力している。<br>
※ このアーキテクチャ図では、代表的な条件 y として、クラスラベルとしているが、他にも様々な条件（アノテーションなど）を入力することが出来る。<br>

但し、この例では、GANのアーキテクチャに対応できるように、<br>

- 識別器へのクラスラベル y の入力は、one-hot エンコードされた画像データのフォーマット（正解画像を白画像、それ以外を黒画像）で入力し、
- 生成器へのクラスラベル y の入力は one-hot エンコードされたバイナリデータの系列のフォーマット（正解を0、それ以外を０）で入力している。

※ cGAN のアルゴリズム自体には、このようなエンコード手法などの具体的な含まれないことに注意。<br>
（従来のGANのアーキテクチャに組み込める形であれば、どのような方法でもよい。）<br>


<a id="cGANの損失関数"></a>

### ◎ cGAN の損失関数
cGAN の生成器と識別器の損失関数は、従来のGANの損失関数に対して、クラスラベルなどの追加条件 y の制約を加えた式（＝条件付き確率）で表現できる。<br>
即ち、識別器の損失関数 L_D  は、<br>
![image](https://user-images.githubusercontent.com/25688193/56723413-dd23d300-6783-11e9-86e3-50c8aaff09ea.png)<br>

生成器の損失関数 L_G  は、<br>
![image](https://user-images.githubusercontent.com/25688193/56723450-f0cf3980-6783-11e9-9362-302766cc6114.png)<br>

そして、先のアーキテクチャ図で示しているように、従来の GAN と同じく、これらの損失関数を誤差逆伝搬法で逆伝搬することで、生成器と識別器の学習を逐次行う。<br>


<a id="cGANの適用例"></a>

### ◎ cGAN の適用例

- [cGAN を利用した手書き文字（MNIST）の自動生成](https://github.com/Yagami360/MachineLearning_Exercises_Python_PyTorch/tree/master/GAN_cGAN_PyTorch)


<a id="WGAN"></a>

## ■ WGAN [Wasserstein GAN]
既存のGANにおける（識別器の）学習は、先で見たように、JSダイバージェンスを最小化していることと同値であった。<br>
しかしながら、このJSダイバージェンスの最小化に基づく学習では、以下のような問題点が存在する。（詳細は後述）<br>

- 真の分布 ![image](https://user-images.githubusercontent.com/25688193/56492380-9a18f400-6526-11e9-8d35-7687a906526f.png) とモデルの分布 ![image](https://user-images.githubusercontent.com/25688193/56492448-d0ef0a00-6526-11e9-8c9b-79fb36a75af0.png) の台（Supp）が重ならない場合において、勾配損失問題が発生する。<br>

- 学習回数に対する loss 値の変化の様子が、生成画像のクオリティに比例しない。（＝生成画像のクオリティが損失関数の値から判断しずらい）<br>

- モード崩壊が発生することがある。

- 損失関数の収束性に問題があり、学習が不安定

そこで、Wasserein GAN では、JSダイバージェンスではなく、Earth-Mover 距離（EMD）（＝Wassertein距離ともいう）と呼ばれる別の距離指標を使用する。
（※厳密には、このWassertein距離の双対表現の式を近似した式）<br>
これにより、既存の GAN（＝JSダイバージェンス最小化）で発生する上記の問題を回避することが出来る。<br>


<a id="JSダイバージェンスとEarth-Mover距離の収束性の違いと勾配消失問題"></a>

### ◎ JSダイバージェンスとEarth-Mover距離の収束性の違いと勾配消失問題
概略で挙げたように、従来のGANにおける問題点の１つは、<br>

- 真の分布とモデルの分布の台が重ならない場合において、勾配消失問題が発生する。<br>

という問題である。<br>
ここでは、この問題点の詳細と、WGANにおける解決策を見ていく。<br>

![image](https://user-images.githubusercontent.com/25688193/56465595-60b78a00-643b-11e9-841d-e4e704f29b57.png)<br>

GANでの学習の目的は、真の分布 ![image](https://user-images.githubusercontent.com/25688193/56492380-9a18f400-6526-11e9-8d35-7687a906526f.png) とモデルの分布 ![image](https://user-images.githubusercontent.com/25688193/56492448-d0ef0a00-6526-11e9-8c9b-79fb36a75af0.png) の２つの確率分布を互いに一致させることであるが、この２つの確率分布の台（Suup）が、上図のように互いに重ならない場合を考える。<br>
※ θ は、生成器のニューラルネットワークの重みパラメーター<br>

このような２つの確率分布の台が交わらない極端なケースの例として、以下のような図のようなケースで、JSダイバージェンスを計算する。<br>

![image](https://user-images.githubusercontent.com/25688193/56465614-f18e6580-643b-11e9-9ab3-5d67522c4e75.png)<br>

![image](https://user-images.githubusercontent.com/25688193/56465620-04089f00-643c-11e9-8eeb-be4c269fd7ce.png)<br>

この計算結果を図示すると、以下のような図となる。<br>

![image](https://user-images.githubusercontent.com/25688193/56465640-4c27c180-643c-11e9-8103-8dcc5f8afaba.png)<br>

真の分布とモデルの分布が重なる θ=0 の部分で、JSダイバージェンスの値が不連続となっている。一方、真の分布とモデルの分布が重ならない θ≠0 の部分では、JSダイバージェンスの値が一定値となっていることが分かる。<br>
※ この様子を確率分布の系列 ![image](https://user-images.githubusercontent.com/25688193/56465651-842f0480-643c-11e9-97f1-4947a7f7399a.png) で考えると、この系列は、JSダイバージェンスのもとで、θ_t→0 で収束していないとも言える。<br>

ここで、GANの目的は、真の分布とモデルの分布を一致させるでことあり、これをJSダイバージェンスの最小化で行っていた。<br>
しかしながら、上図では、学習の余地がある、真の分布とモデルの分布が重ならない θ≠0 の部分において、JSダイバージェンスの値が一定値となっているために、その勾配の値が０になっていまうために、結果として、JSダイバージェンスを最小化して、誤差逆伝搬で学習するのに必要な勾配値が得られないという、いわゆる勾配消失問題が発生していることが分かる。<br>

このように、真の分布とモデルの分布の台（Supp）が互いに重ならない場合において、従来のGANのJSダイバージェンスの最小化による学習では、勾配消失問題が発生してしまう。<br>

<br>

Wassertein GAN では、この勾配消失問題を回避するために、Earth-Mover距離（Wassertein距離）を使用するが、このEarth-Mover距離（Wassertein距離）は、以下のように定義される。<br>
（※ Earth-Mover距離 の詳細は、「[【補足】Earth-Mover 距離（Wassertein距離）](https://github.com/Yagami360/My_NoteBook/blob/master/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6_%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92_%E7%94%9F%E6%88%90%E3%83%A2%E3%83%87%E3%83%AB.md#EMD)」参照のこと）<br>

![image](https://user-images.githubusercontent.com/25688193/56477963-17704480-64e6-11e9-9396-a72c7cae56af.png)<br>

このEarth-Mover距離を、先のJSダイバージェンスと同様にして、２つの確率分布の台が交わらない極端なケースで計算すると、<br>
![image](https://user-images.githubusercontent.com/25688193/56480315-7b016e80-64f4-11e9-860e-fd3963c90c7a.png)<br>
となる。<br>
この計算結果を図示すると、以下のような図となる。<br>
![image](https://user-images.githubusercontent.com/25688193/56480337-8eacd500-64f4-11e9-908a-cde46f6e8743.png)<br>

真の分布とモデルの分布が重なる θ=0 の部分で、EMDの値が０なっている。
一方、真の分布とモデルの分布が重ならない θ≠0 の部分では、EMDの勾配が一定値となっていることが分かる。<br>
つまり、学習が完了していない、真の分布とモデルの分布が重ならない領域において、誤差逆伝搬での学習に必要な、EMDの勾配値が得られているので、勾配消失問題は発生していないことが分かる。<br>


<a id="WGANのアーキテクチャと損失関数"></a>

### ◎  WGANのアーキテクチャと損失関数
先でみたように、Wassertein GAN では、以下のように定義される、Earth-Mover距離（Wassertein距離）を使用する。<br>

![image](https://user-images.githubusercontent.com/25688193/56480424-f82ce380-64f4-11e9-9952-d4f72fbd4aaa.png)<br>

但し、この式では、GANで扱うような画像の次元が大きい場合に、現実的に計算可能ではないので、この式の双対表現の式を利用する。<br>
※ このEarth-Mover距離を計算することは、線形計画法を解くことに一致するので、元の式（主問題）に対する双対表現が得られる。<br>

即ち、<br>
![image](https://user-images.githubusercontent.com/25688193/56480447-11359480-64f5-11e9-9ff8-71541144ff8d.png)<br>

ここで、関数 ![image](https://user-images.githubusercontent.com/25688193/56480469-30342680-64f5-11e9-8519-a93d37cfc1ba.png) は、K-リプシッツ連続な関数であることが、元のEarth-Mover距離の制約条件から要請される。<br>

![image](https://user-images.githubusercontent.com/25688193/56483115-c28ef700-6502-11e9-9d55-67132cc24464.png)<br>

また、このリプシッツ連続性の条件は、先の２つの確率分布の台が互いに重ならない場合で発生する勾配消失問題でみたように、一定の勾配が得られて学習がうまくいくための要件になっているとみなすことも出来る。（上図参照）<br>

![image](https://user-images.githubusercontent.com/25688193/57116586-10242300-6d91-11e9-9d96-d2e4683c6c6d.png)<br>

WGANでは、上図のアーキテクチャ図のように、このリプシッツ連続な関数 f を、
ニューラルネットワークで構成されたクリティック（＝従来の識別器）で表現する。<br>

※ このクリティックは、従来のGANの識別器のように、入力データが本物か偽物かの 0 or 1 の２値を sigmoid 活性化関数で活性出力するのではなく、リプシッツ連続な関数 f の出力（＝連続値になる）をそのまま出力する。そのため、もはや従来の識別器とは異なる機能となっているため、識別器ではなくクリティックという。<br>

このとき、クリティックのニューラルネットワークの重みパラメーターを w とし、この関数 f をパラメーター付きの関数![image](https://user-images.githubusercontent.com/25688193/56492322-5de59380-6526-11e9-87de-75ae8e5916db.png)で表記すると、先の Earth-Mover 距離の双対表現の式は、以下の式で近似できる。（導出略）<br>

![image](https://user-images.githubusercontent.com/25688193/56483339-4d242600-6504-11e9-9523-f94d78cbe79f.png)<br>

そしてWGANでは、クリティックの損失関数 ![image](https://user-images.githubusercontent.com/25688193/56492511-fd0a8b00-6526-11e9-8ed3-38e305f10c56.png) として、このEarth-Mover 距離の双対表現の近似式で採用する。<br>
そして、上記のアーキテクチャ図で示したように、この勾配を誤差逆伝搬することで、学習を行う。<br>


<a id="WGANでのその他の工夫"></a>

### ◎ WGAN でのその他の工夫

- クリティックの学習と生成器の学習の更新間隔：<br>
    従来のGANでは、損失関数（＝JSダイバージェンス）が途中で勾配消失するために、ある生成器の出力に対して、最適途中までしか識別器を学習させることしかできず、そのために、<br>
    「識別器の損失関数を更新（１回）→生成器の損失関数の更新（１回）」 を、各々１回ずつ繰り返すことで、生成器と識別器の学習を逐次行っていた。<br>

    一方、先に述べたように、WGANの損失関数（＝EMD）では、ほとんどいたるところで連続で微分可能あり、誤差逆伝搬での学習のための勾配が得られ続けるので、ある生成器の出力に対して、クリティックの最適状態まで、学習を続けることが可能である。<br>

    従って、WGANでは、正しい損失関数を計算するために<br>
    「識別器の損失関数を更新（n_critic=5 回）→生成器の損失関数の更新（１回）」というように繰り返すことで、生成器と識別器の学習を逐次行なう。<br>
    これにより、安定した学習や、モード崩壊の防止を実現できる。<br>

- リプシッツ連続な関数 f の実現と重みクリッピング：<br>
    ここで、何度も述べているように、クリティックの出力となる関数![image](https://user-images.githubusercontent.com/25688193/56492322-5de59380-6526-11e9-87de-75ae8e5916db.png)は、∀w∈W に対して、リプシッツ連続な関数である必要があるが、WGANでは、このことをニューラルネットワークで実現するために、単純に、−c≤w≤c (例えば c=0.01)  の範囲で重みクリッピングを行うことで実現する。<br>

    ※ 但し、この重みクリッピングによる方法では、勾配爆発や、低い値にクリッピングしすぎることによる勾配消失の問題が発生する可能性がある。<br>
    ※ 後述の WGAN-gp では、このような問題が起らないように、重みクリッピングによるリプシッツ連続性の実現ではなく、勾配ノルムに制限項を加えることで、リプシッツ連続性を実現する。（詳細は後述）

- 最適化アルゴリズムの選択：<br>
	最適化アルゴリズムとして、Adam のようなモーメンタムベースの最適化アルゴリズムを使用すると、ときどき学習が不安定になるという実験的結果がある。（※ この理論的根拠はまだない？）v
	そのため論文では、最適化アルゴリズムとして、モーメンタムベースのアルゴリズムではない RMSProp を採用している。<br>


<a id="WGANのアルゴリズム"></a>

### ◎ WGAN のアルゴリズム
以上のことを踏まえて、最終的に WGAN のアルゴリズムは、以下のようになる。<br>

![image](https://user-images.githubusercontent.com/25688193/56484127-f40ac100-6508-11e9-92c7-e1581e138ae0.png)<br>


<a id="WGANのその他の利点"></a>

### ◎ WGAN のその他の利点
先に述べたように、WGANでは、従来の GAN で問題になっていた、（真の分布とモデルの分布の台が互いに重ならない場合に発生する）勾配消失問題を回避することが出来るが、その他にも以下に上げるようないくつかの利点が存在する。<br>

※ 勾配消失問題の回避を含めたこれらの利点の原因は、互いに独立したものではなく、いづれもWGANの損失関数（＝EMDの近似）の連続性やクリティックの出力のリプシッツ連続性に起因していることに注目。<br>

- 学習が安定している：<br>
	先に述べたように、WGANの損失関数（＝EMD）では、ほとんどいたるところで連続で微分可能あり、誤差逆伝搬での学習のための勾配が得られ続けるので、ある生成器の出力に対して、クリティックの最適状態まで、学習を続けることが可能である。<br>
	従って、WGANでは、正しい損失関数を計算するために、<br>
	「識別器の損失関数を更新（n_critic  回）→生成器の損失関数の更新（１回）」　というように繰り返すことで、生成器と識別器の学習を逐次行うことが可能となり、<br>
	従来のGANでの、「識別器の損失関数を更新（１回）→生成器の損失関数の更新（１回）」 での方式より、学習が安定化する。<br>

- モード崩壊が発生しない：<br>
	モード崩壊は、十分に最適化されたいない識別器に対して、生成器を最適化し、画像を生成することで発生する。<br>
	先に述べたように、WGANの損失関数（＝EMDの近似）では、ほとんどいたるところで連続で微分可能あり、誤差逆伝搬での学習のための勾配が得られ続けるので、ある生成器の出力に対して、クリティック（＝識別器）の最適状態まで、学習を続けることが可能である。<br>
	従って、モード崩壊の発生原因である、十分に最適化されたいないクリティック（＝識別器）という状況が発生しないため、モード崩壊も発生しない。<br>

- 生成画像のクオリティを loss値から判断できる：<br>
    従来のGANにおいては、学習の経過において、loss 値が減少したからといって、必ずしも生成画像のクオリティが良くなるとは限らない。<br>
	そのため、良いクオリティの生成画像を得るために、学習をいつ止めるべきなのかを目視していなくてはいけないという問題が存在した。<br>
    <br>
    一方 WGAN では、学習回数に対する loss 値の変化の様子が、生成画像のクオリティに比例することが、実験的に示されている。（理論的には？）<br>
    そのため、良いクオリティの生成画像を得るために、生成画像が都度目視せずとも、学習の完了（＝loss値の０付近への収束）まで待つだけでよい。<br>
    このことを示したのが、以下の図である。<br>
    ※ この性質は、理論的には、クリティックの出力が、リプシッツ連続で、ほとんどいたるところで線形な関数で表現できていることに起因？

    ![image](https://user-images.githubusercontent.com/25688193/56484634-84e29c00-650b-11e9-8174-9c5167e1010c.png)<br>


<a id="WGANの適用例"></a>

### ◎ WGAN の適用例

- [WGAN を利用した手書き文字（MNIST）の自動生成](https://github.com/Yagami360/MachineLearning_Exercises_Python_PyTorch/tree/master/GAN_WGAN_PyTorch)


<a id="pix2pix"></a>

## ■ pix2pix
従来の image-to-image 変換タスク（＝１対１に対応している画像間を変換すること）では、タスクの種類に応じて個別にアルゴリズムを構築していた。<br>
pix2pix は、image-to-image 変換タスクを、cGAN を用いて実現することにより、様々な種類の image-to-image 変換タスクを、汎用的なフレームワークで実現出来る。<br>
※ 通常の GAN では、生成タスクが主な用途であるが、pix2pix はこのような image-to-image の変換タスクが主な用途である。<br>

具体的には、以下の例のような、image-to-image 変換タスクを共通のフレームワークで実現できる。<br>

- 航空写真から地図を生成<br>
    ![image](https://user-images.githubusercontent.com/25688193/57005997-df6cae00-6c17-11e9-88c7-e664288a75a8.png)<br>

- 輪郭線を画像で埋める<br>
    ![image](https://user-images.githubusercontent.com/25688193/57005999-e7c4e900-6c17-11e9-95f0-2d5d362318d6.png)<br>

- 画像をカラー化する<br>
    ![image](https://user-images.githubusercontent.com/25688193/57006008-fa3f2280-6c17-11e9-906c-f6ba9fe55d17.png)<br>

- 道路や建物の塗り分けから画像生成<br>
    ![image](https://user-images.githubusercontent.com/25688193/57006021-1a6ee180-6c18-11e9-805f-a4fe8ee47473.png)<br>


<a id="pix2pixのアーキテクチャ"></a>

### ◎ pix2pix のアーキテクチャ
![image](https://user-images.githubusercontent.com/25688193/57006356-11cbda80-6c1b-11e9-9f92-98c5ee2b175c.png)<br>

上図は、例えば、航空写真を地図に変換する image-to-image 変換タスクにおける、pix2pix のアーキテクチャ図を示した図である。<br>
この pix2pix のアーキテクチャには、主に、以下のような特徴がある。<br>

- アーキテクチャのベースとしては、cGAN を採用：<br>
    cGAN のアルゴリズムにおいて、入力画像 x を変換前の画像（航空写真）とし、追加条件 y として、変換後の画像（地図画像）とすることで、cGAN での image-to-image 変換タスクを実現している。<br>

- 生成器 G のネットワークとしては、U-Net を採用：<br>
    - U-Net は、セマンティックセグメンテーションのタスクにおいて重要となる「局所的な特徴量と、画像全体の特徴の両方を捉えること」 を同時に実現できる。
    - より詳細には、Encoder 側の浅い層から、画像全体の大域的な特徴量を skip connection 経由で、Decoder 側に送り、Encoder 側の深い層からの、画像の局所的な特徴量を skip connection 経由で、Decoder 側に送る。<br>
        そして、Decoder 側で、これら skip connection で送られてきた大域的特徴量と局所的特徴量を保持したたま、アップサンプリングを行い、変換前と同じ解像度の画像を出力する。<br>
        結果として、「局所的な特徴量と、画像全体の特徴の両方を捉えること」 を同時に実現できる。（詳細は、[【補足】U-Net](https://github.com/Yagami360/My_NoteBook/blob/master/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6_%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92_%E7%94%9F%E6%88%90%E3%83%A2%E3%83%87%E3%83%AB.md#UNet) の項目参照）
    - pix2pix では、生成器のネットワーク構成として、U-Net を採用することで、このセマンティックセグメンテーションのタスクにおいて重要となる「局所的な特徴量と、画像全体の特徴の両方を捉えること」 を同時に実現できるようにしている。<br>

- 識別器 D には、Patch GAN を採用：<br>
    - Patch GAN では、識別器に入力された画像をより小さな（＝局所的な）複数の小領域（＝パッチ）に分解した上で、これら各バッチに対して、本物か偽物かの判定を行い、最後に、全ての応答を平均化して、識別器の最終的な出力とする。<br>
    - この Patch GAN の仕組みにより、識別器がある程度大域的な判定を残しつつ、局所的な特徴量でのみ判定に専念できるので、学習パラメーター数を大幅に減らすことができ、結果として、学習を効率的に進めることができる。<br>
    - 尚、Patch GAN という名前がついているが、この PatchGAN 自体は、GAN 全体のアルゴリズムではなく、単に、識別器のみに対してのアルゴリズムであることに注意。<br>

- 生成器 G に入力する入力ノイズ z は、dropout で実現：<br>
    - 生成器 G に入力する入力ノイズ z は、従来の GAN のように、確率分布 U(0,1)  or N(0,1)  から直接サンプリングして実現するのではなく、生成器のネットワークの複数の中間層に、直接 dropout を施すという意味でのノイズとして実現する。<br>

このようなアーキテクチャのもとで、識別器 D は、入力されたデータのペアが、学習データに含まれている｛変換前の画像（本物の航空写真）, 変換後の画像（地図）｝というペアなのか、或いは、｛ 変換元画像から生成器が生成した画像（偽物の航空写真）、変換後の画像（地図）＞ のペアなのかという判断を行う。<br>

そして、従来の GAN と同じように、この識別器による判定結果が、50% - 50% になるように、生成器と識別器の学習を、交互に実施していく。


<a id="pix2pixの損失関数"></a>

### ◎ pix2pix の損失関数
pix2pix は、cGAN をベースに構築されているが、cGAN の識別器の損失関数 L_D と生成器の損失関数 L_G は、以下のように定義された。<br>

![image](https://user-images.githubusercontent.com/25688193/57006110-0d062700-6c19-11e9-908c-39d457fe628a.png)<br>

pix2pix でのアーキテクチャにそうように書き換えると、<br>

![image](https://user-images.githubusercontent.com/25688193/57006122-21e2ba80-6c19-11e9-9709-e40b49281733.png)<br>

pix2pix では更に、生成器での損失関数 L_G に対して、この cGAN の損失関数をベースにして、L1正則化の項<br>

![image](https://user-images.githubusercontent.com/25688193/57006135-42ab1000-6c19-11e9-9092-821fde5aca8a.png)<br>
を追加する。<br>
これにより、生成器での損失関数は、以下のようになる。<br>

![image](https://user-images.githubusercontent.com/25688193/57010909-1784e880-6c3a-11e9-9403-08c3ac3490f8.png)<br>

この L1正則化の項目は、変換後の本物画像 x と生成器が生成した偽物画像 ![image](https://user-images.githubusercontent.com/25688193/57006148-6d956400-6c19-11e9-9af3-d6860b30f3dc.png) が、”ピクセル単位” でどの程度異なるのか（＝局所的な特徴量）を表している。<br>
セマンティックセグメンテーションのタスクにおいては、「局所的な特徴量と、画像全体の特徴の両方を捉えること」 が重要となる。<br>
pix2pix では、既存の cGAN の損失関数に、L1正則化項を追加することにより、
局所的な特徴量をL1正則化で捉え、全体的な情報の正しさを識別器で捉え判定することを可能にしている。<br>
※ 同じような理由で、L2正則化も考えられるが、L2よりL1のほうが、生成画像のぼやけが少ない傾向があるので、pix2pix では、L1を採用している。<br>


<a id="pix2pixの適用例"></a>

### ◎ pix2pix の適用例

- [pix2pix を利用して、衛星画像から地図を生成する。](https://github.com/Yagami360/MachineLearning_Exercises_Python_PyTorch/tree/master/Pix2Pix_PyTorch)


<a id="ProgressiveGAN（PGGAN）"></a>

## ■ ProgressiveGAN（PGGAN）
GAN において、高解像度の画像生成が困難で有ることの理由としては、以下のような要因が考えられる。<br>

- 高解像度の画像では、（低解像度のときに比べて画像の詳細が分かるので、）正解画像と偽物画像の識別が簡単なタスクとなる。そのため、生成器と識別器の学習が十分に行えない。

- 高解像度の画像を扱うのにより多くのメモリを消費するので、より小さなミニバッチサイズで学習を行う必要がある。その結果として、学習が更に不安定になる。

- 学習に時間がかかる。

ProgressiveGAN では、このような GAN における高解像度の画像生成における問題点を回避することで、高解像度の画像生成を可能にしている。<br>
その基本的なコンセプトは、<br>
「生成器と識別器を、より簡単な低解像度の画像から段階的に学習させて、学習の段階が進むにつれて、高解像の情報をもつ新しい層を追加していく」<br>
というものである。<br>
これにより、上記の問題の回避と、学習の劇的な高速化が可能となる。<br>


<a id="ProgressiveGANのアーキテクチャ"></a>

### ◎ ProgressiveGAN のアーキテクチャ
![image](https://user-images.githubusercontent.com/25688193/58148372-7964cb00-7c99-11e9-8606-43859a669453.png)<br>
![image](https://user-images.githubusercontent.com/25688193/58149636-b3849b80-7c9e-11e9-9abd-2668e344b2ef.png)<br>

上図は、PGGAN のアーキテクチャ全体を段階的な（Progressiveな）ネットワーク構成と学習と共に示した図である。<br>
また上表は、このアーキテクチャにおける各種ネットワークの設定値を示した表である。<br>
この PGGAN のアーキテクチャには、主に、以下のような特徴がある。<br>

- Progressive なネットワークの構成と学習（Progressive growing of GANs）<br>
	PGGAN の基本的なコンセプトは、概要で述べたように、「生成器と識別器を、より簡単な低解像度の画像から段階的に学習させて、学習の段階が進むにつれて、高解像の情報をもつ新しい層を追加していく」 というものである。<br>
	PGGAN ではこれを実現するためのアーキテクチャとして、上図のようなアーキテクチャを採用している。<br>
    このアーキテクチャでの処理は、以下のような段階的な（Progressive な）処理で行われる。<br>

    1. Training Progresses 1<br>
        まず、4×4の低解像度で、従来のGANの構造に従って、学習が安定化するまで十分に生成器と識別器を学習する。<br>
    1. Training Progresses 2<br>
        次に、生成器にアップサンプリングを行う２つの逆畳み込み層が追加され、識別器にダウンサンプリングを行う２つの畳み込み層が追加されたネットワークで、8×8 の解像度で、学習が安定化するまで十分に生成器と識別器を学習する。<br>
    1. Train Progresses 3~9<br>
		同様の処理を、16×16 → 32×32 → 64×64 → ・・・ の解像度と、それに対応するために追加されるアップサンプリングorダウンサンプリング用の畳み込み層で段階的に行っておき、最終的には、1024×1024 の解像度を生成出来るように学習していく。<br>


- 滑らかな解像度の変換：<br>
	PGGAN では、上で述べたように、画像の解像度を２倍 or 1/2 倍の倍々でアップサンプリング or ダウンサンプリングしていくが、例えばある解像度で十分に学習されたネットワークであっても、この倍々処理を急激に行うと、非滑らかな変化を生じさせてしまう。<br>
	そのため、PGGAN では、アップサンプリング or ダウンサンプリングのための新しい畳み込み層をネットワークに追加するときに、画像の RGB 値を線形補間によって、滑らかに変化させるようにする。<br>

    ![image](https://user-images.githubusercontent.com/25688193/58154504-bab2a600-7cac-11e9-8b12-c353e2461cf8.png)<br>

    上図は、この線形補間による滑らかな解像度の変換の様子を、16×16 → 32 × 32 での切り替え時の例で示した図である。<br>

    - (a) : 学習開始時の様子。特に線形補間は行われない。（α=0）<br>
    - (b) : 学習途中での様子。0<α<1 を補間係数として、<br>
        生成器側（G）で、特徴ベクトルから変換したRGB情報（toRGB）を、それぞれ ![image](https://user-images.githubusercontent.com/25688193/58154830-81c70100-7cad-11e9-8723-e3cf34e02cc4.png) で混ぜ合わせる。<br>
        また、識別器側（D）で、RGBから再変換された特徴ベクトル（fromRGB）を、それぞれ ![image](https://user-images.githubusercontent.com/25688193/58154903-a622dd80-7cad-11e9-8f24-bad3dc928d96.png) で混ぜ合わせる。<br>
        そして、学習が進むに連れて α = 0 → 1 とすることで、滑らかな線形補間を行う。<br>
    - (c) : 学習完了時の様子。線形補間も完了している。（α=1）<br>


<a id="生成画像の多様性向上とモード崩壊防止のための工夫"></a>

### ◎ 生成画像の多様性向上とモード崩壊防止のための工夫
PGGAN では、前述のような高解像度の画像生成のための工夫だけでなく、生成画像の多様性向上とモード崩壊防止のための工夫も提案している。

- 簡略化された minibatch discrimination での生成データの多様性の向上とモード崩壊の防止：<br>
	GAN における課題の１つに、モード崩壊というものがある。<br>
	minibatch discrimination では、このモード崩壊を防止するために、識別器にミニバッチデータ内のデータの多様性を検出出来る仕組みを導入している。<br>
    （※ minibatch discrimination についての詳細は、後述の [「【補足】 Minbatch discrimation」](https://github.com/Yagami360/My_NoteBook/blob/master/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6_%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92_%E7%94%9F%E6%88%90%E3%83%A2%E3%83%87%E3%83%AB.md#Minibatchdiscrimination) を参照のこと。）<br>

	PGGAN では、この minibatch discrimination の仕組みを大幅に簡略化した仕組みを導入することで、minibatch discrimination のように、新しいハイパーパラメータを必要とすることなく、生成データの多様性を向上させ、モード崩壊を防止している。<br>

    以下の図は、この簡略化された Minibatch discrimation での全体的な処理の流れを示した図である。<br>

    ![image](https://user-images.githubusercontent.com/25688193/58143545-246b8980-7c86-11e9-878f-dd0d03e276fd.png)<br>
    ![image](https://user-images.githubusercontent.com/25688193/58155038-09ad0b00-7cae-11e9-9d56-87474eb97c16.png)<br>

    上図に示した、簡略化された Minibatch discriminator における、各処理の詳細は、以下のようになる。<br>

    1. ミニバッチデータ内の各画像データの各ピクセル値（RGB値）に対して、同じ位置に対応しているピクセル値（RGB値）で、標準偏差を計算する。<br>
        計算した標準偏差は、H×W×C 分あるので、これを各ピクセルが、標準偏差値となるような H×W×C のテンソルとする。<br>
        ※ このテンソルは、ある画像とミニバッチ内でのその他の画像間のピクセル単位での標準偏差となっているが、これは見方を変えると、ミニバッチ内でのデータの多様性を示していることになる。従って、この処理により、識別器にデータの多様性を認識できる構造が入っていることになる。<br>

    2. 全ての高さ（H）と幅（W）、チャンネル数（C）で、この各ピクセル（＝標準偏差値）を平均化し、スカラー値を取得する。<br>

    3. 得られるスカラーをコピーし、結合することで、１つのH×Wのテンソル（＝特徴マップ）を取得する。<br>


<a id="学習の安定化のための工夫"></a>

### ◎ 学習の安定化のための工夫
一般的に、GAN においては、学習を安定化させることを目的として、batch normalization の仕組みがネットワークに導入されている。<br>
しかしながら、この PGGAN では、GAN において batch norm による学習安定化効果は見られないとして、別の学習安定化のための工夫を使用している。<br>
※ batch norm が GAN において学習安定化効果があるか否かは議論の余地がある。<br>
※ batch norm は、共変量シフトを取り除く効果があるという議論もあるが、これにも議論の余地がある。<br>

batch norm を用いない PGGAN の学習安定化（と学習速度向上）のための工夫は、以下の２つの仕組みから構成される。<br>
尚、この２つの手法ともに、学習可能なハイパーパラメータを含んでいない。<br>

- Equalized learning rate<br>
    一般的には、ディープラーニングにおいては、学習率が減衰するような最適化アルゴリズム（Adamなど）を用いて、ネットワークの重みを更新する。<br>
    PGGAN では、このような重みの更新ではなく、以下に示すようなもっと単純な方法で、ネットワークの重みを更新する。<br>

    1. 生成器と識別器のネットワークの各層（i）の重み w_i の初期値を、![image](https://user-images.githubusercontent.com/25688193/58157794-12a0db00-7cb4-11e9-92b7-70f6058f6ea4.png) で初期化する。<br>
    1. 初期化した重みを、各層の実行時（＝順伝搬での計算時）に、以下の式で再設定する。<br>
        ![image](https://user-images.githubusercontent.com/25688193/58157836-2ba98c00-7cb4-11e9-9b81-98a1d5aedccb.png)<br>

- Pixelwise feature vector normalization in generator<br>
    生成器と識別器の不適切な競争の結果として、生成器と識別器からの出力信号（＝偽物画像と判別結果）が制御不能になることを防止するために、<br>
    PGGAN では、生成器の各畳み込み層の後の、中間層からの出力（＝特徴ベクトル）に対して、”各ピクセル毎に”、以下のような特徴ベクトルの正規化処理を行う。<br>

    ![image](https://user-images.githubusercontent.com/25688193/58161289-f48aa900-7cba-11e9-8b45-0867b5d1f3ee.png)<br>

	この正規化手法により、（batch norm と同様にして、）学習の際の変化の大きなに応じて、信号を増幅 or 減衰出来るようになるので、結果として学習の安定化が得られる。<br>


<a id="GANの生成画像の評価"></a>

### ◎ GAN の生成画像の評価

- Multi-scale statistical similarity for assessing GAN results<br>
    > 記載中...


<a id="StyleGAN"></a>

## ■ StyleGAN
StyleGAN は、高解像度の画像を生成可能な ProgressiveGAN をベースラインとして、画像の画風変換（Style Transfer）の分野の知見を取り入れたアルゴリズムである。<br>

この StyleGAN のアーキテクチャは、従来の GAN の生成器とは、かなり異なるアーキテクチャになっているが、非常にクオリティの高い画像を、高解像度で生成出来る。<br>
又、単に高解像の画像を生成出来るだけでなく、人物画像の高レベルで大域的な属性（顔の輪郭、眼鏡の有無など）から局所的な属性（しわや肌質など）までを切り分けて、アルゴリズムで制御出来るようになっている。<br>

<a id="StyleGANのアーキテクチャ"></a>

### ◎ StyleGAN のアーキテクチャ
![image](https://user-images.githubusercontent.com/25688193/57349935-00dd1500-7197-11e9-9e03-08d12a33febf.png)<br>

上図は、StyleGAN の全体のアーキテクチャ（上段）と、生成器内部の詳細なアーキテクチャ（下段）を示した図である。<br>
この StyleGAN のアーキテクチャには、主に、以下のような特徴がある。<br>

- Style-Based Generator：<br>
    StyleGAN の生成器（Style-Based Generator）は、ProgressiveGAN の生成器をベースラインとして、画風変換（Style Transfer）の分野の知見を取り入れた、従来の GAN の生成器とは、かなり異なるアーキテクチャになっている。<br>
    具体的には、以下のような構造や特徴をもつ。（詳細は後述）<br>
    - Mapping network f
    - Adaptive Instance Normalization (AdaIN)
    - pixel-wise Noise Input と Stochastic variation
    - ProgressiveGAN から入力層を排除
    - Style Mixing

- Mapping network f（潜在空間 Z から潜在空間 W への写像と潜在表現の獲得）：<br>
    従来の GAN では、入力ノイズ z∈Z（Z:潜在空間）を、そのまま生成器に入力していた。
    しかしながら、この方法では、下図で示すように、画像の特徴量の一部の組み合わせが存在しないようなケースにおいて、入力ノイズ z が存在する潜在空間 Z は、線形ではなく entanglement（もつれ）のある歪んだ空間となる。<br>
    ![image](https://user-images.githubusercontent.com/25688193/57284435-1fd29d00-70ec-11e9-9fdd-a818745d4fa4.png)<br>

	このような entanglement（もつれ、歪み） のある空間では、潜在空間のある部分空間からサンプリングしたベクトル（＝上図では特徴軸上のベクトル）だけを使って画像を生成したときに、その生成画像において、複数の要素が同時に変動してしまう（例えば、性別や肌色などが同時に変わる）。<br>
    そのため、このような entanglement な潜在空間では、狙い通りの画像を生成するのが困難になってしまう問題が存在する。<br>

    ※ 一方で、 disentangle（解きほぐし）されている潜在空間では、潜在空間のある部分空間からサンプリングしたベクトル（＝上図では特徴軸上のベクトル）だけを使って画像を生成したときに、その生成画像において、変動する要素が一つのみである。（例えば、性別だけが変わる、肌色だけが変わる）<br>
    そのため、このような disentangle な潜在空間を獲得出来れば、意図している画像の生成が行いやすくなる。<br>

    このような問題に対応するために、StyleGAN では、この潜在変数である入力ノイズ z をまず、mapping network f という８層のMLPから構成されるネットワークに入力し、中間的な潜在空間 W へと写像して、別の潜在変数 w∈W を獲得した上で、後段のネットワークに入力する。<br>
    ここで、mapping network は、学習されるネットワークであるので、これにより、この mapping network で写像された潜在変数 w は、下図で示すように、画像の特徴量の entanglement（もつれ）の少ない状態で最適化されていることが期待できる。<br>
    ![image](https://user-images.githubusercontent.com/25688193/57284625-8061da00-70ec-11e9-9cd9-780f2ebbeb6e.png)<br>

    尚、StyleGAN では、この mapping network f による潜在空間の entanglement（もつれ）の disentanglement （解きほぐし）の程度を、定量的に評価する指標として、新しい２つの指標（Perceptual path length と Linear separability）を提案している。（詳細は後述）<br>

    > ※ disentanglement と潜在表現の獲得：<br>
    > 直訳すると、（複雑に絡み合った問題の）”解きほぐし” の意味であるが、ここでの意味は、データのもつれを解く最適な表現方法を獲得する（disentanglement）こと。

- 各解像度スケールでの Adaptive Instance Normalization (AdaIN) の採用：<br>
    AdaIN は、Style Transfer の文脈では、Instance Normalization　の発展版で、1つのモデルであらゆるスタイル（画風）への変換を可能にした正規化手法である。<br>
    ※ 詳細は、「[【補足】 Style Transfer における正規化手法（INとAdaIN）](https://github.com/Yagami360/My_NoteBook/blob/master/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6_%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92_%E7%94%9F%E6%88%90%E3%83%A2%E3%83%87%E3%83%AB.md#StyleTransfer%E3%81%AB%E3%81%8A%E3%81%91%E3%82%8B%E6%AD%A3%E8%A6%8F%E5%8C%96%E6%89%8B%E6%B3%95%EF%BC%88IN%E3%81%A8AdaIN%EF%BC%89)」 の項目を参照）<br>

    ![image](https://user-images.githubusercontent.com/25688193/57350788-67affd80-719a-11e9-8c32-60bca8dcc128.png)<br>

    StyleGAN において、この AdaIn は、上図のように、Mapping Network によって得られた中間潜在変数 w に対して、アフィン変換（平行移動変換）を施した後に、このアフィン変換によるスケール値とバイアス値を、それぞれAdaIN の制御パラメーターであるスケール項 ![image](https://user-images.githubusercontent.com/25688193/57304365-28da6300-711a-11e9-8f25-19f6a89fd8ac.png) とバイアス項 ![image](https://user-images.githubusercontent.com/25688193/57304393-37287f00-711a-11e9-94bf-4b78ca958831.png) として用いるという用途で適用される。<br>
    ここで、AdaIN の式は、以下のような式で与えられる。<br>

    ![image](https://user-images.githubusercontent.com/25688193/57304514-722ab280-711a-11e9-8c90-bbbe3b0d973c.png)<br>

	この AdaIN の処理は、特徴マップ単位（＝チャンネル単位）での、正規化処理になっているが、この AdaIN による変換を、各解像度スケール（4×4、8×8、16×16、・・・、1024×1024）の畳み込みの後に行うことで、各解像度スケール（4×4、8×8、16×16、・・・、1024×1024）毎に、”画像全体に渡って大域的に”、スタイル（画風）を変化させることが出来るようになる。<br>

- ピクセル単位の Noise Input と Stochastic variation（確率的変動）：<br>
    人物画像の髪やシワなどは、確率的とみなせる細部の局所的な特徴生成として扱うことが出来る。<br>
	従来の GAN では、このような局所的な特徴生成も（大域的な特徴生成と同様にして）、潜在変数としての入力ノイズを直接生成器に入力することで実現する構造になっている。しかしこの方法では、上記の潜在空間の歪みにより、ノイズの影響を制御することが困難である。<br>

    ![image](https://user-images.githubusercontent.com/25688193/57350819-8615f900-719a-11e9-93c7-2584dc4b592e.png)<br>

    これに対して、StyleGAN では、上図のアーキテクチャ図のように、Synthesis Network g での各畳み込みの直後に、ノイズマップで、ピクセル単位のノイズを直接加えることでこれを実現する。<br>
    このノイズはピクセル単位で適用されるので、髪やシワなどの確率的な変動を、ピクセル単位で局所的に直接制御することが可能になる。<br>
    （※ これに対して、AdaIN は、特徴マップ単位で適用されるので、画風（スタイル）といった画像全体に渡っての大域的な属性を制御している。）<br>

    - ノイズの影響と効果：<br>
        ![image](https://user-images.githubusercontent.com/25688193/57357077-13624900-71ad-11e9-9d4e-dbd2939cfdeb.png)<br>
        上図は、この入力ノイズによる生成画像の効果を示した図である。<br>
        (a) ベースとなる画像<br>
        (b) ベースとなる画像はそのままで、それぞれノイズを変えたとき４つの画像<br>
        (c) ノイズの影響を受けている箇所をグレースケースで表示した画像（白部分が影響大、黒部分が影響小）<br>

        ノイズは、髪の毛という確率的とみなせる局所的な特徴のみを変化させ、顔の向きや輪郭などの、画像全体に渡っての大域的な特徴には影響を与えていないことが見てとれる。<br>
        <br>
        ![image](https://user-images.githubusercontent.com/25688193/57357222-6805c400-71ad-11e9-94eb-7bf64f50420a.png)<br>
        上図は、どの解像度スケールの層にノイズを加えるかでの生成画像の効果を示した図である。<br>
        (a) 全ての解像度スケールの層に、ノイズを加えた場合<br>
        (b) ノイズを加えない場合<br>
        (c) 高解像度の層（64×64 ~ 1024×1024）のみに、ノイズを加えた場合<br>
        (d) 低解像度の層（4×4 ~ 32×32）のみに、ノイズを加えた場合<br>

        ノイズを加えないと、全体的に均一で立体感のない画像が生成されている。<br>
        ノイズを加えると、肌質などの局所的で細かなテクスチャーが加味され、よりリアルな画像が生成されていることが見てとれる。<br>
        そして、ノイズを加える解像度スケールの層に応じて、これら局所的で細かなテクスチャーの、局所性や細かさの程度にも差が出ていることが見てとれる。<br>

- ProgressiveGAN から入力層を除外し、代わりに学習済み定数マップを入力：<br>
    StyleGAN のベースアルゴリズムである  ProgressiveGAN では、乱数入力で生成器への初期入力（4×4）を入力している。<br>

    ![image](https://user-images.githubusercontent.com/25688193/57351073-8793f100-719b-11e9-8eaa-93ceadb81aea.png)<br>

    一方、StyleGAN の生成器では、上図のアーキテクチャ図の Synthesis Network g の開始部分で示しているように、入力層を完全に排除し、そのかわりに、学習済み定数マップ（4×4×512）を入力の開始としている。<br>
    これは、StyleGAN が、生成画像を、前述の潜在変数 w と AdaIN、及びノイズマップによって制御しているために、可変な入力が不要であることによるものである。<br>

- Style Mixing：<br>
    ![image](https://user-images.githubusercontent.com/25688193/57352094-3d147380-719f-11e9-827c-53b9ed64c873.png)<br>

    Style Mixing は、上図のように潜在空間よりサンプリングした２つの潜在変数 ![image](https://user-images.githubusercontent.com/25688193/57352230-b1e7ad80-719f-11e9-9d7d-ca5763974256.png)、及び２つの中間潜在空間 ![image](https://user-images.githubusercontent.com/25688193/57352252-c4fa7d80-719f-11e9-9bab-768bace02642.png) に関して、Synthesis Network g に AdaIN のパラメーターとして入力する際に、ある解像度スケールまでは、z1,w1 を入力し、それ以降の解像度スケールには、z2, w2 を入力するように切り替えるという正規化手法である。<br>
    これにより、Synthesis Network g は、隣接した解像度スケールの層間で画風（スタイル）に相関があるように学習しなくなるので、画風（スタイル）の影響を各解像度スケールの層に局在化させることが可能となる。<br>

    - Style Mixing の効果<br>
        ![image](https://user-images.githubusercontent.com/25688193/57359949-efeecc80-71b3-11e9-963e-73812e4c6709.png)<br>

        上図は、１つの潜在変数 w1 から生成される画像（ソースA）と、別の潜在変数 w2 から生成される画像（ソースB）を、それぞれ縦軸横軸に置き、Style Mixing で潜在変数を w1 → w2 に切り替えたときの画像を示した図である。<br>
        上段の画像は、低解像度のスケール層（4×4 ~ 8×8）で、潜在変数を w1 → w2 に切り替えたときの画像。<br>
        中段の画像は、中解像度のスケール層（16×16 ~ 32×32）で、潜在変数を w1 → w2 に切り替えたときの画像。<br>
        上段の画像は、高解像度のスケール層（64×64 ~ 1024×1024）で、潜在変数を w1 → w2 に切り替えたときの画像。<br>

        低解像度のスケール層で StyleMixing した場合は、ソースBから、顔の向きや輪郭、メガネの有無などの、画像全体に渡っての大域的な特徴が引き継がれていることが見てとれる。<br>
        又、中解像度のスケール層で StyleMixing した場合は、ソースBから、顔の向きや輪郭、髪型などの、画像全体に渡っての大域的な特徴が引き継がれており、ソースAからは、肌質などの局所的な特徴が引き継がれていることが見てとれる。<br>
        又、高解像度のスケール層で StyleMixing した場合は、ソースBから、背景色や髪の色などの、ピクセル単位での局所的な特徴が引き継がれていることが見てとれる。<br>


<a id="潜在空間におけるentanglement（もつれ）のdisentanglement（解きほぐし）の評価"></a>

### ◎ 潜在空間における entanglement（もつれ）の disentanglement （解きほぐし）の評価
先の MappinNetwork での説明で見たように、潜在空間における entanglement と disentanglement  の程度は、意図した画像を生成するための重要な要素である。<br>
従って、これを定量的に評価出来たら有益なのであるが、StyleGAN では、これを定量化する指標として、Perceptual path length と  Linear separabilityの２つの指標を提案している。<br>

※ 尚、これに関連して、特徴が分解されていること（＝disentanglement  な潜在空間）を定量化する指標も、既に考案されているが、この指標を利用するためには、入力画像を潜在変数に写像するエンコーダーが存在していなくてはならない。そのために、StyleGAN のアーキテクチャを変更するのは根本的な解決ではないため、StyleGAN ではこの手法は採用していない。

- Perceptual path length：<br>
    entanglement（もつれ、歪み） のある空間では、各特徴が絡み合ってうまく分解できていないために、下図のように、潜在空間 Z のある端点（１）から別の端点（９）までの線形補間を実施した場合に、途中の潜在空間での値 z で生される生成画像に、非線形な変化（４，５，６）が生じる。<br>
    ![image](https://user-images.githubusercontent.com/25688193/57284523-51e3ff00-70ec-11e9-813d-e2d17b43e28a.png)<br>

	また直感的にも、disentanglement  な潜在空間では、滑らかな変化となり、entanglement な潜在空間では、急激な変化となると考えられる。<br>

    そのため StyleGAN では、このような線形補間において、どれだけ急激に画像が変化するのかを計測することにより、Perceptual path length と呼ばれる潜在空間の entanglement（もつれ、歪み）or disentanglement （解きほぐし）の定量的指標として利用する。<br>

    即ち、潜在空間 Z での全ての両端（上図では１，９）での Perceptual path length の平均 l_z は、以下のような式で定義される。<br>
    ![image](https://user-images.githubusercontent.com/25688193/57367690-6bf11080-71c4-11e9-92f0-e193377c84b5.png)<br>

    又、中間潜在空間 W での Perceptual path length の平均 l_w  は、以下のような式で定義される。<br>
    ![image](https://user-images.githubusercontent.com/25688193/57367739-8a570c00-71c4-11e9-8c43-231d91e31564.png)<br>

    尚、この距離関数 d は、２つのVGG16 の埋め込み（embedding）から得られる特徴間の距離（perceptually-based pairwise image distance）から計算され、これが、画像間のピクセル単位の距離を測っているので、直感的に一致するものとなっている。<br>

- Linear separability：<br>
    先に述べたように、disentangle（解きほぐし）されている潜在空間では、潜在空間のある部分空間からサンプリングしたベクトル（＝上図では特徴軸上のベクトル）だけを使って画像を生成したときに、その生成画像において、変動する要素が一つのみである。（例えば、性別だけが変わる、肌色だけが変わる）<br>
    これは見方を変えると、潜在空間内の各点（＝ある１つの特徴を生成する潜在変数）が、線形超平面で分離可能であることを意味している。<br>

    そのため StyleGAN では、潜在空間の内の点がどれだけうまく線形分離可能であるかを測定することで、Linear separability と呼ばれる潜在空間の entanglement（歪み）or disentanglement （解きほぐし）の定量的指標として利用する。<br>

    より詳細には、以下のような手順で評価する。<br>
    1. 生成された画像にラベル付けするために、２値特徴（例えば、男性の顔と女性の顔）の数に応じて、StyleGAN とは別の補助ネットワーク（分類器）を用意する。<br>

    1. この補助ネットワークを、（人物画像が分類できるように）CelebA-Hデータセットで学習する。<br>

    1. １つの特徴の分類性能を測定するために、StyleGAN から20万枚の画像を生成し、それを先の学習した補助ネットワークを使って分類する。<br>
        そして、分類器のスコアの高かった半分の10万枚のみを、ラベル付き潜在変数として採用する。<br>
        ※ この際のラベリングは、この分類器での分類結果（例えば、男性の顔に分類と女性の顔に分類）を利用してラベリングされる。<br>

    1. これらのラベリングされた潜在変数が、その潜在空間上で、ラベル毎に線形分離可能であるかを、線形SVMで実際に分類してみて評価する。<br>

    1. 分離のしやすさの定量化のために、各特徴に対して、以下の条件付きエントロピーの exp を計算する。<br>

        ![image](https://user-images.githubusercontent.com/25688193/57427662-e1f68580-725f-11e9-8317-e5fc68d13475.png)<br>

        この条件付きエントロピー H(Y│X) は、１つのサンプルの真のクラスを決定するために、どのくらい多くの追加情報が必要であるかを示しており、この値が小さいほど、分類が容易であることを示している。<br>

- Perceptual path length と Linear separability での性能比較結果：<br>

    ![image](https://user-images.githubusercontent.com/25688193/57427769-68ab6280-7260-11e9-975c-000572e39523.png)<br>
    
    上表は、StyleGAN の論文中で提案されている新たなデータセット FFHQ（Flickr-Faces HQ）において、Perceptual path length と Linear separability を計測した表である。<br>
    StyleGAN のベースラインである従来の ProgressiveGAN の生成器（B）と比較して、StyleGAN の生成器（D,E,F）では、Perceptual path length と Linear separability の値が共に、大幅に小さくなっており、潜在空間 Z よりも、中間潜在空間 W のほうが、disentanglement（解きほぐし） の程度が改善していることが見てとれる。<br>
    又、この改善の程度は、StyleGAN にノイズ有りで StyleMixing 無しとしたアーキテクチャが最も改善され、StyleMixing を導入すると、潜在空間の entanglement  の程度を多少強めてしまうことも見てとれる。<br>


<a id="補足事項"></a>

## ■ 補足事項

<a id="KLダイバージェンス"></a>

### ◎ KLダイバージェンス [Kullback-Leibler(KL) diviergence]
２つの確率分布 P,Q 間の距離を表す指標として、KLダイバージェンスと呼ばれる以下のような指標が考えられる。<br>

![image](https://user-images.githubusercontent.com/25688193/56255740-ee743c00-6100-11e9-9e09-3574daab5b09.png)<br>

※ ここで、定義から分かるように、KLダイバージェンスは、２つの確率分布 P,Q に対して、対称ではない。（![image](https://user-images.githubusercontent.com/25688193/56255798-25e2e880-6101-11e9-9ff6-5abfdd6e8b82.png)）これは、例えば、A地点からB地点までの距離と、その反対のB地点からA地点までの距離が異なることを意味している。従って、距離の公理を満たしていないことになり、厳密には距離ではないことになるが、便宜上、統計的距離という。<br>

以下の図は、２つの確率分布 P,Q の具体的な確率分布の形状として、いくつかの正規分布でKLダイバージェンスを求めた図である。<br>

![image](https://user-images.githubusercontent.com/25688193/57173202-12f14780-6e67-11e9-9b26-9d036487fa6a.png)<br>

２つの確率分布 P,Q が完全に一致するとき、０の値となっており、<br>
２つの確率分布 P,Q が重ならない部分が大きくなるにつれ、大きな値となっていることが分かる。<br>

#### ☆ KLダイバージェンスとエントロピーの関係
KLダイバージェンスは、クロスエントロピーとエントロピーの差で記述することも出来る。<br>
即ち、エントロピーは<br>
![image](https://user-images.githubusercontent.com/25688193/56255899-95f16e80-6101-11e9-9791-95c012937999.png)<br>
クロスエントロピーは、<br>
![image](https://user-images.githubusercontent.com/25688193/56255930-ab669880-6101-11e9-9e78-d06bfecee5c4.png)<br>
であることより、<br>
![image](https://user-images.githubusercontent.com/25688193/56255946-bb7e7800-6101-11e9-9633-beb2a3426628.png)<br>
となり、従って、<br>
![image](https://user-images.githubusercontent.com/25688193/56255976-d6e98300-6101-11e9-9b53-a2304f892bc7.png)<br>
の関係が成り立つことがわかる。<br>


<a id="JSダイバージェンス"></a>

### ◎ JSダイバージェンス [Jensen-Shannon(JS) divergence]
KLダイバージェンスは、距離の公理の一つである対称性の性質を満たさない（![image](https://user-images.githubusercontent.com/25688193/56255798-25e2e880-6101-11e9-9ff6-5abfdd6e8b82.png)）ので、扱いづらいという問題があった。<br>
（※この対称性の性質を満たさないことは、例えば、A地点からB地点までの距離と、その反対のB地点からA地点までの距離が異なることを意味している。　）

JSダイバージェンスは、対称性の性質を満たすように、KLダイバージェンスを使って、以下のように定義される２つの確率分布 P,Q 間の距離指標である。<br>
![image](https://user-images.githubusercontent.com/25688193/56256830-3d23d500-6105-11e9-8d7d-4f101841b809.png)<br>

以下の図は、２つの確率分布 P,Q の具体的な確率分布の形状として、いくつかの正規分布でKLダイバージェンスとJSダイバージェンスを求めた図である。<br>

![image](https://user-images.githubusercontent.com/25688193/57173174-92cae200-6e66-11e9-929f-00a4bd409a50.png)<br>

黄色枠で示した確率分布は、２つの確率分布 P,Q の平均値である M である。<br>
この平均分布 M は、各々の確率分布 P,Q から見て対称であるが、<br>
JSダイバージェンスでは、その定義式より、（対称である）平均分布 M と各々の確率分布 P, Q とのKLダイバージェンスを計算することで、対称性の性質を満たすようにしていることがわかる。<br>
２つの確率分布 P,Q が重ならない部分が大きくなるにつれ、JSダイバージェンスは小さな値となっていることが分かる。<br>

<a id="Inceptionscore"></a>

### ◎ Inception score
GAN においては、生成画像を人目ではなく、如何にして自動的に評価するのか？というのが１つの課題となっている。<br>
Inception score は、このような問題を解決するために考案された、GANの生成画像を評価するための代表的な評価値の１つである。<br>

Inception score では、Inception モデルと呼ばれる画像識別モデルでの予想確率を用いて、スコアの計算が行われる。<br>
より詳細には、画像データを与えない場合の、Inception モデルのラベル y の予想確率 p(y) と、i 番目の画像データ x_i を入力した場合の、Inception モデルのラベル y の予測確率 ![image](https://user-images.githubusercontent.com/25688193/58064442-1271e400-7bbd-11e9-9ddc-bf85f2bfa048.png) を用いて、以下のように、その２つの確率分布間 ![image](https://user-images.githubusercontent.com/25688193/58064470-3a614780-7bbd-11e9-84b8-6266117564dd.png) のKL-ダイバージェンスの和で定義される。<br>

![image](https://user-images.githubusercontent.com/25688193/58064512-58c74300-7bbd-11e9-9989-fc7b1c5f8ac0.png)


ここで、Inception score は、上記のように、ISモデルの２つの予想確率分布間のKL-ダイバージェンスの和で定義されているので、２つの確率分布の差異が大きくなるほど、ISスコアも大きくなるようになっている。<br>

![image](https://user-images.githubusercontent.com/25688193/58064568-94620d00-7bbd-11e9-93ff-d98eeabfe97c.png)

上図は、あるケースにおける２つの確率分布 ![image](https://user-images.githubusercontent.com/25688193/58064470-3a614780-7bbd-11e9-84b8-6266117564dd.png) を形状を示した図である。<br>
p(y) は、画像データという条件を与えない場合の Inception モデルからの予想確率となっているので、一般的に、ある特定のクラスラベルの確からしさが分からず、一様分布に近い平坦な形状となっている。<br>
一方、![image](https://user-images.githubusercontent.com/25688193/58064442-1271e400-7bbd-11e9-9ddc-bf85f2bfa048.png) は、ある画像データ x_i を指定した場合の Inception モデルからの予想確率となっているので、その分ある特定のクラスラベルの確からしさが高まり、凹凸のある形状になっている。<br>
この際に、![image](https://user-images.githubusercontent.com/25688193/58064442-1271e400-7bbd-11e9-9ddc-bf85f2bfa048.png) において、ある特定のクラスラベルでの確からしさが高まり、凹凸が激しくなるほど、２つの分布間の差異が大きくなるので、ISスコアも大きくなる。（左図）<br>
逆に、ある特定のクラスラベルでの確からしさそれほど高まらず、比較的平坦になるほど、２つの分布間の差異が小さくなるので、ISスコアも小さくなる。（右図）<br>
これは言い換えると、「Inception モデルで識別しやすい画像であるほど、ISスコアが大きくなる。」 ことを意味している。<br>


<a id="EMD"></a>

### ◎ Earth-Mover距離（Wassertein距離）
Earth-Mover距離（Wassertein距離）は、２つの確率分布間の距離指標の１つであるが、下図のように、確率分布を１つの山とみなすと、片方の確率分布の山をもう片方の確率分布の山に，土を輸送して変形させるための最小の労力であるという最適輸送問題ともみなせる。<br>
（※それ故、Earth-Mover という名前がついている。）<br>

![image](https://user-images.githubusercontent.com/25688193/56466456-69b15700-644d-11e9-869d-6c6ebf66279d.png)<br>

![image](https://user-images.githubusercontent.com/25688193/56466469-88afe900-644d-11e9-9a29-4f37e14e1b4b.png)<br>

![image](https://user-images.githubusercontent.com/25688193/56466479-bdbc3b80-644d-11e9-9a23-c143b4f06f8d.png)<br>

そして、Earth-Mover距離（EMD,Wassertein距離）は、この 「（輸送する労力）＝（輸送する土の量）×（輸送する距離）」 の関係において、全ての組み合わせで総和した以下のような式で定義できる。<br>

![image](https://user-images.githubusercontent.com/25688193/56477963-17704480-64e6-11e9-9396-a72c7cae56af.png)<br>

２つの確率分布の山を一致させるさせるためには、無数の方法が考えられるが、そららの方法の中で最小の労力（＝最小のEMD）で済むような、最適な輸送方法を考えるのが、考えるべき最適輸送問題となる。<br>

そして、この最適輸送問題は、一般的な線形計画法を用いて解くことが出来る。<br>

しかし、結論から述べると、この最適輸送問題を、単純に線形計画法の主形式で解く方法では、GANのように確率変数 x,y の次元が大きい場合には、その計算量が現実的ではなくなる。<br>
一方、この線形計画問題の双対形式で与える式では、現実的に計算可能な式になっている。<br>

そして、この線形計画法の双対形式での、
Earth-Mover距離（EMD,Wassertein距離）の式は、以下のようになる。（式の導出略）<br>

![image](https://user-images.githubusercontent.com/25688193/56466815-c151c180-6451-11e9-8474-4fb0e160df4e.png)<br>

※ 関数 f のリプシッツ連続性の要件は、線形計画問題おける制約条件から発生している。<br>


<a id="UNet"></a>

### ◎ U-Net
セマンティックセグメンテーション（領域抽出）のタスクでは、局所的な特徴と、画像全体の特徴の両方を、元の画像上で捉えることが重要となる。<br>
CNN では、畳み込み層が物体の局所的な特徴を抽出し、プーリング層が物体の全体的な位置情報をぼかしている役割を持っていた。<br>
そのため、層が深くなるほど、抽出される特徴は、より局所的になり、物体の全体的な位置情報はより曖昧になるので、従来のCNNベースの手法（VGG16など）では、局所的な特徴量と、画像全体の特徴の両方を捉えることが困難となる。<br>

U-Net は、セマンティックセグメンテーションのタスクにおけるこの問題を解決するために、全結合層を利用してしない Fully Convolutional Network（FCN）のアーキテクチャをベースとして、ダウンサンプリングのパス（contractiong path）とアップサンプリングのパス（extractiong path）、及び、skip connection の構造を加えることで、この 「（セマンティックセグメンテーションで重要となる）局所的な特徴量と、画像全体の特徴の両方を捉えること」 を実現する。<br>

※ U-Net は、このように、画像のセマンティックセグメンテーションでの用途で考察された手法であり、医療画像セグメンテーションのコンペで当時の SOTA を達成している。<br>
※ 尚、この U-Net は、オートエンコーダー（AE）としての側面もある。<br>
※ 又、この U-Net は、pix2pix の生成器側のネットワークとして採用されている。<br>


<a id="UNetのアーキテクチャ"></a>

#### ☆ U-Net のアーキテクチャ
![image](https://user-images.githubusercontent.com/25688193/56942254-969ff100-6b54-11e9-81e2-f9ffe31eef12.png)<br>

上図は、U-Net のアーキテクチャ図である。<br>
この U-Net のアーキテクチャには、主に、以下のような特徴がある。<br>

- ダウンサンプリングとアップサンプリング（Encoder と Decoder）
    - 概要で述べたように、セマンティックセグメンテーションのタスクにおいては、物体の局所的な特徴と、物体の全体的な位置情報を同時に捉えることが重要となるが、U-Net では、max pooling によるダウンサンプリング側（Encoder側）の深い層で局所的な特徴を捉え、浅い層で、全体的な位置情報を捉えること出来るようにしている。
    - そして、up-conv によるアップサンプリング側（Decoder側）で、特徴を保持したまま、画像の解像度を復元し、出力する。
    - これらの一連の処理は、Encoder（符号器）と Decoder（復号器）に相当する処理となっているため、U-Net は、一種のオートエンコーダー（AE）ともみなせる。

- skip connections
    - U-Net の主な特徴の１つは、Encoder と Decoder との間に、skip connections と呼ばれる構造を持つことである。（※ この構造の効果は、ResNet に似ている。）
    - これは、CNNでの、Encoder-Decoder のように全ての情報をボトルネックまでダウンサンプリングさせるのではなく、共通の特徴量（＝局所的な特徴と物体の全体的な位置情報）は Encoder-Decoder 間を、skip connections 経由でスキップさせて、上図のアーキテクチャ図の下側のボトルネック部分を回避することで、共通の特徴量におけるデータ欠損を回避させるというものである。
    - より詳細には、Encoder 側の深い層ほど局所的特徴が強く、浅い層ほど全体的な位置情報が強くなっているので、深い層での skip connections は、局所的特徴が強い特徴マップをスキップさせ、浅い層での skip connections は、全体的な位置情報が強い特徴マップをスキップさせることになるが、Decoder 側では、特徴を保持したままアップサンプリングが可能であるので、結果として、「（セマンティックセグメンテーションで重要となる）局所的な特徴量と、画像全体の特徴の両方を捉えること」 を実現できるというものである。
    - 尚、Encoder 側からのスキップした特徴マップと、Decoder側からのアップサンプリングされた特徴マップをマージする際には、Encoder 側の特徴マップの中央部分を crop（切り出し）して、両者のサイズを一致させた上でマージを行う。これは、Encoder 側では、padding=0 の畳み込みで特徴サイズが（ダウンサンプリングとは違う意図で）小さくなっているために、Decoder 側で２倍間隔でアップサンプリングされた特徴マップと、サイズが一致しないくなってしまうためである。

- 全結合層を利用していない。
    - U-Net では、全結合層は使用されていない。
    - このような畳み込みのみからなるネットワークは、 Fully Convolutional Network（FCN）と呼ばれるが、U-Net は、この Fully Convolutional Network（FCN）のアーキテクチャをベースとして設計されている。
    - U-Net で全結合層が使用されていない理由としては、CNN での画像分類のアーキテクチャの出力（この場合は最後の層が全結合層になる必要がある）とは異なり、ネットワークの出力が画像であることが１つの理由として考えられる。
    - <font color="Pink">別の理由としては、overlap-tile strategy によって、任意の大きな画像のシームレスなセグメンテーションを許可するため？（元論文での記載より）</font>

<a id="UNetの学習方法">

#### ☆ U-Net の学習方法

> 記載中...


<a id="UNetの適用例">

#### ☆ U-Net の適用例

- [UNet によるセマンティックセグメンテーションを利用して、衛星画像から地図を生成する。](https://github.com/Yagami360/MachineLearning_Exercises_Python_PyTorch/tree/master/UNet_PyTorch)


<a id="Minibatchdiscrimination"></a>

### ◎ Minibatch discrimination
GAN における課題の１つにモード崩壊という問題が存在する。

この現象は、GAN のアーキテクチャにおいて、識別器に、生成器からの生成画像の多様性を認識できる構造がないために発生してしまう問題であるとみなすことも出来る。<br>
※ この多様性を知る構造が識別器に備わっていれば、多様性を増大するような方向に、誤差逆伝播法で勾配を生成器に伝搬すれば、多様性のある生成画像が自動生成出来て、モード崩壊を回避出来ることになる。<br>

Minibatch discrimation では、この考えに基づき、識別器に、ミニバッチデータ全体の多様性を、ミニバッチ間でのデータの距離から判断するようにする。<br>

以下の図は、この Minibatch discrimation での全体的な処理の流れを示した図である。<br>

![image](https://user-images.githubusercontent.com/25688193/58080159-9ba10f00-7bed-11e9-8cee-82301b043669.png)<br>
![image](https://user-images.githubusercontent.com/25688193/58080209-b4112980-7bed-11e9-995d-b295e7f8e244.png)<br>

上図に示した、Minibatch discriminator における、各処理の詳細は、以下のようになる。<br>

1. 各ミニバッチデータの画像 ![image](https://user-images.githubusercontent.com/25688193/58080387-15d19380-7bee-11e9-96a5-01484e16606d.png) に対して、識別器の中間層からの出力（＝特徴ベクトル）<br>
    ![image](https://user-images.githubusercontent.com/25688193/58080415-25e97300-7bee-11e9-9110-ea0540850a13.png)<br>
    を得る。<br>

2.  この各中間層からの出力 ![image](https://user-images.githubusercontent.com/25688193/58080415-25e97300-7bee-11e9-9110-ea0540850a13.png) に、３階のテンソル ![image](https://user-images.githubusercontent.com/25688193/58080454-3dc0f700-7bee-11e9-9f3f-944e94fcc959.png) を乗算して、B×C の行列<br>
    ![image](https://user-images.githubusercontent.com/25688193/58080504-503b3080-7bee-11e9-84af-09a3fa3b54e7.png)<br>
    を得る。（※ ここで、テンソルの次元数 B,C は学習可能なハイパーパラメータである。）<br>

3. 画像 x_i における行列 M_i の b行目のベクトル M_(i,b)  と、<br>
    別の画像 x_j における行列 M_j の b行目のベクトル M_(j,b) とのL1ノルムを<br>
    ![image](https://user-images.githubusercontent.com/25688193/58080670-955f6280-7bee-11e9-96ff-fb083b46d166.png)<br>
    で計算することで、２つの画像間の距離を計算する。<br>

4. 計算した２つの画像間の距離 ![image](https://user-images.githubusercontent.com/25688193/58080787-cb9ce200-7bee-11e9-8a90-b4db79dd357a.png) に対して、ミニバッチデータ内の他の全ての画像 ![image](https://user-images.githubusercontent.com/25688193/58080973-29312e80-7bef-11e9-81e5-f70424c28d48.png) に関しての和を取る。<br>
    ![image](https://user-images.githubusercontent.com/25688193/58080855-ee2efb00-7bee-11e9-825b-25ea51f52181.png)<br>
    ※ この計算結果は、ある画像とミニバッチ内でのその他の画像間の距離となっているが、これは見方を変えると、ミニバッチ内でのデータの多様性を示していることになる。従って、この処理（と後述の5）により、識別器にデータの多様性を認識できる構造が入っていることになる。<br>

5. 画像 x_i における行列 M_i の 行数 b=1~B に関して同じような計算を行い、それらを合わせてベクトル化する。<br>
    ![image](https://user-images.githubusercontent.com/25688193/58081744-8b3e6380-7bf0-11e9-9d6a-848bf90597f3.png)<br>

6. 先に得ていた識別器の中間層からの出力 f(x_i) と結合（concat）したもの<br>
    ![image](https://user-images.githubusercontent.com/25688193/58081651-6518c380-7bf0-11e9-877e-8fd289225af2.png)<br>
    を、次の層への入力とする。<br>


<a id="StyleTransferにおける正規化手法（INとAdaIN）"></a>

### ◎ Style Transfer における正規化手法（INとAdaIN）

- Instance Normalization（IN）<br>
    BatchNorm のように、バッチサイズ単位で正規化するのではなく、特徴マップ単位（＝画像の縦横幅単位）で正規化する手法。<br>

    ![image](https://user-images.githubusercontent.com/25688193/57304779-ed8c6400-711a-11e9-9b8a-d1e88e6ecb23.png)<br>

    具体的には、以下の式で与えられる。<br>
    ![image](https://user-images.githubusercontent.com/25688193/57304837-009f3400-711b-11e9-9756-04a87e1e97cd.png)<br>
    Style Transfer のタスクにおいて、Batch Normalization をこの Instance Normalization に変えることで、Style Transfer の質において大きな向上が達成されることが報告されている。

- Adaptive Instance Normalization (AdaIN)<br>

> 記載中...


<a id="参考"></a>

## ■ 参考

### ◎ 参考サイト

**強調文字**付きは、特に参考にさせたもらったサイトです。<br>

- 全般
    - [**GAN（と強化学習との関係）**](https://www.slideshare.net/masa_s/gan-83975514)
    - [IIBMP2016 深層生成モデルによる表現学習](https://www.slideshare.net/pfi/iibmp2016-okanohara-deep-generative-models-for-representation-learning)
    - [タカハシ春の GAN 祭り！〜 一日一GAN(๑•̀ㅂ•́)و✧ 〜- ABEJA Arts Blog](https://tech-blog.abeja.asia/entry/everyday_gan)

- VAE
    - [**Variational Autoencoder徹底解説**](https://qiita.com/kenmatsu4/items/b029d697e9995d93aa24)
    - [**猫でも分かるVariational AutoEncoder**](https://www.slideshare.net/ssusere55c63/variational-autoencoder-64515581)
    - 実装
        - - [PyTorch (11) Variational Autoencoder - 人工知能に関する断創録](http://aidiary.hatenablog.com/entry/20180228/1519828344)

- GAN
    - [**【元論文】[1406.2661] Generative Adversarial Networks**](https://arxiv.org/abs/1406.2661)
    - [**今さら聞けないGAN（1）　基本構造の理解**](https://qiita.com/triwave33/items/1890ccc71fab6cbca87e)
    - [**Generative Adversarial Networks(GAN)を勉強して、kerasで手書き文字生成する - 緑茶思考ブログ**](http://yusuke-ujitoko.hatenablog.com/entry/2017/05/08/010314)
    - [GANの論文を読んだ自分なりの理解とTensorflowでのGANの実装メモ](http://owatank.hatenablog.com/entry/2018/04/20/180151)

- DCGAN
    - [**【元論文】[1511.06434] Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks**](https://arxiv.org/abs/1511.06434)
    - 実装
        - [**DCGAN Tutorial — PyTorch Tutorials 1.0.0.dev20190327 documentation**](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html?highlight=dataloader)
        - [GitHub - znxlwm/pytorch-MNIST-CelebA-GAN-DCGAN: Pytorch implementation of Generative Adversarial Networks (GAN) and Deep Convolutional Generative Adversarial Networks (DCGAN) for MNIST and CelebA datasets](https://github.com/znxlwm/pytorch-MNIST-CelebA-GAN-DCGAN)
        - [**PyTorch (12) Generative Adversarial Networks (MNIST) - 人工知能に関する断創録**](http://aidiary.hatenablog.com/entry/20180304/1520172429)

- Conditional GAN（cGAN）
    - [**【元論文】[1411.1784] Conditional Generative Adversarial Nets**](https://arxiv.org/abs/1411.1784)
    - [**今さら聞けないGAN（6） Conditional GANの実装**](https://qiita.com/triwave33/items/f6352a40bcfbfdea0476)
    - 実装
        - [GitHub/znxlwm/pytorch-MNIST-CelebA-cGAN-cDCGAN](https://github.com/znxlwm/pytorch-MNIST-CelebA-cGAN-cDCGAN)
        - [**PyTorch で Conditional GAN をやってみる**](http://cedro3.com/ai/pytorch-conditional-gan/)

- Wasserstein GAN
    - [**【元論文】[1701.07875] Wasserstein GAN**](https://arxiv.org/abs/1701.07875)
    - [**Read-through: Wasserstein GAN**](https://www.alexirpan.com/2017/02/22/wasserstein-gan.html)
    - [**From GAN to WGAN**](https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html)
    - [GAN — Wasserstein GAN & WGAN-GP](https://medium.com/@jonathan_hui/gan-wasserstein-gan-wgan-gp-6a1a2aa1b490)
    - [**Wasserstein GAN と Kantorovich-Rubinstein 双対性 - Qiita**](https://qiita.com/mittyantest/items/0fdc9ce7624dbd2ee134)
    - [**[DL輪読会]Wasserstein GAN/Towards Principled Methods for Training Generative Adversarial Networks**](https://www.slideshare.net/DeepLearningJP2016/dlwasserstein-gantowards-principled-methods-for-training-generative-adversarial-networks)
    - [Wasserstein GAN（WGAN）でいらすとや画像を生成してみる - 緑茶思考ブログ](http://yusuke-ujitoko.hatenablog.com/entry/2017/05/20/145924)
    - [Wasserstein GAN [arXiv:1701.07875] – ご注文は機械学習ですか？](http://musyoku.github.io/2017/02/06/Wasserstein-GAN/)
    - [Wasserstein GAN (WGAN) - DeepLearningを勉強する人](http://wanwannodao.hatenablog.com/entry/2017/02/28/051353)
    - 実装
        - [**GitHub - martinarjovsky/WassersteinGAN**](https://github.com/martinarjovsky/WassersteinGAN)

- pix2pix
    - [**【元論文】Image-to-Image Translation with Conditional Adversarial Networks**](https://arxiv.org/abs/1611.07004)
    - [**pix2pixでポップアートから写真を復元してみた (追記あり)**](https://qiita.com/hiromu1996/items/38f1bd5a78336fa8ca25)
    - [**深層学習を利用した食事画像変換で飯テロ**](https://qiita.com/negi111111/items/6d6f19edc060a91662ec)
    - [**[DL輪読会]Image-to-Image Translation with Conditional Adversarial Networks**](https://www.slideshare.net/DeepLearningJP2016/dlimagetoimage-translation-with-conditional-adversarial-networks)
    - [Image-to-Image Translation with Conditional Adversarial Nets](https://phillipi.github.io/pix2pix/)
    - [Image-to-Image Translation in Tensorflow](https://affinelayer.com/pix2pix/)
    - 実装
        - [GitHub/junyanz/pytorch-CycleGAN-and-pix2pix](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)
        - [**GitHub/eriklindernoren/PyTorch-GAN**](https://github.com/eriklindernoren/PyTorch-GAN#pix2pix)

- U-Net
    - [**【元論文】U-Net: Convolutional Networks for Biomedical Image Segmentation**](https://arxiv.org/abs/1505.04597)
    - [**Deep learningで画像認識⑨〜Kerasで畳み込みニューラルネットワーク vol.5〜**](https://lp-tech.net/articles/5MIeh)
    - [U-Net：ディープラーニングによるSemantic Segmentation手法](https://blog.negativemind.com/2019/03/15/semantic-segmentation-by-u-net/)
    - [U-Net: Convolutional Networks for Biomedical Image Segmentationの紹介](https://www.slideshare.net/KCSKeioComputerSocie/unet-convolutional-networks-for-biomedicalimage-segmentation?ref=https://blog.negativemind.com/2019/03/15/semantic-segmentation-by-u-net/)
    - [オートエンコーダーとしてのU-Net（自己符号化から白黒画像のカラー化まで）](https://qiita.com/koshian2/items/603106c228ac6b7d8356)
    - [Semantic Segmentation — U-Net (Part 1)](https://medium.com/@keremturgutlu/semantic-segmentation-u-net-part-1-d8d6f6005066)
    - 実装
        - [**GitHub/GunhoChoi/Kind-PyTorch-Tutorial12_Semantic_Segmentation/**](https://github.com/GunhoChoi/Kind-PyTorch-Tutorial/tree/master/12_Semantic_Segmentation)

- Minibatch discrimation
    - [**Improved Techniques for Training GANs [arXiv:1606.03498] – ご注文は機械学習ですか？**](http://musyoku.github.io/2016/12/23/Improved-Techniques-for-Training-GANs/)
    - [Toy ProblemでGANのmode collapseを可視化 - Qiita](https://qiita.com/haru-256/items/b9584d404da3d9ea51e0)

- ProgressiveGAN
    - [**【元論文】[1710.10196] Progressive Growing of GANs for Improved Quality, Stability, and Variation**](https://arxiv.org/abs/1710.10196)
    - [**【Progressive Growing of GANs for Improved Quality, Stability, and Variation】を読んだのでまとめる - St_Hakky’s blog**](https://www.st-hakky-blog.com/entry/2017/11/22/173112)
    - [【論文メモ:PGGAN】Progressive Growing of GANs for Improved Quality, Stability, and Variation - Re:ゼロから始めるML生活](https://tsunotsuno.hatenablog.com/entry/pggan)
    - [GAN — Progressive growing of GANs – Jonathan Hui – Medium](https://medium.com/@jonathan_hui/gan-progressive-growing-of-gans-f9e4f91edf33)

- StyleGAN
    - [**【元論文】[1812.04948] A Style-Based Generator Architecture for Generative Adversarial Networks**](https://arxiv.org/abs/1812.04948)
    - [**StyleGAN「写真が証拠になる時代は終わった。」 - Qiita**](https://qiita.com/Phoeboooo/items/7be15acb960837adab21)
    - [**StyleGAN: A Style-Based Generator Architecture for Generative Adversarial Networks - 機械学習奮闘記**](https://sakuma-dayo.hatenablog.com/entry/2019/03/06/201429)
    - [**ニューラルネットワークでStyle Transferを行う論文の紹介 - Qiita**](https://qiita.com/kidach1/items/0e7af5981e39955f33d6)
    - [ニューラルネットワークでStyle Transferを行う論文の紹介](https://qiita.com/kidach1/items/0e7af5981e39955f33d6)
    - [**Style-based GANs – Generating and Tuning Realistic Artificial Faces | Lyrn.AI**](https://www.lyrn.ai/2018/12/26/a-style-based-generator-architecture-for-generative-adversarial-networks/)
    - [Arbitrary Style Transfer in Real-Time With Adaptive Instance Normalization の紹介](https://www.slideshare.net/HHiroto/arbitrary-style-transfer-in-realtime-with-adaptive-instance-normalization-84174047)
    - 実装
        - [PyTorchでStyleGAN - Qiita](https://qiita.com/t-ae/items/afc969c48450507dc421)

- その他
    - [**KL divergenceとJS divergenceの可視化**](http://yusuke-ujitoko.hatenablog.com/entry/2017/05/07/200022)
    - [正規分布間のKLダイバージェンス](https://qiita.com/ceptree/items/9a473b5163d5655420e8)
    - [**bluewidz nota: Inception score**](http://bluewidz.blogspot.com/2017/12/inception-score.html)

