# 生成モデル [generative model]
機械学習を生成モデル手法（GAN、VAE等）について勉強したことをまとめたノート（忘備録）です。現在執筆中<br>

## 目次 [Contents]

1. 概要
1. [変分オートエンコーダー [VAE : Variational AutoEncoder]](#VAE)
    1. [VAE のアーキテクチャ](#VAEのアーキテクチャ)
    1. [VAE の学習とKLダイバージェンス](#VAEの学習とKLダイバージェンス)
    <!--
    1. [潜在変数の空間と生成画像の分布の関係](#潜在変数の空間と生成画像の分布の関係)
    -->
1. [GAN [Generative Adversarial Networks]](#GAN)
    1. [GAN のアーキテクチャ](#GANのアーキテクチャ)
    1. [識別器の動作と損失関数](#識別器の動作と損失関数)
    1. [生成器の動作と損失関数](#生成器の動作と損失関数)
    1. [密度比推定による識別器の役割の再解釈](#密度比推定による識別器の役割の再解釈)
    1. [JSダイバージェンスによる識別器の損失関数の再解釈](#JSダイバージェンスによる識別器の損失関数の再解釈)
    1. [GANの学習の困難さ](#GANの学習の困難さ)
        1. [GANの収束性](#GANの収束性)
        1. [モード崩壊](#モード崩壊)
        1. [勾配損失問題](#勾配損失問題)
1. [DCGAN [Deep Convolutional GAN]](#DCGAN)
    1. [DCGAN のアーキテクチャ](#DCGANのアーキテクチャ)
    1. [DCGAN の適用例](#DCGANの適用例)
1. [Conditional GAN（cGAN）](#ConditionalGAN（cGAN）)
    1. [cGAN のアーキテクチャ](#cGANのアーキテクチャ)
    1. [cGAN の損失関数](#cGANの損失関数)
    1. [cGAN の適用例](#cGANの適用例)
1. [WGAN [Wasserstein GAN]](#WGAN)
    1. [JSダイバージェンスと Earth-Mover 距離の収束性と勾配消失問題](#JSダイバージェンスとEarth-Mover距離の収束性の違いと勾配消失問題)
    1. [WGAN のアーキテクチャと損失関数](#WGANのアーキテクチャと損失関数)
    1. [WGANでのその他の工夫](#WGANでのその他の工夫)
    1. [WGAN のアルゴリズム](#WGANのアルゴリズム)
    1. [WGANのその他の利点](#WGANのその他の利点)
    1. [WGAN の適用例](#WGANの適用例)
1. WGAN-gp
1. [pix2pix](#pix2pix)
    1. [pix2pix のアーキテクチャ](#pix2pixのアーキテクチャ)
    1. [pix2pix の損失関数](#pix2pixの損失関数)
    1. [pix2pix の適用例](#pix2pixの適用例)
1. [CycleGAN](#CycleGAN)
    1. [CycleGAN のアーキテクチャ](#CycleGANのアルゴリズム)
    1. [CycleGAN の実験結果](#CycleGANの実験結果)
1. [StarGAN](#StarGAN)
    1. [StarGAN のアーキテクチャ](#StarGANのアーキテクチャ)
    1. [StarGAN の実験結果](#StarGANの実験結果)
1. [SAGAN [Self-Attention Generative Adversarial Networks]](#SAGAN)
    1. [SAGAN のアーキテクチャ](#SAGANのアーキテクチャ)
    1. [SAGAN の損失関数](#SAGANの損失関数)
    1. [SAGAN の実験結果](#SAGANの実験結果)
1. [ProgressiveGAN（PGGAN）](#ProgressiveGAN（PGGAN）)
    1. [ProgressiveGAN のアーキテクチャ](#ProgressiveGANのアーキテクチャ)
    1. [生成画像の多様性向上とモード崩壊防止のための工夫](#生成画像の多様性向上とモード崩壊防止のための工夫)
    1. [学習の安定化のための工夫](#学習の安定化のための工夫)
    1. [PGGAN の実験結果](#PGGANの実験結果)
    1. [PGGAN の適用例](#PGGANの適用例)
1. [StyleGAN](#StyleGAN)
    1. [StyleGAN のアーキテクチャ](#StyleGANのアーキテクチャ)
    1. [潜在空間における entanglement（もつれ）の disentanglement （解きほぐし）の評価](#潜在空間におけるentanglement（もつれ）のdisentanglement（解きほぐし）の評価)
1. [GANimation](#GANimation)
    1. [GANimation のアーキテクチャ](#GANimationのアーキテクチャ)
    1. [GANimation の損失関数](#GANimationの損失関数)
    1. [GANimation の実験結果](#GANimationの実験結果)
1. [補足事項](#補足事項)
    1. [【補足】KLダイバージェンス [Kullback-Leibler(KL) diviergence]](#KLダイバージェンス)
    1. [【補足】JSダイバージェンス [Jensen-Shannon(JS) divergence]](#JSダイバージェンス)
    1. [【補足】Inception score](#Inceptionscore)
    1. [【補足】Earth-Mover 距離（Wassertein距離）](#EMD)
    1. [【補足】U-Net](#UNet)
        1. [U-Net のアーキテクチャ](#UNetのアーキテクチャ)
        1. U-Net の学習方法
        1. [U-Net の適用例](#UNetの適用例)
    1. [【補足】Minibatch discrimination](#Minibatchdiscrimination)
    1. [【補足】 Style Transfer における正規化手法（INとAdaIN）](#StyleTransferにおける正規化手法（INとAdaIN）)
    <!--
    1. [【補足（外部リンク）】情報理論 / 情報数理](http://yagami12.hatenablog.com/entry/2017/09/17/103228)
    1. [【補足（外部リンク）】ゲーム理論](http://yagami12.hatenablog.com/entry/2018/03/02/171939)
    -->
1. [参考サイト](#参考)
<!--
1. 論文翻訳
    1. [【論文翻訳（非公開）】Generative Adversarial Networks](https://github.com/Yagami360/MachineLearning-Papers_Survey/blob/master/papers/Generative_Adversarial_Networks/GenerativeAdversarialNetworks.md)
    1. [【論文翻訳（非公開）】Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://github.com/Yagami360/MachineLearning-Papers_Survey/blob/master/papers/Unsupervised_Representation_Learning_with_Deep_Convolutional_Generative_Adversarial_Networks/DeepConvolutionalGAN.md)
    1. [【論文翻訳（非公開）】Conditional Generative Adversarial Nets](https://github.com/Yagami360/MachineLearning-Papers_Survey/blob/master/papers/Conditional_Generative_Adversarial_Nets/ConditionalGAN.md)
    1. [【論文翻訳（非公開）】Wasserstein GAN](https://github.com/Yagami360/MachineLearning-Papers_Survey/blob/master/papers/Wasserstein_GAN/WassersteinGAN.md)
    1. [【論文翻訳（非公開）】Image-to-Image Translation with Conditional Adversarial Networks](https://github.com/Yagami360/MachineLearning-Papers_Survey/blob/master/papers/Image-to-Image_Translation_with_Conditional_Adversarial_Networks/pix2pix.md)
    1. [【論文翻訳（非公開）】U-Net: Convolutional Networks for Biomedical Image Segmentation](https://github.com/Yagami360/MachineLearning-Papers_Survey/blob/master/papers/U-Net_Convolutional_Networks_for_Biomedical/UNet.md)
    1. [【論文翻訳（非公開）】A Style-Based Generator Architecture for Generative Adversarial Networks](https://github.com/Yagami360/MachineLearning-Papers_Survey/blob/master/papers/A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks/StyleGAN.md)
-->

---

<a id="VAE"></a>

## ■ 変分オートエンコーダー [VAE : Variational AutoEncoder]

<a id="VAEのアーキテクチャ"></a>

### ◎ VAE のアーキテクチャ
（生成モデルの分野における）オートエンコーダーとは、教師なし学習のもとで、データを表現するための特徴を獲得するために、以下のようなアーキテクチャと処理手順を持った手法である。<br>

![image](https://user-images.githubusercontent.com/25688193/56261188-05248e00-6115-11e9-9ee8-c4ebe45b67dc.png)<br>

【オートエンコーダーの処理手順】<br>
1. 入力データ（上図では手書き数字画像データ）x を、エンコーダー（ニューラルネットワーク）で潜在変数 z に変換する。<br>
	この様子は、見方を変えると入力データの符号化しているようにみなせるため、この入力データを変換するニューラルネットワークをエンコーダーと呼ぶ。<br>
    又、潜在変数 z の次元が、入力データ x より小さい場合、このエンコーダーでの処理は次元削除になっているとみなすことも出来る。<br>
1. 潜在変数 z を、デコーダー（ニューラルネットワーク）に入力し、元の画像に再変換する。<br>
    この様子は、見方を変えるとエンコーダーで符号化した潜在変数を、再度（入力データである画像に）復号化しているようにみなせるため、この潜在変数を変換するニューラルネットワークをデコーダーと呼ぶ。<br>

<br>

このようなオートエンコーダーの内、以下のようなアーキテクチャ図のように、潜在変数 z を生成する分布が、正規分布 N(0,1) であるように設定した（ z~N(0,1)  ）オートエンコーダーを、VAE [Variational AutoEncoder] という。<br>
（これに対して、通常の AutoEncoder は潜在変数 z を生成する分布を仮定していない。）<br>

![image](https://user-images.githubusercontent.com/25688193/57172796-dff78580-6e5f-11e9-91cf-a71af9ab692a.png)<br>

※ ここで、VAE におけるエンコーダーは、正規分布に従う潜在変数 z を直接出力するのではなく、潜在変数 z が従う正規分布の平均値 μ(x) と分散値 σ(x) を生成していることに注意。<br>


<a id="VAEの学習とKLダイバージェンス"></a>

### ◎ VAE の学習とKLダイバージェンス
VAE の学習の目的は、潜在変数 z から画像を生成する確率分布 p_θ (x) の最大化である。<br>
但し、確率分布のままでは扱いづらいため、その対数尤度 ![image](https://user-images.githubusercontent.com/25688193/56268166-b1716f00-612b-11e9-96f6-e219ce7af871.png) の最大化を考える。<br>

結論から述べると、この対数尤度 ![image](https://user-images.githubusercontent.com/25688193/56268166-b1716f00-612b-11e9-96f6-e219ce7af871.png) は、以下の式ように、変分下限 [ELBO : Evidence Lower BOund] なるものととKLダイバージェンスの和で表現できる。（詳細計算略）<br>
![image](https://user-images.githubusercontent.com/25688193/56268264-dd8cf000-612b-11e9-88a3-52169bd47f44.png)<br>

ここで、KLダイバージェンスの項 ![image](https://user-images.githubusercontent.com/25688193/56268329-07dead80-612c-11e9-8a6c-49949d8b5470.png) はその定義より、常に０以上の値となるが、VAE の学習の目的である、![image](https://user-images.githubusercontent.com/25688193/56268673-e03c1500-612c-11e9-8022-a4c2974fd409.png) のときは０の値となるので、対数尤度 ![image](https://user-images.githubusercontent.com/25688193/56268166-b1716f00-612b-11e9-96f6-e219ce7af871.png) を最大化したければ、変分下限 L(θ,ϕ,x) を最大化すれば良いことが分かる。<br>

次に、この変分下限 L(θ,ϕ,x) は、以下の式に変形出来る。（詳細計算略）<br>
![image](https://user-images.githubusercontent.com/25688193/56268734-0cf02c80-612d-11e9-89b8-92d02bad08f3.png)<br>

従って、変分下限 L(θ,ϕ,x)  を最大化するためには、KLダイバージェンスを最小化し、復元誤差を最大化すれば良いことが分かる。<br>

まとめると、VAE の学習の目的である対数尤度 ![image](https://user-images.githubusercontent.com/25688193/56268166-b1716f00-612b-11e9-96f6-e219ce7af871.png) の最大化したければ、変分下限 L(θ,ϕ,x) を最大化すれば良いが、<br>
そのためには、２つの確率分布 ![image](https://user-images.githubusercontent.com/25688193/56268861-550f4f00-612d-11e9-96bd-fc7fe83cc895.png) のKLダイバージェンス<br>
![image](https://user-images.githubusercontent.com/25688193/56268970-8d169200-612d-11e9-82fc-b09556914cd9.png)<br>
を最小化し、<br>
復元誤差<br>
![image](https://user-images.githubusercontent.com/25688193/56268998-a15a8f00-612d-11e9-9935-a448a70220c8.png)<br>
を最大化すれば良いこととになる。<br>


<!--
> 図を自作のものに要修正

<a id="潜在変数の空間と生成画像の分布の関係"></a>

### ◎ 潜在変数の空間と生成画像の分布の関係
以下の図は、潜在変数 z の次元を２次元として、エンコーダー側から正規分布に従って出力される潜在変数 z の値と、実際にデコーダーから出力される画像の対応関係を示した図である。<br>

![image](https://user-images.githubusercontent.com/25688193/56263478-0908de00-611e-11e9-890a-d34fbc180164.png)<br>

エンコーダーから出力される潜在変数 z の空間において、正規分布 N(0,1) の密度が大きい領域（＝中央部分）では、3,5,8,9,2 の画像などの似た形状の画像の種類が多く分布していることがわかる。反対に、1,7,0 などの似ていない形状の画像の種類は、正規分布の密度が小さい領域に分布していることが分かる。<br>

<br>

更に、以下の左下図は、潜在変数 z の次元を２次元として、潜在変数 z の値を動かした場合（赤矢印）の、デコーダー側から出力される出力画像の変化を表した図である。<br>
![image](https://user-images.githubusercontent.com/25688193/56262854-7e26e400-611b-11e9-88d6-05e974c449a6.png)<br>
先のエンコーダー側から出力される潜在変数の値の分布図（右上図）と、概ね一致していることが分かる。<br>
-->

<a id="GAN"></a>

## ■ GAN [Generative Adversarial Networks]

<!--
> VAE からの話（真の分布とモデルの分布の一致化等）の延長で、議論を展開した文章に修正すること。
-->

GAN は、Generator（生成器）と Discriminator（識別器）という２つの機構から構成される生成モデルである。<br>

生成器は、学習用データと同じようなデータ（偽物画像）を生成する。<br>
一方、識別器は、（この学習用データと、生成器が出力した偽物画像を入力とし）、これらのデータが、学習用データから来たものであるのか、或いは、生成器から来た偽物画像であるのかを識別する。<br>

そして、この生成器と識別器の双方が、敵対的に学習していくにつれ、次第に、識別器が本物の学習用データと見分けがつかないデータを生成することが出来るようになるという流れとなる。<br>
※ このとき、完全に学習が進むと、生成器は、データが本物画像なのか偽物画像なのかを、完全に見分けがつかなくなるので、識別率が 50% となる。<br>

尚、この生成器と識別器による処理は、一種の関数とみなせるが、この表現として、一般的に、多層パーセプトロン（MLP）が用いられる。<br>
※ ニューラルネットワークを用いると、既存の最尤推定による生成モデルでは爆発的に増加する計算量を誤差逆伝搬法で解決できる。<br>


<a id="GANのアーキテクチャ"></a>

### ◎ GAN のアーキテクチャ
![image](https://user-images.githubusercontent.com/25688193/57116628-5bd6cc80-6d91-11e9-9fa6-2089ffd7bbbc.png)<br>
![image](https://user-images.githubusercontent.com/25688193/57116750-22eb2780-6d92-11e9-9644-9d56420e743d.png)<br>

<!--
![image](https://user-images.githubusercontent.com/25688193/56490862-a0f13800-6521-11e9-9e51-77b579de22db.png)<br>
![image](https://user-images.githubusercontent.com/25688193/56491160-85d2f800-6522-11e9-8d67-bf2eabb2fe91.png)<br>
-->

<a id="識別器の動作と損失関数"></a>

### ◎ 識別器の動作と損失関数
識別器の損失関数は、以下のような式で与えられる。<br>
![image](https://user-images.githubusercontent.com/25688193/57116895-0ef3f580-6d93-11e9-8b5c-0e36bf0a92e3.png)<br>
※ この式の、![image](https://user-images.githubusercontent.com/25688193/57117890-529e2d80-6d9a-11e9-8cb6-9f8a696b62a4.png) の部分は、ゲーム理論で言うところの、min-max 法（想定される最小の利益が最大になるように最適化）に相当する式になっている。（ゲームは、２プレイヤーのゼロサムゲーム）<br>

ここで、この損失関数の式は、以下のような識別器の動作の意味に対応している。<br>
① 識別器は、本物画像 x が入力されたとき、本物画像 x を（正しく）本物に判定するように ![image](https://user-images.githubusercontent.com/25688193/57117139-d8b77580-6d94-11e9-8cfd-e396d0eaa99a.png) を出力しようとする。<br>
⇒ 第１項 ![image](https://user-images.githubusercontent.com/25688193/57116786-4f9f3f00-6d92-11e9-9d03-0a2d9f2b2acb.png) の最大化に対応<br>

![image](https://user-images.githubusercontent.com/25688193/57117538-b2470980-6d97-11e9-8adf-b0ceb7eb2fac.png)<br>

② 識別器は、偽物画像 ![image](https://user-images.githubusercontent.com/25688193/57116920-36e35900-6d93-11e9-89d9-040c8a126c56.png) が入力されたとき、偽物画像 ![image](https://user-images.githubusercontent.com/25688193/57116920-36e35900-6d93-11e9-89d9-040c8a126c56.png) を（正しく）偽物に判定するように ![image](https://user-images.githubusercontent.com/25688193/57116877-e966ec00-6d92-11e9-8fe9-48cab2627791.png) を出力しようとする。<br>
⇒ 第２項 ![image](https://user-images.githubusercontent.com/25688193/57117395-a9a20380-6d96-11e9-9140-97bb8f91e868.png) の最大化に対応<br>

![image](https://user-images.githubusercontent.com/25688193/57117512-790e9980-6d97-11e9-8bda-d6c37db14c02.png)<br>

<a id="生成器の動作と損失関数"></a>

### ◎ 生成器の動作と損失関数
![image](https://user-images.githubusercontent.com/25688193/56271987-b38bfb80-6134-11e9-9053-092bc8d798db.png)<br>

ここで、この損失関数の式は、以下のような生成器の動作の意味に対応している。<br>
➀ 識別器が、偽物画像 ![image](https://user-images.githubusercontent.com/25688193/57116920-36e35900-6d93-11e9-89d9-040c8a126c56.png) を（正しく）偽物だと判断し、![image](https://user-images.githubusercontent.com/25688193/57116877-e966ec00-6d92-11e9-8fe9-48cab2627791.png) を出力する <br>
⇒ 生成器は識別器を騙すことに失敗しているので、損失関数 L_G の値は大きくなる。（＝大きくなるように定義している）<br>

➁ 識別器が、偽物画像 ![image](https://user-images.githubusercontent.com/25688193/57116920-36e35900-6d93-11e9-89d9-040c8a126c56.png) を（誤って）偽物だと判断し、![image](https://user-images.githubusercontent.com/25688193/57117139-d8b77580-6d94-11e9-8cfd-e396d0eaa99a.png) を出力する。
⇒ 生成器は識別器を騙すことに成功しているので、損失関数 L_G の値は小さくなる。（＝小さくなるように定義している）<br>


<a id="密度比推定による識別器の役割の再解釈"></a>

### ◎ 密度比推定による識別器の役割の再解釈
ここでは、密度比推定の観点から、GANにおける識別器が果たす役割を、理論的に再解釈する。

先の VAE では、デコーダー側での潜在変数 z から画像を生成する確率分布 p_θ (x) が、正規分布やベルヌーイ分布の形になると仮定した上で、その確率分布の対数尤度を最大化するように学習していた。<br>

一方、GANでは、このような生成器の確率分布 ![image](https://user-images.githubusercontent.com/25688193/56272299-62c8d280-6135-11e9-8467-89448e51e0d0.png) の具体的な形を仮定することなしに、暗黙的な生成モデルとして考えている。<br>
従って、VAEのときのように、確率分布の対数尤度 ![image](https://user-images.githubusercontent.com/25688193/56272169-1f6e6400-6135-11e9-8ad7-c4ff1eb0c28e.png) を直接評価できないので、モデルの分布 ![image](https://user-images.githubusercontent.com/25688193/56272299-62c8d280-6135-11e9-8467-89448e51e0d0.png) と真の分布 ![image](https://user-images.githubusercontent.com/25688193/56272229-3ca33280-6135-11e9-8682-7ab994fc852f.png) の密度比<br>
![image](https://user-images.githubusercontent.com/25688193/56272333-77a56600-6135-11e9-8e19-728221fc907b.png)<br>
でモデルの分布がどのくらい真の分布っぽいかの尤度を評価することになる。<br>

ここで、データ集合 X のうち、<br>
半分のデータがモデルの分布 ![image](https://user-images.githubusercontent.com/25688193/56272299-62c8d280-6135-11e9-8467-89448e51e0d0.png) から生成されてものと考え、そのラベルを y=0 とし、<br>
もう半分のデータが真の分布 ![image](https://user-images.githubusercontent.com/25688193/56272229-3ca33280-6135-11e9-8682-7ab994fc852f.png) から生成されてものと考え、そのラベルを y=1 とすると、<br>
モデルの分布と真の分布は、ラベルが設定された際の条件付き確率分布<br>
![image](https://user-images.githubusercontent.com/25688193/56272437-a9b6c800-6135-11e9-8d73-9695403240ea.png)<br>
で表現できる。<br>

従って、密度比の式は、<br>
![image](https://user-images.githubusercontent.com/25688193/56272551-e2ef3800-6135-11e9-85a0-36c6526f3cd0.png)<br>
つまり、観測データ x が、モデルの分布 ![image](https://user-images.githubusercontent.com/25688193/56272299-62c8d280-6135-11e9-8467-89448e51e0d0.png) から来たのか？真の分布 ![image](https://user-images.githubusercontent.com/25688193/56272229-3ca33280-6135-11e9-8682-7ab994fc852f.png) から来たのか？を予想するような識別モデル<br>
![image](https://user-images.githubusercontent.com/25688193/56272572-fbf7e900-6135-11e9-8825-6d826fa79631.png)<br>
を学習出来れば、密度比を推定することが出来る。<br>
この識別モデルこれはまさに、先に述べた識別器 D そのもの役割であり、従って、識別器 D は、密度比を推定しているともみなすことも出来る。<br>
※ 識別器によって、GAN における尤度推定を、NNが得意としている分類問題に置き換えている点に注目。<br>


<a id="JSダイバージェンスによる識別器の損失関数の再解釈"></a>

### ◎ JSダイバージェンスによる識別器の損失関数の再解釈
GAN の学習の目的は、モデルの分布と真の分布を近づけ ![image](https://user-images.githubusercontent.com/25688193/56332861-ea155500-61cc-11e9-8032-98293e456e86.png) とすることである。このことは、以下で見るようにモデルの分布と真の分布間のJSダイバージェンスを最小化していることに一致する。<br>

真の分布 ![image](https://user-images.githubusercontent.com/25688193/56272229-3ca33280-6135-11e9-8682-7ab994fc852f.png) とモデルの分布 ![image](https://user-images.githubusercontent.com/25688193/56272299-62c8d280-6135-11e9-8467-89448e51e0d0.png) との間のJSダイバージェンスは、<br>

![image](https://user-images.githubusercontent.com/25688193/56332894-02856f80-61cd-11e9-9b0d-1e8993fbae3e.png)<br>

ここで、上式の赤字の項 ![image](https://user-images.githubusercontent.com/25688193/56332952-3cef0c80-61cd-11e9-8cc2-78d1f05581a7.png) は、識別器の損失関数そのものであるので、損失関数を最小化することは、JSダイバージェンスを最小化することと等価であることが分かる。<br>
そして、このJSダイバージェンスを最小化することは、モデルの分布と真の分布の２つの分布を一致させることであるので、<br>
結局のところ、<br>
「GANの学習の目的であるモデルの分布と真の分布を近づけ ![image](https://user-images.githubusercontent.com/25688193/56332861-ea155500-61cc-11e9-8032-98293e456e86.png) とすること。」<br>
⇔「（真の分布とモデルの分布の）JSダイバージェンスを最小化すること。」<br>
⇔「（識別器の）損失関数を最小化すること。」<br>
の関係が成り立つことが分かる。<br>

<a id="GANの収束性"></a>

### ◎ GANの学習の困難さ
GANの学習の難しさの要因には、大きく分けて以下のような要因がある。

- 損失関数の収束性の問題
- モード崩壊
- 勾配損失問題
- 生成画像のクオリティーの評価が、損失関数から判断し難い


<a id="GANの収束性"></a>

#### ☆ GANの収束性
GANの学習の難しさの１つの要因に、損失関数の収束性の問題がある。<br>

- GANは、識別器と生成器の２人プレイヤーゼロサムゲームになっているが、このゲームの最適解は、ナッシュ均衡点になる。<br>

- ２人プレイヤーゼロサムゲームのナッシュ均衡点は、鞍点になる。<br>
    ![image](https://user-images.githubusercontent.com/25688193/56336649-213f3280-61dc-11e9-8b55-9360b88fcc82.png)<br>

- 識別器の損失関数<br>
    ![image](https://user-images.githubusercontent.com/25688193/56336686-3f0c9780-61dc-11e9-9719-49f1faf7b2d5.png)<br>
    の形状が、上図のような凸関数であれば、SGDによってナッシュ均衡点（＝鞍点）に収束されることが保証されるが、非凸関数の場合は、保証されない。（GANではニューラルネットワークにより損失関数を表現するので、関数の形状は非凸関数）<br>

- このような非凸関数に対して、SGDで最適化（＝鞍点の探査）を行っていくと、ナッシュ均衡点（＝鞍点）に行かず、振動する可能性がある。<br>
    例えば、損失関数が L(a,b)=a×b という単純な形であった場合にも、SGDで生成器と識別器を交互に最適化していくと、以下の図のように、損失関数の値が発散する。<br>
    ![image](https://user-images.githubusercontent.com/25688193/56336712-4f247700-61dc-11e9-9557-f4f661c2d747.png)<br>

<a id="モード崩壊"></a>

#### ☆ モード崩壊 [Mode collapse]
学習が不十分な識別器に対して、生成器を最適化した場合や、生成器への入力ノイズ z の潜在変数としての次元が足りたていない場合などにおいて、生成器による生成画像が、ある特定の画像（例えば、MNISTでは数字の３などの特定の画像）に集中してしまい、学習用データが本来持っている多様な種類の画像（例えば、MNISTでは数字の0~9の画像）を生成できなくなってしまう問題がある。<br>
GANにおいて発生するこのような問題を、モード崩壊といい、GANの学習の困難さの要因の１つになっている。<br>
※ ここでいうモード（流行）とは、最頻出値のこと。<br>

<!--
> 自作の図を要追加
![image](https://user-images.githubusercontent.com/25688193/56337604-10dd8680-61e1-11e9-8c81-fdb1090c67e2.png)<br>
-->

<a id="勾配損失問題"></a>

#### ☆ 勾配損失問題
先に見たように、学習が十分でない識別器に対して、生成器を最適化すると、モード崩壊が発生してしまう。<br>
それを防ぐために、ある生成器 G の状態に対しての識別器を完全に学習すると、今度は勾配損失問題が発生してしまう。<br>
このように、GANでは、モード崩壊と勾配損失問題が互いに反して発生してしまうというジレンマを抱えている。<br>


<a id="DCGAN"></a>

## ■ DCGAN [Deep Convolutional GAN]
DCGAN は、GAN における生成器と識別器の内部を、「”プーリング層なし”の全て畳み込み層で構成される CNN 」に置き換えたものである。<br>

CNN は、画像認識タスクにおいて、優れたモデルになっているが、DCGAN もまた、画像生成において、（通常の GAN や他の生成モデルに比べて）優れたモデルになっている。<br>


<a id="DCGANのアーキテクチャ"></a>

### ◎ DCGAN のアーキテクチャ
![image](https://user-images.githubusercontent.com/25688193/57172563-c0129280-6e5c-11e9-941a-dc29501e74a3.png)<br>

![image](https://user-images.githubusercontent.com/25688193/57172588-226b9300-6e5d-11e9-8c40-d7e348a3debe.png)<br>

上図は、DCGAN の全体のアーキテクチャ（ネットワーク構成は略式表記）と、生成器・識別器内部の詳細なネットワーク構成を示したアーキテクチャ図である。<br>
DCGAN における生成器と識別器の構造は、以下のような特徴を持つ。<br>

➀ 生成器：Generator<br>

- 通常の CNN とは異なり、プーリング処理を行わない。（プーリング層は存在しない）<br>
- deconvolution（逆畳み込み）を用いて、アップサンプリングする。<br>
    ※ deconvolution という言葉は別の他の手法で既出のため、厳密には、deconvolution ではなく fractionally-strided convolution 或いは transposed convolution である。<br>
- 活性化関数として、Relu を使用。但し、出力層の活性化関数は tanh（※通常の GAN の generator と同じ）<br>
- batch normalization で学習させる。<br>
    但し、全ての層に適用されると学習が不安定となってしまうため、generator の出力層には適用しないようにする。<br>

➁ 識別器：Discriminator<br>

- 通常の CNN とは異なり、プーリング処理を行わない。（プーリング層は存在しない）<br>
- その代わりに、ストライド幅 2 の畳み込みで、ダウンサンプリングする。<br>
- 全結合層 [fully connected layer] を取り除き、代わりに、GAP : global average pooling で置き換える。<br>
    即ち、１つの特徴マップに対し、それらの平均をとって、１つの出力ノードに対応させる。（下図）<br>
    ![image](https://user-images.githubusercontent.com/25688193/57119115-c3494800-6da2-11e9-89cb-1e6c8640683c.png)<br>

- 通常の GAN の discriminator とは異なり、全ての層の活性化関数として、<br>
    Leaky ReLU (![image](https://user-images.githubusercontent.com/25688193/57119196-436fad80-6da3-11e9-9870-7db07a0f43e6.png)) を使用する。<br>
    これは、通常の Relu では、x≦0 の領域で、勾配が 0 になるために、学習を進めることが出来なくなってしまうのに対し、Leaky Relu では、x≦0 の領域でも、勾配が 0 とはならないので、学習を進めることが出来るようになることに起因する。<br>
    ![image](https://user-images.githubusercontent.com/25688193/57119284-e3c5d200-6da3-11e9-9c5f-c304098cd6ef.png)<br>

- batch normalization で学習させる。<br>
    但し、全ての層に適用されると学習が不安定となってしまうため、discriminator の入力層には適用しないようにする。<br>


<a id="DCGANの適用例"></a>

### ◎ DCGAN の適用例

- [DCGAN を利用した手書き文字（MNIST）の自動生成](https://github.com/Yagami360/MachineLearning_Exercises_Python_PyTorch/tree/master/GAN_DCGAN_PyTorch)


<a id="ConditionalGAN（cGAN）"></a>

## ■ Conditional GAN（cGAN）
従来の GAN では、潜在変数としての入力ノイズ z の値を動かすことで、様々な種類の画像を生成することができた。<br>
しかしながら、画像の生成過程において、今生成している画像がどのような種類（例えば、MNISTでは数字の番号）の画像であるのかを、GAN自身が知ることはできないかった。<br>

Conditional GAN（cGAN）は、従来の GAN の入力に、生成する画像のクラスラベルなどの条件 y を付与することにより、GAN自身に、今どのような種類の画像を生成しているのか？といった、データの生成過程を指示することを可能にし、結果として、条件 y で指定した特定の画像のみを生成することが出来るようにする。<br>


<a id="cGANのアーキテクチャ"></a>

### ◎ cGAN のアーキテクチャ
![image](https://user-images.githubusercontent.com/25688193/57116464-42815080-6d90-11e9-8d21-db015d449e10.png)<br>

上図は、cGAN のアーキテクチャを示した図である。<br>
cGAN では、従来の GAN に対して、生成器の入力と識別器の入力の双方に、正解となるクラスラベルなど条件 y を入力している。<br>
※ このアーキテクチャ図では、代表的な条件 y として、クラスラベルとしているが、他にも様々な条件（アノテーションなど）を入力することが出来る。<br>

但し、この例では、GANのアーキテクチャに対応できるように、<br>

- 識別器へのクラスラベル y の入力は、one-hot エンコードされた画像データのフォーマット（正解画像を白画像、それ以外を黒画像）で入力し、
- 生成器へのクラスラベル y の入力は one-hot エンコードされたバイナリデータの系列のフォーマット（正解を0、それ以外を０）で入力している。

※ cGAN のアルゴリズム自体には、このようなエンコード手法などの具体的な含まれないことに注意。<br>
（従来のGANのアーキテクチャに組み込める形であれば、どのような方法でもよい。）<br>


<a id="cGANの損失関数"></a>

### ◎ cGAN の損失関数
cGAN の生成器と識別器の損失関数は、従来のGANの損失関数に対して、クラスラベルなどの追加条件 y の制約を加えた式（＝条件付き確率）で表現できる。<br>
即ち、識別器の損失関数 L_D  は、<br>
![image](https://user-images.githubusercontent.com/25688193/56723413-dd23d300-6783-11e9-86e3-50c8aaff09ea.png)<br>

生成器の損失関数 L_G  は、<br>
![image](https://user-images.githubusercontent.com/25688193/56723450-f0cf3980-6783-11e9-9362-302766cc6114.png)<br>

そして、先のアーキテクチャ図で示しているように、従来の GAN と同じく、これらの損失関数を誤差逆伝搬法で逆伝搬することで、生成器と識別器の学習を逐次行う。<br>


<a id="cGANの適用例"></a>

### ◎ cGAN の適用例

- [cGAN を利用した手書き文字（MNIST）の自動生成](https://github.com/Yagami360/MachineLearning_Exercises_Python_PyTorch/tree/master/GAN_cGAN_PyTorch)


<a id="WGAN"></a>

## ■ WGAN [Wasserstein GAN]
既存のGANにおける（識別器の）学習は、先で見たように、JSダイバージェンスを最小化していることと同値であった。<br>
しかしながら、このJSダイバージェンスの最小化に基づく学習では、以下のような問題点が存在する。（詳細は後述）<br>

- 真の分布 ![image](https://user-images.githubusercontent.com/25688193/56492380-9a18f400-6526-11e9-8d35-7687a906526f.png) とモデルの分布 ![image](https://user-images.githubusercontent.com/25688193/56492448-d0ef0a00-6526-11e9-8c9b-79fb36a75af0.png) の台（Supp）が重ならない場合において、勾配損失問題が発生する。<br>

- 学習回数に対する loss 値の変化の様子が、生成画像のクオリティに比例しない。（＝生成画像のクオリティが損失関数の値から判断しずらい）<br>

- モード崩壊が発生することがある。

- 損失関数の収束性に問題があり、学習が不安定

そこで、Wasserein GAN では、JSダイバージェンスではなく、Earth-Mover 距離（EMD）（＝Wassertein距離ともいう）と呼ばれる別の距離指標を使用する。
（※厳密には、このWassertein距離の双対表現の式を近似した式）<br>
これにより、既存の GAN（＝JSダイバージェンス最小化）で発生する上記の問題を回避することが出来る。<br>


<a id="JSダイバージェンスとEarth-Mover距離の収束性の違いと勾配消失問題"></a>

### ◎ JSダイバージェンスとEarth-Mover距離の収束性の違いと勾配消失問題
概略で挙げたように、従来のGANにおける問題点の１つは、<br>

- 真の分布とモデルの分布の台が重ならない場合において、勾配消失問題が発生する。<br>

という問題である。<br>
ここでは、この問題点の詳細と、WGANにおける解決策を見ていく。<br>

![image](https://user-images.githubusercontent.com/25688193/56465595-60b78a00-643b-11e9-841d-e4e704f29b57.png)<br>

GANでの学習の目的は、真の分布 ![image](https://user-images.githubusercontent.com/25688193/56492380-9a18f400-6526-11e9-8d35-7687a906526f.png) とモデルの分布 ![image](https://user-images.githubusercontent.com/25688193/56492448-d0ef0a00-6526-11e9-8c9b-79fb36a75af0.png) の２つの確率分布を互いに一致させることであるが、この２つの確率分布の台（Suup）が、上図のように互いに重ならない場合を考える。<br>
※ θ は、生成器のニューラルネットワークの重みパラメーター<br>

このような２つの確率分布の台が交わらない極端なケースの例として、以下のような図のようなケースで、JSダイバージェンスを計算する。<br>

![image](https://user-images.githubusercontent.com/25688193/56465614-f18e6580-643b-11e9-9ab3-5d67522c4e75.png)<br>

![image](https://user-images.githubusercontent.com/25688193/56465620-04089f00-643c-11e9-8eeb-be4c269fd7ce.png)<br>

この計算結果を図示すると、以下のような図となる。<br>

![image](https://user-images.githubusercontent.com/25688193/56465640-4c27c180-643c-11e9-8103-8dcc5f8afaba.png)<br>

真の分布とモデルの分布が重なる θ=0 の部分で、JSダイバージェンスの値が不連続となっている。一方、真の分布とモデルの分布が重ならない θ≠0 の部分では、JSダイバージェンスの値が一定値となっていることが分かる。<br>
※ この様子を確率分布の系列 ![image](https://user-images.githubusercontent.com/25688193/56465651-842f0480-643c-11e9-97f1-4947a7f7399a.png) で考えると、この系列は、JSダイバージェンスのもとで、θ_t→0 で収束していないとも言える。<br>

ここで、GANの目的は、真の分布とモデルの分布を一致させるでことあり、これをJSダイバージェンスの最小化で行っていた。<br>
しかしながら、上図では、学習の余地がある、真の分布とモデルの分布が重ならない θ≠0 の部分において、JSダイバージェンスの値が一定値となっているために、その勾配の値が０になっていまうために、結果として、JSダイバージェンスを最小化して、誤差逆伝搬で学習するのに必要な勾配値が得られないという、いわゆる勾配消失問題が発生していることが分かる。<br>

このように、真の分布とモデルの分布の台（Supp）が互いに重ならない場合において、従来のGANのJSダイバージェンスの最小化による学習では、勾配消失問題が発生してしまう。<br>

<br>

Wassertein GAN では、この勾配消失問題を回避するために、Earth-Mover距離（Wassertein距離）を使用するが、このEarth-Mover距離（Wassertein距離）は、以下のように定義される。<br>
（※ Earth-Mover距離 の詳細は、「[【補足】Earth-Mover 距離（Wassertein距離）](https://github.com/Yagami360/My_NoteBook/blob/master/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6_%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92_%E7%94%9F%E6%88%90%E3%83%A2%E3%83%87%E3%83%AB.md#EMD)」参照のこと）<br>

![image](https://user-images.githubusercontent.com/25688193/56477963-17704480-64e6-11e9-9396-a72c7cae56af.png)<br>

このEarth-Mover距離を、先のJSダイバージェンスと同様にして、２つの確率分布の台が交わらない極端なケースで計算すると、<br>
![image](https://user-images.githubusercontent.com/25688193/56480315-7b016e80-64f4-11e9-860e-fd3963c90c7a.png)<br>
となる。<br>
この計算結果を図示すると、以下のような図となる。<br>
![image](https://user-images.githubusercontent.com/25688193/56480337-8eacd500-64f4-11e9-908a-cde46f6e8743.png)<br>

真の分布とモデルの分布が重なる θ=0 の部分で、EMDの値が０なっている。
一方、真の分布とモデルの分布が重ならない θ≠0 の部分では、EMDの勾配が一定値となっていることが分かる。<br>
つまり、学習が完了していない、真の分布とモデルの分布が重ならない領域において、誤差逆伝搬での学習に必要な、EMDの勾配値が得られているので、勾配消失問題は発生していないことが分かる。<br>


<a id="WGANのアーキテクチャと損失関数"></a>

### ◎  WGANのアーキテクチャと損失関数
先でみたように、Wassertein GAN では、以下のように定義される、Earth-Mover距離（Wassertein距離）を使用する。<br>

![image](https://user-images.githubusercontent.com/25688193/56480424-f82ce380-64f4-11e9-9952-d4f72fbd4aaa.png)<br>

但し、この式では、GANで扱うような画像の次元が大きい場合に、現実的に計算可能ではないので、この式の双対表現の式を利用する。<br>
※ このEarth-Mover距離を計算することは、線形計画法を解くことに一致するので、元の式（主問題）に対する双対表現が得られる。<br>

即ち、<br>
![image](https://user-images.githubusercontent.com/25688193/56480447-11359480-64f5-11e9-9ff8-71541144ff8d.png)<br>

ここで、関数 ![image](https://user-images.githubusercontent.com/25688193/56480469-30342680-64f5-11e9-8519-a93d37cfc1ba.png) は、K-リプシッツ連続な関数であることが、元のEarth-Mover距離の制約条件から要請される。<br>

![image](https://user-images.githubusercontent.com/25688193/56483115-c28ef700-6502-11e9-9d55-67132cc24464.png)<br>

また、このリプシッツ連続性の条件は、先の２つの確率分布の台が互いに重ならない場合で発生する勾配消失問題でみたように、一定の勾配が得られて学習がうまくいくための要件になっているとみなすことも出来る。（上図参照）<br>

![image](https://user-images.githubusercontent.com/25688193/57116586-10242300-6d91-11e9-9d96-d2e4683c6c6d.png)<br>

WGANでは、上図のアーキテクチャ図のように、このリプシッツ連続な関数 f を、
ニューラルネットワークで構成されたクリティック（＝従来の識別器）で表現する。<br>

※ このクリティックは、従来のGANの識別器のように、入力データが本物か偽物かの 0 or 1 の２値を sigmoid 活性化関数で活性出力するのではなく、リプシッツ連続な関数 f の出力（＝連続値になる）をそのまま出力する。そのため、もはや従来の識別器とは異なる機能となっているため、識別器ではなくクリティックという。<br>

このとき、クリティックのニューラルネットワークの重みパラメーターを w とし、この関数 f をパラメーター付きの関数![image](https://user-images.githubusercontent.com/25688193/56492322-5de59380-6526-11e9-87de-75ae8e5916db.png)で表記すると、先の Earth-Mover 距離の双対表現の式は、以下の式で近似できる。（導出略）<br>

![image](https://user-images.githubusercontent.com/25688193/56483339-4d242600-6504-11e9-9523-f94d78cbe79f.png)<br>

そしてWGANでは、クリティックの損失関数 ![image](https://user-images.githubusercontent.com/25688193/56492511-fd0a8b00-6526-11e9-8ed3-38e305f10c56.png) として、このEarth-Mover 距離の双対表現の近似式で採用する。<br>
そして、上記のアーキテクチャ図で示したように、この勾配を誤差逆伝搬することで、学習を行う。<br>


<a id="WGANでのその他の工夫"></a>

### ◎ WGAN でのその他の工夫

- クリティックの学習と生成器の学習の更新間隔：<br>
    従来のGANでは、損失関数（＝JSダイバージェンス）が途中で勾配消失するために、ある生成器の出力に対して、最適途中までしか識別器を学習させることしかできず、そのために、<br>
    「識別器の損失関数を更新（１回）→生成器の損失関数の更新（１回）」 を、各々１回ずつ繰り返すことで、生成器と識別器の学習を逐次行っていた。<br>

    一方、先に述べたように、WGANの損失関数（＝EMD）では、ほとんどいたるところで連続で微分可能あり、誤差逆伝搬での学習のための勾配が得られ続けるので、ある生成器の出力に対して、クリティックの最適状態まで、学習を続けることが可能である。<br>

    従って、WGANでは、正しい損失関数を計算するために<br>
    「識別器の損失関数を更新（n_critic=5 回）→生成器の損失関数の更新（１回）」というように繰り返すことで、生成器と識別器の学習を逐次行なう。<br>
    これにより、安定した学習や、モード崩壊の防止を実現できる。<br>

- リプシッツ連続な関数 f の実現と重みクリッピング：<br>
    ここで、何度も述べているように、クリティックの出力となる関数![image](https://user-images.githubusercontent.com/25688193/56492322-5de59380-6526-11e9-87de-75ae8e5916db.png)は、∀w∈W に対して、リプシッツ連続な関数である必要があるが、WGANでは、このことをニューラルネットワークで実現するために、単純に、−c≤w≤c (例えば c=0.01)  の範囲で重みクリッピングを行うことで実現する。<br>

    ※ 但し、この重みクリッピングによる方法では、勾配爆発や、低い値にクリッピングしすぎることによる勾配消失の問題が発生する可能性がある。<br>
    ※ 後述の WGAN-gp では、このような問題が起らないように、重みクリッピングによるリプシッツ連続性の実現ではなく、勾配ノルムに制限項を加えることで、リプシッツ連続性を実現する。（詳細は後述）

- 最適化アルゴリズムの選択：<br>
	最適化アルゴリズムとして、Adam のようなモーメンタムベースの最適化アルゴリズムを使用すると、ときどき学習が不安定になるという実験的結果がある。（※ この理論的根拠はまだない？）v
	そのため論文では、最適化アルゴリズムとして、モーメンタムベースのアルゴリズムではない RMSProp を採用している。<br>


<a id="WGANのアルゴリズム"></a>

### ◎ WGAN のアルゴリズム
以上のことを踏まえて、最終的に WGAN のアルゴリズムは、以下のようになる。<br>

![image](https://user-images.githubusercontent.com/25688193/56484127-f40ac100-6508-11e9-92c7-e1581e138ae0.png)<br>


<a id="WGANのその他の利点"></a>

### ◎ WGAN のその他の利点
先に述べたように、WGANでは、従来の GAN で問題になっていた、（真の分布とモデルの分布の台が互いに重ならない場合に発生する）勾配消失問題を回避することが出来るが、その他にも以下に上げるようないくつかの利点が存在する。<br>

※ 勾配消失問題の回避を含めたこれらの利点の原因は、互いに独立したものではなく、いづれもWGANの損失関数（＝EMDの近似）の連続性やクリティックの出力のリプシッツ連続性に起因していることに注目。<br>

- 学習が安定している：<br>
	先に述べたように、WGANの損失関数（＝EMD）では、ほとんどいたるところで連続で微分可能あり、誤差逆伝搬での学習のための勾配が得られ続けるので、ある生成器の出力に対して、クリティックの最適状態まで、学習を続けることが可能である。<br>
	従って、WGANでは、正しい損失関数を計算するために、<br>
	「識別器の損失関数を更新（n_critic  回）→生成器の損失関数の更新（１回）」　というように繰り返すことで、生成器と識別器の学習を逐次行うことが可能となり、<br>
	従来のGANでの、「識別器の損失関数を更新（１回）→生成器の損失関数の更新（１回）」 での方式より、学習が安定化する。<br>

- モード崩壊が発生しない：<br>
	モード崩壊は、十分に最適化されたいない識別器に対して、生成器を最適化し、画像を生成することで発生する。<br>
	先に述べたように、WGANの損失関数（＝EMDの近似）では、ほとんどいたるところで連続で微分可能あり、誤差逆伝搬での学習のための勾配が得られ続けるので、ある生成器の出力に対して、クリティック（＝識別器）の最適状態まで、学習を続けることが可能である。<br>
	従って、モード崩壊の発生原因である、十分に最適化されたいないクリティック（＝識別器）という状況が発生しないため、モード崩壊も発生しない。<br>

- 生成画像のクオリティを loss値から判断できる：<br>
    従来のGANにおいては、学習の経過において、loss 値が減少したからといって、必ずしも生成画像のクオリティが良くなるとは限らない。<br>
	そのため、良いクオリティの生成画像を得るために、学習をいつ止めるべきなのかを目視していなくてはいけないという問題が存在した。<br>
    <br>
    一方 WGAN では、学習回数に対する loss 値の変化の様子が、生成画像のクオリティに比例することが、実験的に示されている。（理論的には？）<br>
    そのため、良いクオリティの生成画像を得るために、生成画像が都度目視せずとも、学習の完了（＝loss値の０付近への収束）まで待つだけでよい。<br>
    このことを示したのが、以下の図である。<br>
    ※ この性質は、理論的には、クリティックの出力が、リプシッツ連続で、ほとんどいたるところで線形な関数で表現できていることに起因？

    ![image](https://user-images.githubusercontent.com/25688193/56484634-84e29c00-650b-11e9-8174-9c5167e1010c.png)<br>


<a id="WGANの適用例"></a>

### ◎ WGAN の適用例

- [WGAN を利用した手書き文字（MNIST）の自動生成](https://github.com/Yagami360/MachineLearning_Exercises_Python_PyTorch/tree/master/GAN_WGAN_PyTorch)


<a id="pix2pix"></a>

## ■ pix2pix
従来の image-to-image 変換タスク（＝１対１に対応している画像間を変換すること）では、タスクの種類に応じて個別にアルゴリズムを構築していた。<br>
pix2pix は、image-to-image 変換タスクを、cGAN を用いて実現することにより、様々な種類の image-to-image 変換タスクを、汎用的なフレームワークで実現出来る。<br>
※ 通常の GAN では、生成タスクが主な用途であるが、pix2pix はこのような image-to-image の変換タスクが主な用途である。<br>

具体的には、以下の例のような、image-to-image 変換タスクを共通のフレームワークで実現できる。<br>

- 航空写真から地図を生成<br>
    ![image](https://user-images.githubusercontent.com/25688193/57005997-df6cae00-6c17-11e9-88c7-e664288a75a8.png)<br>

- 輪郭線を画像で埋める<br>
    ![image](https://user-images.githubusercontent.com/25688193/57005999-e7c4e900-6c17-11e9-95f0-2d5d362318d6.png)<br>

- 画像をカラー化する<br>
    ![image](https://user-images.githubusercontent.com/25688193/57006008-fa3f2280-6c17-11e9-906c-f6ba9fe55d17.png)<br>

- 道路や建物の塗り分けから画像生成<br>
    ![image](https://user-images.githubusercontent.com/25688193/57006021-1a6ee180-6c18-11e9-805f-a4fe8ee47473.png)<br>


<a id="pix2pixのアーキテクチャ"></a>

### ◎ pix2pix のアーキテクチャ
![image](https://user-images.githubusercontent.com/25688193/57006356-11cbda80-6c1b-11e9-9f92-98c5ee2b175c.png)<br>

上図は、例えば、航空写真を地図に変換する image-to-image 変換タスクにおける、pix2pix のアーキテクチャ図を示した図である。<br>
この pix2pix のアーキテクチャには、主に、以下のような特徴がある。<br>

- アーキテクチャのベースとしては、cGAN を採用：<br>
    cGAN のアルゴリズムにおいて、入力画像 x を変換前の画像（航空写真）とし、追加条件 y として、変換後の画像（地図画像）とすることで、cGAN での image-to-image 変換タスクを実現している。<br>

- 生成器 G のネットワークとしては、U-Net を採用：<br>
    - U-Net は、セマンティックセグメンテーションのタスクにおいて重要となる「局所的な特徴量と、画像全体の特徴の両方を捉えること」 を同時に実現できる。
    - より詳細には、Encoder 側の浅い層から、画像全体の大域的な特徴量を skip connection 経由で、Decoder 側に送り、Encoder 側の深い層からの、画像の局所的な特徴量を skip connection 経由で、Decoder 側に送る。<br>
        そして、Decoder 側で、これら skip connection で送られてきた大域的特徴量と局所的特徴量を保持したたま、アップサンプリングを行い、変換前と同じ解像度の画像を出力する。<br>
        結果として、「局所的な特徴量と、画像全体の特徴の両方を捉えること」 を同時に実現できる。（詳細は、[【補足】U-Net](https://github.com/Yagami360/My_NoteBook/blob/master/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6_%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92_%E7%94%9F%E6%88%90%E3%83%A2%E3%83%87%E3%83%AB.md#UNet) の項目参照）
    - pix2pix では、生成器のネットワーク構成として、U-Net を採用することで、このセマンティックセグメンテーションのタスクにおいて重要となる「局所的な特徴量と、画像全体の特徴の両方を捉えること」 を同時に実現できるようにしている。<br>

- 識別器 D には、Patch GAN を採用：<br>
    - Patch GAN では、識別器に入力された画像をより小さな（＝局所的な）複数の小領域（＝パッチ）に分解した上で、これら各バッチに対して、本物か偽物かの判定を行い、最後に、全ての応答を平均化して、識別器の最終的な出力とする。<br>
    - この Patch GAN の仕組みにより、識別器がある程度大域的な判定を残しつつ、局所的な特徴量でのみ判定に専念できるので、学習パラメーター数を大幅に減らすことができ、結果として、学習を効率的に進めることができる。<br>
    - 尚、Patch GAN という名前がついているが、この PatchGAN 自体は、GAN 全体のアルゴリズムではなく、単に、識別器のみに対してのアルゴリズムであることに注意。<br>

- 生成器 G に入力する入力ノイズ z は、dropout で実現：<br>
    - 生成器 G に入力する入力ノイズ z は、従来の GAN のように、確率分布 U(0,1)  or N(0,1)  から直接サンプリングして実現するのではなく、生成器のネットワークの複数の中間層に、直接 dropout を施すという意味でのノイズとして実現する。<br>

このようなアーキテクチャのもとで、識別器 D は、入力されたデータのペアが、学習データに含まれている｛変換前の画像（本物の航空写真）, 変換後の画像（地図）｝というペアなのか、或いは、｛ 変換元画像から生成器が生成した画像（偽物の航空写真）、変換後の画像（地図）＞ のペアなのかという判断を行う。<br>

そして、従来の GAN と同じように、この識別器による判定結果が、50% - 50% になるように、生成器と識別器の学習を、交互に実施していく。


<a id="pix2pixの損失関数"></a>

### ◎ pix2pix の損失関数
pix2pix は、cGAN をベースに構築されているが、cGAN の識別器の損失関数 L_D と生成器の損失関数 L_G は、以下のように定義された。<br>

![image](https://user-images.githubusercontent.com/25688193/57006110-0d062700-6c19-11e9-908c-39d457fe628a.png)<br>

pix2pix でのアーキテクチャにそうように書き換えると、<br>

![image](https://user-images.githubusercontent.com/25688193/57006122-21e2ba80-6c19-11e9-9709-e40b49281733.png)<br>

pix2pix では更に、生成器での損失関数 L_G に対して、この cGAN の損失関数をベースにして、L1正則化の項<br>

![image](https://user-images.githubusercontent.com/25688193/57006135-42ab1000-6c19-11e9-9092-821fde5aca8a.png)<br>
を追加する。<br>
これにより、生成器での損失関数は、以下のようになる。<br>

![image](https://user-images.githubusercontent.com/25688193/57010909-1784e880-6c3a-11e9-9403-08c3ac3490f8.png)<br>

この L1正則化の項目は、変換後の本物画像 x と生成器が生成した偽物画像 ![image](https://user-images.githubusercontent.com/25688193/57006148-6d956400-6c19-11e9-9af3-d6860b30f3dc.png) が、”ピクセル単位” でどの程度異なるのか（＝局所的な特徴量）を表している。<br>
セマンティックセグメンテーションのタスクにおいては、「局所的な特徴量と、画像全体の特徴の両方を捉えること」 が重要となる。<br>
pix2pix では、既存の cGAN の損失関数に、L1正則化項を追加することにより、
局所的な特徴量をL1正則化で捉え、全体的な情報の正しさを識別器で捉え判定することを可能にしている。<br>
※ 同じような理由で、L2正則化も考えられるが、L2よりL1のほうが、生成画像のぼやけが少ない傾向があるので、pix2pix では、L1を採用している。<br>


<a id="pix2pixの適用例"></a>

### ◎ pix2pix の適用例

- [pix2pix を利用して、衛星画像から地図を生成する。](https://github.com/Yagami360/MachineLearning_Exercises_Python_PyTorch/tree/master/Pix2Pix_PyTorch)


<a id="CycleGAN"></a>

## ■ CycleGAN
image-to-image 変換のタスクにおいて、一般的に利用可能な学習用データは、左下図のような x と y が対応した画像でペア付けされた学習用データではなく、その多くは右下図のような対応した画像でペア付けされていない学習用データである。<br>

![image](https://user-images.githubusercontent.com/25688193/59178850-2e085300-8b9b-11e9-8500-4de333f1156d.png)<br>

CycleGAN は、このようなペア付けされていないx to学習用データに対しても、image-to-image 変換タスクが適用できるようにした、GAN ベースの image-to-image アルゴリズムの１つである。

その主な特徴は、GANの生成器において、入力画像 x から変換画像 y への写像 ![image](https://user-images.githubusercontent.com/25688193/59179063-d3bbc200-8b9b-11e9-91d7-226f9b42380f.png) を学習するだけでなく、別の生成器で、その逆の変換画像 y から元の入力画像 x への写像 ![image](https://user-images.githubusercontent.com/25688193/59179253-73795000-8b9c-11e9-8a5e-1cb2a2a8ed72.png) も学習し、これら写像（＝生成器）G, F による各々の写像先を一致させる（ ![image](https://user-images.githubusercontent.com/25688193/59179033-bd156b00-8b9b-11e9-8d79-7718d861292f.png) ）ための cycle consistency loss を導入しいる点にある。<br>

この cycle consistent な仕組みにより、ペア付けされていない学習用データに対しても、image-to-image 変換タスクが適用出来るようになり、従来の image-to-image 変換手法（pix2pix など）よりも優れたパフォーマンスを発揮することが可能となる。<br>


<a id="CycleGANのアルゴリズム"></a>

### ◎ CycleGAN のアルゴリズム
![image](https://user-images.githubusercontent.com/25688193/59238384-e1735500-8c38-11e9-96ee-ef24c70c8f54.png)<br>

上図は、例えば、地図を航空写真に変換する image-to-image 変換タスクにおける CycleGAN のアーキテクチャ図を示した図である。<br>
この CycleGAN のアーキテクチャには、主に、以下のような特徴がある。<br>

- cycle consistent な仕組み：<br>
    従来の GAN での生成器では、入力画像 x から変換画像 y への写像のみを、真の分布とモデルの分布が一致するように学習するが、このように学習された写像に関しての入力画像 x と変換画像 y の対応関係が、image-to-image の意味で意味のある画像のペアになっていることは保証されない。<br>

    これは、このような真の分布とモデルの分布の一致させ方が、その制約の弱さ（＝自由度の高さ）故に、無限通り存在し、結果として得られる学習された写像も、無限通り存在してしまうことに起因する。<br>
    そのため、CycleGAN では、cycle consistent な仕組みにより、写像の制約を強め（＝自由度を小さくし）、学習された写像に関しての入力画像 x と変換画像 y の対応関係が、image-to-image の意味で意味のある画像のペアになるようにする。<br>

    具体的には、上図のアーキテクチャ図のように、変換画像を元の画像に戻す生成器（＝写像）![image](https://user-images.githubusercontent.com/25688193/59179253-73795000-8b9c-11e9-8a5e-1cb2a2a8ed72.png) を導入し、これら写像（＝生成器） G, F による各々の写像先を一致させる（![image](https://user-images.githubusercontent.com/25688193/59179033-bd156b00-8b9b-11e9-8d79-7718d861292f.png)）ようにするための、cycle consistency loss を導入する。（※ この損失関数に関しての詳細は後述）<br>
    この順方向の cycle consistency 要件 ![image](https://user-images.githubusercontent.com/25688193/59239166-94dd4900-8c3b-11e9-9ce4-6f3f72a805fd.png) と、その逆方向の cycle consistency 要件 ![image](https://user-images.githubusercontent.com/25688193/59239248-c9510500-8c3b-11e9-88c1-c273bbf31381.png) により、写像の制約が強まり、結果として得られる学習された写像の取り得る範囲も狭まり、この写像に関しての入力画像 x と変換画像 y の対応関係が、image-to-image の意味で意味のある画像のペアになるようになる。<br>

- CycleGAN の損失関数：<br>
    CycleGAN の損失関数は、以下で定義される adversarial loss と consistency loss の和で定義される。<br>
    ![image](https://user-images.githubusercontent.com/25688193/59243062-1f787500-8c49-11e9-8e36-0443ca3cfc55.png)<br>
    そして、この損失関数の minmax で生成器と識別器を最適化する。<br>
    ![image](https://user-images.githubusercontent.com/25688193/59243104-48990580-8c49-11e9-9da9-8c82e2d78215.png)<br>

    - adversarial loss :<br>

        ![image](https://user-images.githubusercontent.com/25688193/59239683-4cbf2600-8c3d-11e9-815c-450afa6f7007.png)<br>

        従来の GAN のように、本物画像と生成器が生成した偽物画像に対しての、識別器の判定結果を元にした損失関数で、真の分布とモデルの分布の２つの分布を一致させるように働く。<br>
        但し、CycleGAN では、上図のアーキテクチャ図のように、順方向の写像を学習する生成器 G とそれに対応した識別器 D_Y。及び、逆方向の写像を学習する生成器 F とそれに対応した識別器 D_X の２つの経路が存在するので、adversarial loss もそれに対応して２つ存在することになる。

    - cycle consistency loss :<br>

        ![image](https://user-images.githubusercontent.com/25688193/59240437-6a8d8a80-8c3f-11e9-858f-595e648cd44b.png)<br>

        cycle consistent 要件（![image](https://user-images.githubusercontent.com/25688193/59179033-bd156b00-8b9b-11e9-8d79-7718d861292f.png)）を実現する損失関数。<br>
        上図のアーキテクチャ図における緑線とオレンジ線で示した部分に対応しており、<br>
        変換前の画像 x を生成器 G で写像し、生成器 F で逆写像したときの、画像 F(G(x)) が、元の画像 x と一致する（![image](https://user-images.githubusercontent.com/25688193/59239166-94dd4900-8c3b-11e9-9ce4-6f3f72a805fd.png)）ように、その画像間のL１ノルム ![image](https://user-images.githubusercontent.com/25688193/59240672-b809f780-8c3f-11e9-80f7-bfb5257be114.png) で、順方向の cycle consistency loss を定義する。<br>

        同様にして、変換後の画像 y を生成器 F で写像し、生成器 G で逆写像したときの、画像 G(F(y)) が、元の画像 y と一致する（![image](https://user-images.githubusercontent.com/25688193/59239248-c9510500-8c3b-11e9-88c1-c273bbf31381.png)）ように、その画像間のL１ノルム ![image](https://user-images.githubusercontent.com/25688193/59240728-f7384880-8c3f-11e9-9589-4affd3a926db.png) で、逆方向の cycle consistency loss を定義する。<br>

        ※ 後述の実験結果で見るように、単方向の cycle consistency loss のみでは、学習が不安定になりモード崩壊が発生してしまう傾向があるため、双方向の cycle consistency loss を採用している。<br>

- 生成器のネットワーク構成：<br>

    ![image](https://user-images.githubusercontent.com/25688193/59247779-6a9b8380-8c5b-11e9-8477-a73f3d2f30cd.png)<br>
    生成器のネットワーク構成としては、上図のように、以下の３つのネットワークを順に連結したものを採用する。<br>
    ① ダウンサンプリングを行うための、ストライド幅２の２つの畳み込み層<br>
    ② いくつかの residual blocks（ResNet のブロック）<br>
    ③ アップサンプリングを行うための、ストライド幅 1/2 の２つの逆畳み込み層<br>

    ここで、ResNet の residual blocks 数は、入力画像のサイズによって切り替える。即ち、<br>
    128 × 128 の入力画像 ⇒ 6 つの residual blocks<br>
    256 × 256、及びそれ以上の解像度の入力画像 ⇒ 9 つの residual blocks<br>

    ※ このネットワーク構成は、Johnson らの論文「Perceptual Losses for Real-Time Style Transfer and Super-Resolution」 (https://arxiv.org/abs/1603.08155) で提案されているネットワーク構成をベースとしている。<br>

	※ pix2pix の生成器のネットワーク構成は、U-Net であるが、その後発である CycleGAN の生成器のネットワーク構成が、	Johnson らの論文ベースのネットワーク構成（conv + residual blocks + deconv）である理由は？<br>
    → 単純に生成画像の品質が向上するため？学習時間が短縮するため？あるいはその両方？<br>

- 識別器のネットワーク構成：<br>
    pix2pix と同様にして、PatchGAN の構造を採用する。（各パッチのサイズは 70 × 70 ピクセル）<br>
    この Patch GAN では、識別器に入力された画像をより小さな（＝局所的な）複数の小領域（＝パッチ）に分解した上で、これら各バッチに対して、本物か偽物かの判定を行い、最後に、全ての応答を平均化して、識別器の最終的な出力とする。<br>
    これにより、識別器がある程度大域的な判定を残しつつ、局所的な特徴量でのみ判定に専念できるので、学習パラメーター数を大幅に減らすことができ、結果として、学習を効率的に進めることができる。<br>

- instance normalization：<br>
    先の生成器のネットワーク構成でベースとしている Johnson らの論文と同じように、batch normalization ではなく、instance normalization を使用する。<br>

    ※ GAN による image-to-image 変換手法（CycleGAN, StarGAN、GANimation など）の多くで、batch norm ではなくて instance norm を採用しているのは、元々 Style Transfer の分野において、batch norm を instance norm に置き換えることで、生成画像の品質において大きな向上が観測が報告されたため、この知見取り入れたことによる。<br>

    <font color="Pink">※ batch norm を instance norm に置き換えることで、学習安定化の改善効果が得られるが、生成画像の品質の向上効果はあるのか？また、ablation study 等での生成画像の品質向上の定性的定量的比較結果はある？</font>

- 損失関数の誤差２乗和への置き換え：<br>
    学習を安定化させることを目的として、先に定義した損失関数<br>

    ![image](https://user-images.githubusercontent.com/25688193/59246054-18a42f00-8c56-11e9-889c-b2b225d34355.png)<br>
    における負の対数尤度計算（赤字部分）を、誤差２乗法の計算に置き換える。<br>
    これにより、生成器 G の学習は、<br>
    ![image](https://user-images.githubusercontent.com/25688193/59173839-bc73d900-8b89-11e9-88ff-a70e2c56893b.png)<br>
    で表現できる。<br>
    また識別器 D の学習は、![image](https://user-images.githubusercontent.com/25688193/59173922-12488100-8b8a-11e9-9368-17e647b9593b.png)<br>
    で表現できる。<br>
    ※ この手法は、他論文 (https://arxiv.org/abs/1611.04076) で提案されている手法。<br>

- 識別器の学習に生成器からの生成画像を履歴を使用：<br>
    一般的な GAN では、生成器が生成した最新の生成画像で識別器の学習を行うが、その方法では、損失関数のエポック数経過による変化が発振して不安定になる傾向がある。<br>
    CycleGAN では、損失関数を安定化させる目的で、生成器が生成した最新の生成画像ではなく、生成器が生成した画像の過去の履歴 50 回分のバッファを使用して識別器の学習を行う。<br>
    ※ この手法は、他論文(https://arxiv.org/abs/1612.07828) で提案されている Shrivastava et al’s strategy という手法。<br>


<a id="CycleGANの実験結果"></a>

### ◎ CycleGAN の実験結果
以下の表は、CycleGAN の論文中での各実験で共通する実験条件を示している。<br>
![image](https://user-images.githubusercontent.com/25688193/59248575-c8c96600-8c5d-11e9-9f93-6a7fe5ffd146.png)<br>
※ 学習用データ等のその他の実験条件は、各実験ごとに異なるものを使用。<br>

論文中の各実験は、以下のような評価指標に基づいて評価される。<br>

- AMT perceptual studies：<br>
    Cycle GAN が生成した偽物画像、或いは、性能比較のためのベースライン手法が生成した偽物画像と本物画像を被験者に見せて、被験者の主観的な判断で、その画像が本物か偽物かの判定を行う。<br>
    ツールとしては、Amazon Mechanical Turk (AMT)　を使用。<br>
    テストするアルゴリズム毎に、２５人の被験者でテスト。<br>
    試行回数は、50 回。（この内最初の 10 回は、被験者の正解不正解での反応をみるための練習試行。残りの 40 回が、各アルゴリズムが被験者をだました割合を計算するための本番試行）<br>
    地図 [map] ↔ 航空写真 [aerial photo] の image-to-image 変換タスクの実験評価で使用される。<br>

- FCN score :<br>
	全結合ネットワーク（FCN）で、生成された写真から、（セマンティックセグメンテーションされた）ラベルのラベルマップを予想する。<br>
    都会の風景 [cityscapes] ↔ 写真 [photo] の image-to-image 変換タスクの実験評価で使用される。<br>

- Semantic segmentation metrics :<br>
	写真 [photo] ↔ ラベル [labels] のパフォーマンスを評価するために、他論文の Cityscapes benchmark (https://arxiv.org/abs/1604.01685) から、ピクセル単位での正解率、クラス単位での正解率、IOU の平均値を含む標準的な指標を使用する。<br>

各種 image-to-image 変換タスクに対しての実験結果を以下に示す。<br>

- 地図 ↔ 航空写真の変換タスクでの各種手法の比較結果：<br>

    ![image](https://user-images.githubusercontent.com/25688193/59317044-4cd22b00-8cfc-11e9-82c7-2e9e039797e5.png)<br>
    上図は、地図 ↔ 航空写真の変換タスクにおける、各種手法の定性的な比較結果を示している。<br>
    いずれの手法も、右端の正解データ（Ground truth）と完全に一致するような画像は生成出来ていないが、この内、pix2pix と CycleGAN が最も正解データに近い画像を生成出来ていることが見てとれる。<br>

    ![image](https://user-images.githubusercontent.com/25688193/59317145-a175a600-8cfc-11e9-84a8-70d4da5b35b4.png)<br>
    上表は先に述べた AMT perceptual studies での、地図 ↔ 航空写真の変換タスクにおける、各種手法の定量的な比較結果を示している。<br>
	生成画像は、解像度 256×256 での地図画像 → 航空写真、及び、航空写真→地図画像である。<br>
	CycleGAN の手法のみが、およそ 1/4 の確率で被験者を騙せるような偽物画像を生成出来ていることが見てとれる。<br>

- 都会風景画像 ↔ ラベル画像の変換タスクでの各種手法の比較結果：<br>

    ![image](https://user-images.githubusercontent.com/25688193/59316933-c4ec2100-8cfb-11e9-837d-5fa54ba8e673.png)<br>
    上図は、都会風景画像 ↔ ラベル画像の変換タスクにおける、各種手法の定性的な比較結果を示している。<br>
	pix2pix と CycleGAN が、最も正解データ（右端の Ground truth）に近い画像を生成出来ていることが見てとれる。<br>

    ![image](https://user-images.githubusercontent.com/25688193/59320306-d25bd800-8d08-11e9-815a-e53e6ffd1057.png)<br>
	上の２つの表は、それぞれ、ラベル画像 → 都会風景画像、都会風景画像 → ラベル画像の変換タスクに関して、先に述べた FCN score と Semantic segmentation metrics（ピクセル単位での正解率、クラス単位での正解率、IOU） での各種手法の定量的な比較結果を示した表である。<br>
	全ての指標で、CycleGAN が最も高いパフォーマンスを示していることが見てとれる。<br>

- 都会風景画像 ↔ ラベル画像の変換タスクでの ablation study での比較結果：<br>
    
    ![image](https://user-images.githubusercontent.com/25688193/59320387-236bcc00-8d09-11e9-8942-68f35635b460.png)<br>
	上の２つの表は、それぞれ、ラベル画像 → 都会風景画像、都会風景画像 → ラベル画像の変換タスクに関して、CycleGAN の完全な損失関数とそれから一部の損失関数項を抜き取ったモデル（右側の式）での所謂 ablation study での比較結果を示している。<br>
    総合的に見て、全ての項ありの CycleGAN (ours) が最も高いパフォーマンスを発揮しており、それ故に、CycleGAN ではこの完全な損失関数を採用している。<br>

    ![image](https://user-images.githubusercontent.com/25688193/59323114-42239000-8d14-11e9-8fed-d13650b4b60c.png)<br>

	上図は、都会風景画像 ↔ ラベル画像の変換タスクにおける、CycleGAN の損失関数の ablation study での定性的な比較結果を示している。<br>
	GAN alone と GAN + forward、GAN + backward では、入力画像 Input を変えても、同じ画像を生成しており、モード崩壊が発生していることが見てとれる。<br>

	このように単方向の cycle concictncy loss のみの損失関数では、モード崩壊が発生してしまう傾向があるため、CycleGAN では、完全な損失関数をもつ CycleGAN (ours) を採用している。<br>

- CycleGAN の限界：<br>

    ![image](https://user-images.githubusercontent.com/25688193/59324054-42be2580-8d18-11e9-9638-f48ba7f3d975.png)<br>
    上図は、CycleGAN での image-to-image 変換タスクのいくつかの失敗ケースを示している。<br>

	上図左側にある犬 ↔ 猫の変換ように形状の幾何学的な変更が必要な image-to-image 変換タスクにおいては、形状全体の大域的な特徴ではなく、局所的な特徴のみの最小限で変換していまっていることが見てとれる。<br>
	このような変換となってしまう原因としては、以下のような理由が考えられる。<br>

    1. cycle consistency loss の効果が強すぎる：<br>
        cycle consistency loss は、入力画像のサイクリックな写像後の画像が元の入力画像に一致させるといったように、恒等写像の形に近い写像に誘導する効果があるが、犬 ↔ 猫の変換タスクのような全体の形状を大きく変換する変換タスクを実現する写像は、このような恒等写像とは大きく異なる写像となるので、cycle consistency loss が不利に働いてしまう。<br>

    2. 識別器の PathGAN による判定：<br>
        CycleGAN では、識別器の内部構造として PatchGAN を採用しているが、PatchGAN で行われる判定は、各パッチ内の局所的な特徴量のみでの判定であり、犬 ↔ 猫の変換タスクで、局所的な特徴量のみで変換してしまっている。<br>

    また、上図右側にあるの馬 ↔ シマウマの変換もうまく変換出来ていないが、これは、この CycleGAN が、ImageNet にある野生の馬とシマウマ画像のペアで学習を行っており、乗馬している人の画像で学習を行っていないためだと考えられる。<br>


<a id="StarGAN"></a>

## ■ StarGAN
従来の GAN ベースの image-to-image 変換手法（pix2pix や CycleGAN など）は、
１つのデータセット内での１対１の２つのドメイン間での image-to-image 変換を想定しているものであった。<br>
※ ここで用いられる ”ドメイン” という用語は、同じ属性（髪の色、性別、年齢など）をもつ集合を意味していることに注意。<br>

しかしながら、それぞれ別の属性ラベルが付与されている複数のデータセットが利用できれば便利である。例えば、CelebA データセットには、女性・黒髪などの顔の見た目に関する４０種類の属性ラベルが付与されており、RaFD データセットには、喜び・怒りなどの感情に関する８種類の属性ラベルが付与されているが、これら２つのデータセットを組み合わせられれば、より多くの image-to-image 変換（例えば、黒髪 ↔ 喜びの表情、女性 ↔ 怒りの表情など）が出来るようになる。<br>

![image](https://user-images.githubusercontent.com/25688193/59559089-4c5acc80-903b-11e9-9ad5-9cb32fe7cb98.png)<br>

StarGAN は、このような複数データセット内での多対多の複数ドメイン間での image-to-image 変換タスクを、効率的かつ効果的に実現する GAN ベースの image-to-image 変換手法である。<br>

その基本的なコンセプトは、上図右側のアーキテクチャ図のように、<br>
複数のデータセットでの複数のドメイン（図では１〜５。この内１〜３がデータセットA、４〜５が別のデータセットB）間で共通の生成器（G）を使用し、複数のデータセット内での全ての可能なドメイン間の写像を同時に学習するようにするものである。<br>
そしてこの際に、生成器（G）への入力として、入力画像だけでなく変換したいドメインのラベル情報 c も受け取ることで、入力画像 x を所望の目標ドメイン c での画像 y へ変換出来るようにする。<br>

※ 従来の１対１のドメイン間での GAN ベースの image-to-image 変換（pix2pix, CycleGAN など）でも、複数ドメイン間の image-to-image 変換は実現できるが、この場合上図左側のように、存在するドメインペアの全ての組み合わせで、別途モデルを用意する必要があり、それ故に、非効率的で非効果的であり、また拡張性やロバスト性に限界がある。<br>


<a id="StarGANのアーキテクチャ"></a>

### ◎ StarGAN のアーキテクチャ

![image](https://user-images.githubusercontent.com/25688193/59558570-3398e980-9030-11e9-9611-2ce673518154.png)<br>
![image](https://user-images.githubusercontent.com/25688193/59573347-81275c00-90ed-11e9-8f6f-ea2055dfb797.png)<br>

上図は、StarGAN のアーキテクチャ図を示した図である。<br>
この StarGAN のアーキテクチャには、主に、以下のような特徴がある。<br>

- 複数ドメイン間の写像を行う生成器（G）：<br>

    ![image](https://user-images.githubusercontent.com/25688193/59561179-2132a600-9058-11e9-9485-10fd6351775b.png)<br>
    複数のデータセットでの複数のドメイン間の imge-to-image 変換を可能にするために、上図のように複数のデータセットから構成される複数のドメイン（図では１〜５。この内１〜３がデータセットA、４〜５が別のデータセットB）間で共通の生成器（G）を使用し、複数のデータセット内での全ての可能なドメイン間の写像を同時に学習するようにする。
    この際に、生成器（G）への入力として、入力画像だけでなく変換したいドメインのラベル情報 c も受け取ることで、入力画像 x を所望の目標ドメイン c での画像 y へ変換出来るようにする。<br>
    （言い換えれば、この生成器 G が、複数のドメイン c に対して、G(x,c) → y となるように学習を行う。）<br>

    ここで、このドメインラベルは、one-hot ベクトルによって表現される。<br>
    また、全てのドメイン間の写像を公平に学習するために、変換後の画像が所属する目標ドメインラベル c は、ランダムに生成されるようにする。<br>

- 識別器のドメインラベルを分類する補助的な分類器：<br>
    ![image](https://user-images.githubusercontent.com/25688193/59561198-67880500-9058-11e9-8c0c-b0e1fe65fad8.png)<br>
    本物画像　x の所属するドメイン c' と、生成器が生成した偽物画像 G(x,c) の所属するドメイン c が、各々正しいドメイン c',c に分類されることを生成器と識別器が学習するために、上図のように、識別器に従来の入力画像が本物画像か偽物画像なのかを判別するネットワーク D_src に加えて、ドメインラベルを分類する補助的な分類器 [auxiliary classifier] D_cls を追加する。<br>
    そして、後述で定義する domain classification loss とその損失値からの誤差逆伝播で、本物画像 x と偽物画像 G(x,c) の所属するドメインが、各々所望のドメイン c',c に分類されるように生成器と識別器を学習する。<br>
    ※ この補助的な分類器 [auxiliary classifier] は、ACGAN の論文で提案されているもの。<br>

- 損失関数：<br>
    StarGAN の生成器の損失関数と識別器の損失関数は、以下で定義される３つの損失関数（adversarial loss、domain classification loss、cycle consistency loss）の線形結合<br>
    ![image](https://user-images.githubusercontent.com/25688193/59561366-81c2e280-905a-11e9-9da7-20f05ca9c569.png)<br>
    で定義される。<br>

    - adversarial loss：<br>
        ![image](https://user-images.githubusercontent.com/25688193/59559162-942e2380-903c-11e9-8ff8-db7d254390e7.png)<br>
        従来の GAN のように、本物画像と生成器が生成した偽物画像に対しての、識別器の判定結果を元にした損失関数で、真の分布とモデルの分布の２つの分布を一致させるように働く。<br>
        実際の StarGAN では、学習安定化のために、以下のように定義されるWGANの損失関数を使用する。<br>
        ![image](https://user-images.githubusercontent.com/25688193/59573528-5db0e100-90ee-11e9-8794-18ec624fd8d4.png)<br>

    - domain classification loss :<br>
		本物画像 x の所属するドメイン c' と、生成器が生成した偽物画像 G(x,c)  の所属するドメイン c が、それぞれ正しいドメイン c',c に分類されるように生成器と識別器を学習させるための損失関数。<br>

        1. 識別器側の domain classification loss：<br>
            ![image](https://user-images.githubusercontent.com/25688193/59559172-adcf6b00-903c-11e9-8942-99184105d2d6.png)<br>
    		識別器の補助分類器 D_cls は、ドメインラベル c′ に所属する本物画像 x が入力されたとき、本物画像 x の所属ドメインを、（正しく）c′ に判定するように ![image](https://user-images.githubusercontent.com/25688193/59559190-f424ca00-903c-11e9-9981-7898841a44a3.png) を出力しようとする。<br>
    		⇒ ![image](https://user-images.githubusercontent.com/25688193/59559172-adcf6b00-903c-11e9-8942-99184105d2d6.png) の最小化に対応<br>
            <!--
            ※ 識別器としてWGAN を適用するときは、D_cls(c′|x) = 0.0 ~ 1.0 で活性化された値ではなく、そのままの関数値となるが、ここでは簡単にこの損失関数の効果を理解するために活性化された値（0.0 ~ 1.0）で説明している。<br>
            -->

        2. 生成器側の domain classification loss：<br>
            ![image](https://user-images.githubusercontent.com/25688193/59559195-0b63b780-903d-11e9-938b-966609d01602.png)<br>
    		生成器としては、変換後の偽物画像 G(x,c) の所属ドメイン c が、（正しく）c に判定するように、補助分類器に ![image](https://user-images.githubusercontent.com/25688193/59559217-5f6e9c00-903d-11e9-97b8-1de972c2691d.png) を出力させようとする。<br>
    		⇒ ![image](https://user-images.githubusercontent.com/25688193/59559195-0b63b780-903d-11e9-938b-966609d01602.png) の最小化に対応<br>
            ※ 識別器としてWGAN を適用するときは、D_cls(c|G(x,c)) = 0.0 ~ 1.0 で活性化された値ではなく、そのままの関数値となるが、ここでは簡単にこの損失関数の効果を理解するために活性化された値（0.0 ~ 1.0）で説明している。<br>

    - cycle consistency loss :<br>
        ![image](https://user-images.githubusercontent.com/25688193/59559238-d310a900-903d-11e9-8043-c565c8760252.png)<br>
        CycleGAN で提案されている cycle consistency loss。<br>
		入力画像 x を生成器で写像して、別の逆方向の生成器で逆写像したもの G(G(x,c)  | c')  が、元の入力画像 x に一致するという cycle consistent 要件を実現する損失関数。<br>
        画像の ”ピクセル毎” のL1ノルムで定義されている。<br>

- マスクベクトル：<br>
    StarGAN では、複数のデータセットにおける複数のドメインラベルを扱うことを目的としているが、この際に、各データセット内では、複数のデータセット内での全てのドメインラベル情報の内の一部しかわからないケースが発生する。<br>
    ※ これは例えば、CelebA データセットと RaFD データセットの２つのデータセットでの複数ドメイン間での image-to-image 変換タスクを考えた場合に、複数のデータセット内での全てのドメインラベル情報は、顔の外面の｛髪の色、性別、年齢・・・｝ラベルと表情ラベル｛幸福、怒り、・・・｝の両方から構成されるが、CelebA データセットでは、顔の外面ラベル｛髪の色、性別、年齢・・・｝という一部しか存在せず、逆に、RaFD データセットでは、表情ラベル｛幸福、怒り、・・・｝という一部しか存在しないといったケースのことである。<br>

    このようなケースでは、先の cycle consistency loss の式で、画像のピクセル毎のL1ノルムを計算するのに必要な完全なドメインラベル情報 c',c が得られないことになるので、学習が行えず問題である。<br>

	この問題を解決するために、StarGAN では、データセットに該当しないドメインラベルを０、該当するドメインラベルを１に割り当て、０に割り当てられたデータセットのドメインラベルは無視できるようにするためのマスクベクトル m を導入し、以下に定義されるドメインラベルの統一バージョン c ̃ を使用する。<br>
    ![image](https://user-images.githubusercontent.com/25688193/59572864-f04f8100-90ea-11e9-91f6-22609db44ca1.png)<br>

    ※ 例えば、データセット１が４つのドメインを持ち、データセット２が３つのドメインを持ち両者を利用するとき、データセット２でのドメインラベルの統一バージョンは、<br>
    ![image](https://user-images.githubusercontent.com/25688193/59572925-47edec80-90eb-11e9-9435-3c3f6967c73a.png)<br>
    とかける。
    ここで、マスクベクトルの１番目の要素が０で指定されており、データセット１のドメインラベルは無視される意味合いとなる。<br>

- 生成器のネットワーク構成は、CycleGAN ベース：<br>

    ![image](https://user-images.githubusercontent.com/25688193/59574278-98684880-90f1-11e9-95d4-55899ae3b588.png)<br>
    生成器のネットワーク構成は、CycleGAN の生成器のネットワークと同様にして、以下の３つのネットワークを順に連結したものを採用する。<br>
	① ダウンサンプリングを行うための、ストライド幅２の２つの畳み込み層<br>
	② ６つの residual blocks（ResNet のブロック）<br>
    ③ アップサンプリングを行うための、ストライド幅 1/2 の２つの逆畳み込み層<br>

- 識別器として、PatchGAN を採用：<br>

    ![image](https://user-images.githubusercontent.com/25688193/59574324-cb124100-90f1-11e9-9607-229de6426646.png)<br>

	image-to-image のタスクにおいては、画像の高周波成分（＝局所的な特徴量）と低周波成分（＝大域的な特徴量）の両方を捉えることが重要となるが、StarGAN においては、この識別器の PatchGAN の構造により、画像の高周波成分（＝局所的な特徴量）を強く捉えることを可能にしている。<br>

- instance normalization を採用：<br>
    生成器に対しては、instance normalization で正規化処理を行う。<br>
    識別器に対しては、正規化処理を適用しない。<br>


<a id="StarGANの実験結果"></a>

### ◎ StarGAN の実験結果
以下の表は、StarGAN の論文中での実験の実験条件を示している。<br>

![image](https://user-images.githubusercontent.com/25688193/59578529-9909da80-9103-11e9-9311-15c8382040a2.png)<br>

- CelebA データセットでの定性的な比較：<br>

    ![image](https://user-images.githubusercontent.com/25688193/59587836-290a4d00-9121-11e9-83b3-5ca5d1005b30.png)<br>
	上図は、CelebAデータセットに対して、従来のGAN手法（DIAT、IcGAN、CycleGAN）の cross-domain モデルで学習したものと、StarGAN で学習したモデルからの生成画像を比較した図である。<br>
	※ 従来のGAN手法（DIAT、IcGAN、CycleGAN）の cross-domain モデルは、CelebAデータセット内の全ての可能な属性のペアで複数回学習したものである。<br>

    StarGAN での生成画像が、最もクオリティの高い画像を生成出来ていることが見てとれる。<br>

    ![image](https://user-images.githubusercontent.com/25688193/59587976-80a8b880-9121-11e9-964f-9e4fb013d11a.png)<br>
    この理由としては、従来のGAN手法での cross-domain モデルでの学習では、各々のドメイン間のモデル（上図では、例えばG21,G12）を、固定されたドメイン間の変換対（上図では、１↔２）でのみ学習するために、そのドメイン間のモデルが過学習してしまう傾向があるのに対し、<br>
    StarGAN では、ランダムに目標ドメインを定め、モデル（G）が全ての可能なドメイン間（上図では、１〜５）を一様に学習することで、より汎化能力が高いモデルになっているために、結果として上図の比較において最も品質の高い画像が生成出来ているこということが考えられる。<br>

- CelebA データセットでの AMT による定量的な比較：<br>

    ![image](https://user-images.githubusercontent.com/25688193/59589066-11809380-9124-11e9-8a18-10f5fa0e8290.png)<br>

    上表は、従来のGAN手法（DIAT、IcGAN、CycleGAN）でのシングルモデル、及び cross-domain モデルが生成した生成画像と、StarGAN が生成した生成画像に対して、Amazon Mechanical Turk (AMT)　を利用して定量的に評価したものを比較した表である。<br>
    表中のパーセンテージは、比較対象の４種類の手法で生成した画像をシャッフルし、それを被験者に見せて、知覚的なリアリズム、属性での変換品質、人物の元の特徴の保存性の面で、どれが最も正しく変換されているかを選んでもらった統計値を表している。<br>
    被験者数は、シングル変換タスクで 100 人、複数変換タスクで 146 人である。<br>

    シングル属性の変換タスクにおいて、部分的に StarGAN に近い値となっている従来のGAN手法も存在するが、マルチ属性の変換タスクにおいては、全ての変換タスクにおいて、StarGAN の性能が優位に高いことが見てとれる。<br>

- RaFD データセットでの定性的な比較：<br>

    ![image](https://user-images.githubusercontent.com/25688193/59590017-18a8a100-9126-11e9-8069-ce53362544a6.png)<br>

    上図は、RaFD データセットに対して、従来のGAN手法（DIAT、IcGAN、CycleGAN）の cross-domain モデルで学習したものと、StarGAN で学習したモデルからの生成画像を比較した図である。<br>

    StarGAN が最も自然な表情で、人物の元の特徴を保存した画像を生成出来ていることが見てとれる。<br>

    このような結果となる理由としては、学習用データ数の違いによる汎化能力の違いが考えられる。<br>
    具体的には、RaFD 画像は、１ドメインあたり500 枚の画像という比較的少ない数のサンプルから構成されており、そのため、従来のGAN手法（DIAT、IcGAN、CycleGAN）の cross-domain モデルでは、２つのドメイン間を学習するとき、一度に 1000 枚の学習用画像を使用するのみである。<br>
    一方、StarGAN では、その構造により全ての可能なドメイン間でのデータ全体 4000 枚を学習用データとして使用できるので、これは自然にデータオーギュメンテイションが行なわれていることを意味し、結果として、汎化能力の高いモデルとなっているということである。<br>

- RaFD データセットでの定量的な比較：<br>

    ![image](https://user-images.githubusercontent.com/25688193/59595774-6676d680-9131-11e9-8c3b-66411553fdd3.png)<br>

    上表は、RaFD データセットで学習した ResNet-18 を顔表情分類器として利用し、各手法が生成した生成画像をこの分類器で識別した場合の、分類エラー率を示した表である。<br>
    ※ この分類器 ResNet-18 の、学習に用いた RaFD データセットでの正解率は、99.55% である。<br>

    StarGAN が最も低い分類エラーとなっており、最も本物と見分けがつかない画像を生成出来ていることが見てとれる。<br>
    また、モデルのパラメーターの数に関しても、StarGAN が最もパラメーター数の少ないモデルになっており、その観点からは扱いやすいモデルになっていることが見てとれる。<br>

- 複数のデータセット（CelebA+RaFD）での定量的な比較：<br>

    ![image](https://user-images.githubusercontent.com/25688193/59647264-7fc16680-91b5-11e9-8ac0-94fe400105b3.png)<br>

	上図は、StarGAN の複数データセットでの生成画像の効果を把握するために、RaFD のみの単一のデータセットで学習した StarGAN （StarGAN (SNG)）と、CelebAとRaFDの複数のデータセットで学習した StarGAN （StarGAN (JNT)）での生成画像を定性的に比較した図である。<br>
    
	複数のデータセットで学習した StarGAN のほうが、よりクリアで品質の高い画像が生成出来ていることが見てとれる。<br>

- マスクベクトルの効果の定性的な比較：<br>

    ![image](https://user-images.githubusercontent.com/25688193/59648913-a84c5f00-91bb-11e9-87e6-916e5f453e33.png)<br>
    
	上図は、１番目のデータセットを CelebA、２番目のデータセットを RaFD とした場合に、複数のデータセットで学習した StarGAN の生成画像におけるマスクベクトルの効果を把握するために、適切なマスクベクトル [0,1] での生成画像と誤ったマスクベクトル [1,0] での生成画像を定量的に比較した図である。<br>

	誤ったマスクベクトル [1,0] では、狙いとしている表情の変化が行われておらず、狙いとしていない年齢の変化が行われてしまっていることが見てとれる。<br>

	これは、誤ったマスクベクトル [1,0] では、RaFD での表情ラベルが無視され、CelebA での顔の属性ラベルの "young" が採用されたために発生した振る舞いであり、適切なマスクベクトルと誤ったマスクベクトルの両者の比較から、マスクベクトルの役割が適切に動作していることがわかる。


<a id="SAGAN"></a>

## ■ SAGAN [Self-Attention Generative Adversarial Network]
従来の畳み込みでは、１回の畳み込み演算で局所的な受容野からの情報のみが畳み込まれるために、画像全体に渡っての大域的な特徴を取り込むためには、複数の畳み込み層を重ねていく必要がある。<br>
一方で、大域的な特徴量を取り込もうとして複数の畳み込み層をモデルに組み込むと、今度はパラメーター数が増えて学習が安定化しずらくなり、また計算効率も低下するという問題も発生する。<br>
	
SAGAN では、畳み込み時における attention 構造を GAN にも導入する。<br>
これにより下図のように、画像の離れた部分の長距離依存性（＝大域的な特徴量）を少ない計算コストで捉えることを可能になり、従来の（畳み込みベースの）GAN における、上記のような畳み込み時の局所性や計算効率の問題を解決出来る。<br>

![image](https://user-images.githubusercontent.com/25688193/61586929-24bacf00-abba-11e9-95f1-b218ad7a6ddc.png)

> 上図は、SAGAN の attention 構造で取り込まれている部分を視覚化した図である。画像中の色分けされた各点に対して、attention 部分を対応する色の矢印で表現している。attention を構造を導入することに、画像の離れた部分の情報が取り込まれていることが見てとれる。

<a id="SAGANのアーキテクチャ"></a>

### ◎ SAGAN のアーキテクチャ
![image](https://user-images.githubusercontent.com/25688193/61586275-a0ad1b00-abaa-11e9-9a38-4c6da8ae7a5f.png)

上図は、SAGAN の畳み込み時におけるアーキテクチャを示した図である。SAGAN のアーキテクチャには、主に以下のような特徴がある。<br>

- 生成器＆識別器での畳み込み時の attention 構造の追加：<br>
    SAGAN では概要で述べたように、画像の離れた部分の長距離依存性（＝大域的な特徴量）を少ない計算コストで捉えることを可能にする目的として、生成器と識別器での畳み込み時に、以下のような処理で構成される attention 構造を導入している。<br>

    1. まず、畳み込み層での中間層からの特徴マップ x に対して、以下のように定義される attention map を作成する。<br>
        ![image](https://user-images.githubusercontent.com/25688193/61587148-7fa2f500-abbf-11e9-9ee0-5cc91148ba94.png)<br>
        ※ この β_(j,i) は、中間層からの特徴マップ x の j 番目の領域を合成するときに、モデルが i 番目の領域に関与する（＝注意を向ける）度合いを表している。<br>

    1. この attention map を元に、以下の式に従って、attention 付きの出力を求める。<br>
        ![image](https://user-images.githubusercontent.com/25688193/61587155-93e6f200-abbf-11e9-9b12-5cf48325946e.png)<br>
    
    1. この attention 付きの出力に学習可能なスカラー値をかけて元の特徴マップ x に加算したものを、最終的な出力とする。<br>
        ![image](https://user-images.githubusercontent.com/25688193/61587159-a103e100-abbf-11e9-9dd4-d7bc7e6f5b62.png)<br>
        ※ ここで、この学習可能なスカラー値の初期値は０であるので、この attention 構造による効果は、まず通常の畳み込みのように画像の近傍のみに強く作用するが、その後徐々に学習しながらこのスカラー値を変化させることで、画像の離れた部分で非局所的に強く作用していく動作となる。<br>

- スペクトル正則化 [spectral normalization] :<br>
	SAGAN では、学習を安定化させることを目的として、生成器と識別器の両方で、batch norm を spectral normalization に置き換える。<br>
	※ spectral normalization の元論文では、識別器のみに適用することになっていたが、SAGAN では生成器と識別器の両方に適用する。<br>
	※ これは、生成器への spectral normalization の適用は、パラメーターの大きさの増大を防ぎ、異常な勾配で学習されることを防止出来るので学習を安定化させることが出来き、また、１回の生成器の更新に対する識別器の更新回数を少なく出来るので、計算コストも著しく減少させる効果もあったためである。<br>

- two- timescale update rule (TTUR) :<br>
    two- timescale update rule (TTUR) は、生成器と識別器で別の学習率を使用する手法のことである。<br>
	WGAN では、１回の生成器の更新に対して、複数回の識別器の更新（一般的には５回）を行っていたが、SAGAN ではこの TTUR を採用し、生成器と識別器で別の学習率を使用することで、この１回の生成器の更新に対しての複数回の識別器の更新回数をより少なくするようにする。<br>
    これにより、より最適化の遅い識別器の計算効率を上げることができる。<br>

<a id="SAGANの損失関数"></a>

### ◎ SAGAN の損失関数
SAGAN の損失関数は、シンプルに以下のような adversarial loss のみとなる。<br>
但し、学習を安定化させることを目的として行われる、負の対数尤度計算の最小二乗誤差関数への置き換え（LS-GAN）は行わず、代わりに、ヒンジ損失関数項で置き換える。<br>
※ ヒンジ損失関数では、ある範囲外からは勾配が０になるので、生成器と識別器の不健全な競争の結果として、識別器が不本意に強くなりすぎることを防止でき、結果して学習を安定化させる効果があると考えられる。<br>

![image](https://user-images.githubusercontent.com/25688193/61589882-6a43c000-abeb-11e9-9fee-5b9c30237611.png)

<a id="SAGANの実験結果"></a>

### ◎ SAGAN の実験結果

> 記載中...


<a id="ProgressiveGAN（PGGAN）"></a>

## ■ ProgressiveGAN（PGGAN）
GAN において、高解像度の画像生成が困難で有ることの理由としては、以下のような要因が考えられる。<br>

- 高解像度の画像では、（低解像度のときに比べて画像の詳細が分かるので、）正解画像と偽物画像の識別が簡単なタスクとなる。そのため、生成器と識別器の学習が十分に行えない。

- 高解像度の画像を扱うのにより多くのメモリを消費するので、より小さなミニバッチサイズで学習を行う必要がある。その結果として、学習が更に不安定になる。

- 学習に時間がかかる。

ProgressiveGAN では、このような GAN における高解像度の画像生成における問題点を回避することで、高解像度の画像生成を可能にしている。<br>
その基本的なコンセプトは、<br>
「生成器と識別器を、より簡単な低解像度の画像から段階的に学習させて、学習の段階が進むにつれて、高解像の情報をもつ新しい層を追加していく」<br>
というものである。<br>
これにより、上記の問題の回避と、学習の劇的な高速化が可能となる。<br>


<a id="ProgressiveGANのアーキテクチャ"></a>

### ◎ ProgressiveGAN のアーキテクチャ
![image](https://user-images.githubusercontent.com/25688193/58148372-7964cb00-7c99-11e9-8606-43859a669453.png)<br>
![image](https://user-images.githubusercontent.com/25688193/59819731-23e91000-9364-11e9-9dc6-a23ee21f2fc8.png)<br>

上図は、PGGAN のアーキテクチャ全体を段階的な（Progressiveな）ネットワーク構成と学習と共に示した図である。<br>
また上表は、このアーキテクチャにおける各種ネットワークの設定値を示した表である。<br>
この PGGAN のアーキテクチャには、主に、以下のような特徴がある。<br>

- Progressive なネットワークの構成と学習（Progressive growing of GANs）<br>
	PGGAN の基本的なコンセプトは、概要で述べたように、「生成器と識別器を、より簡単な低解像度の画像から段階的に学習させて、学習の段階が進むにつれて、高解像の情報をもつ新しい層を追加していく」 というものである。<br>
	PGGAN ではこれを実現するためのアーキテクチャとして、上図のようなアーキテクチャを採用している。<br>
    このアーキテクチャでの処理は、以下のような段階的な（Progressive な）処理で行われる。<br>

    1. Training Progresses 1<br>
        まず、4×4の低解像度で、従来のGANの構造に従って、学習が安定化するまで十分に生成器と識別器を学習する。<br>
    1. Training Progresses 2<br>
        次に、生成器にアップサンプリングを行う nearest neightbor フィルタリングと２つの逆畳み込み層を追加し、識別器にダウンサンプリングを行う average pooing と２つの畳み込み層を追加したネットワークで、8×8 の解像度で、学習が安定化するまで十分に生成器と識別器を学習する。<br>
    1. Train Progresses 3~9<br>
		同様の処理を、16×16 → 32×32 → 64×64 → ・・・ の解像度と、それに対応するために追加されるアップサンプリング用の nearest neightbor フィルタリングと逆畳み込み層、及び、ダウンサンプリング用の average pooing と畳み込み層で段階的に行っておき、最終的には、1024×1024 の解像度を生成出来るように学習していく。<br>

    ※ この Training Progresses 1 ~ Training Progresses 8 は、例えば、それぞれ１エポック間に渡って学習し、最後の Training Progresses 9 を残りのエポックを学習させるなどの実装が考えられる。<br>
    ※ また後述の滑らかな解像度の変換を、１エポック間に渡って行う場合、「１エポック目（Training Progresses 1）、２エポック目（Training Progresses 1 ~ 2）、３エポック目（Training Progresses 2）、４エポック目（Training Progresses 2 ~ 3）、５エポック目（Training Progresses 3）、・・・、１５エポック目（Training Progresses 8 ~ 9）、１６～最終エポック目（Training Progress 9）」といった実装となる。<br>

	※ 画像の自動生成にタスクにおいては、画像の大域的特徴量と局所的特徴量の両方を捉えることが重要であるが、このProgressive なネットワークの構成での処理は、単に画像を高解像度化するだけでなく、低解像度での処理で画像で大域的特徴量を先に捉え、高解像度での処理で後で局所的特徴量を捉えるという効果も考えられる。<br>

	※ 尚、PGGAN の公式実装では、4 × 4 の解像度スケールでの潜在変数 z の入力時に、PixelNorm で正規化する処理を入れている。<br>
	ここでの PixelNorm は、後述で説明する、学習の安定化を目的とした生成器の各畳み込み層の後の中間層からの出力である特徴ベクトルに対して行う、”各ピクセル毎の”正規化処理ではなくて、
	shape = 512(C)×1(H)×1(W) の潜在変数に対して PixelNorm（※shape=512×1×1 なので実質チャンネル次元を正規化しているのみ）して、超球面上の点に変換することで、潜在変数をランダムに変化させても超球面上の点を動くようにし、滑らかな補完を実現するものである。<br>

- 滑らかな解像度の変換：<br>
	PGGAN では、上で述べたように、画像の解像度をnearest neightbor フィルタリングと average pooing とを用いて、それぞれ２倍 or 1/2 倍の倍々でアップサンプリング or ダウンサンプリングしていくが、例えばある解像度で十分に学習されたネットワークであっても、この倍々処理を急激に行うと、非滑らかな変化を生じさせてしまう。<br>
	そのため、PGGAN では、アップサンプリング or ダウンサンプリングのための新しい畳み込み層をネットワークに追加するときに、画像の RGB 値を線形補間によって、滑らかに変化させるようにする。<br>

    ![image](https://user-images.githubusercontent.com/25688193/59819581-7118b200-9363-11e9-8f5f-eca88f0c03f7.png)<br>

    上図は、この線形補間による滑らかな解像度の変換の様子を、16×16 → 32 × 32 での切り替え時の例で示した図である。<br>

    - (a) : 遷移開始時の様子。特に線形補間は行われない。（α=0）<br>
    - (b) : 遷移途中での様子。0<α<1 を補間係数として、<br>
        生成器側（G）で、特徴ベクトルから変換したRGB情報（toRGB）を、それぞれ ![image](https://user-images.githubusercontent.com/25688193/58154830-81c70100-7cad-11e9-8723-e3cf34e02cc4.png) で混ぜ合わせる。<br>
        また、識別器側（D）で、RGBから再変換された特徴ベクトル（fromRGB）を、それぞれ ![image](https://user-images.githubusercontent.com/25688193/58154903-a622dd80-7cad-11e9-8f24-bad3dc928d96.png) で混ぜ合わせる。<br>
        そして、学習が進むに連れて α = 0 → 1 とすることで、滑らかな線形補間を行う。<br>
    - (c) : 遷移完了時の様子。線形補間も完了している。（α=1）<br>

    ※ この遷移（α=0.0 ~ 1.0）は、例えば、１エポック間の学習を通じて行うようにする。（１エポック開始時点：α=0.0、１エポック完了時点：α=1.0）<br>

<a id="生成画像の多様性向上とモード崩壊防止のための工夫"></a>

### ◎ 生成画像の多様性向上とモード崩壊防止のための工夫
PGGAN では、前述のような高解像度の画像生成のための工夫だけでなく、生成画像の多様性向上とモード崩壊防止のための工夫も提案している。

- minibatch standard deviation（簡略化された minibatch discrimination）での生成データの多様性の向上とモード崩壊の防止：<br>
	GAN における課題の１つに、モード崩壊というものがある。<br>
	minibatch discrimination では、このモード崩壊を防止するために、識別器にミニバッチデータ内のデータの多様性を検出出来る仕組みを導入している。<br>
    （※ minibatch discrimination についての詳細は、後述の [「【補足】 Minbatch discrimation」](https://github.com/Yagami360/My_NoteBook/blob/master/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6_%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92_%E7%94%9F%E6%88%90%E3%83%A2%E3%83%87%E3%83%AB.md#Minibatchdiscrimination) を参照のこと。）<br>

	PGGAN では、この minibatch discrimination の仕組みを大幅に簡略化した仕組みである minibatch standard deviation を導入することで、minibatch discrimination のように、新しいハイパーパラメータを必要とすることなく、生成データの多様性を向上させ、モード崩壊を防止している。<br>
	
    以下の図は、この minibatch standard deviation での全体的な処理の流れを示した図である。<br>

    ![image](https://user-images.githubusercontent.com/25688193/58143545-246b8980-7c86-11e9-878f-dd0d03e276fd.png)<br>
    ![image](https://user-images.githubusercontent.com/25688193/58155038-09ad0b00-7cae-11e9-9d56-87474eb97c16.png)<br>

    上図に示した、minibatch standard deviation における、各処理の詳細は、以下のようになる。<br>

    1. ミニバッチデータ内の各画像データの各ピクセル値（RGB値）に対して、同じ位置に対応しているピクセル値（RGB値）で、標準偏差を計算する。<br>
        計算した標準偏差は、H×W×C 分あるので、これを各ピクセルが、標準偏差値となるような H×W×C のテンソルとする。<br>
        ※ このテンソルは、ある画像とミニバッチ内でのその他の画像間のピクセル単位での標準偏差となっているが、これは見方を変えると、ミニバッチ内でのデータの多様性を示していることになる。従って、この処理により、識別器にデータの多様性を認識できる構造が入っていることになる。<br>

    2. 全ての高さ（H）と幅（W）、チャンネル数（C）で、この各ピクセル（＝標準偏差値）を平均化し、スカラー値を取得する。<br>

    3. 得られるスカラーをコピーし、結合することで、１つのH×Wのテンソル（＝特徴マップ）を取得する。<br>


<a id="学習の安定化のための工夫"></a>

### ◎ 学習の安定化のための工夫
一般的に、GAN においては、学習を安定化させることを目的として、batch normalization の仕組みがネットワークに導入されている。<br>
しかしながら、この PGGAN では、GAN において batch norm による学習安定化効果は見られないとして、別の学習安定化のための工夫を使用している。<br>
※ batch norm が GAN において学習安定化効果があるか否かは議論の余地がある。<br>
※ batch norm は、共変量シフトを取り除く効果があるという議論もあるが、これにも議論の余地がある。<br>

batch norm を用いない PGGAN の学習安定化（と学習速度向上）のための工夫は、以下の２つの仕組みから構成される。<br>
尚、この２つの手法ともに、学習可能なハイパーパラメータを含んでいない。<br>

- Equalized learning rate<br>
    一般的には、ディープラーニングにおいては、学習率が減衰するような最適化アルゴリズム（Adamなど）を用いて、ネットワークの重みを更新する。<br>
    PGGAN では、このような重みの更新ではなく、以下に示すようなもっと単純な方法で、ネットワークの重みを更新する。<br>

    1. 生成器と識別器のネットワークの各層（i）の重み w_i の初期値を、![image](https://user-images.githubusercontent.com/25688193/58157794-12a0db00-7cb4-11e9-92b7-70f6058f6ea4.png) で初期化する。<br>
    1. 初期化した重みを、各層の実行時（＝順伝搬での計算時）に、以下の式で再設定する。<br>
        ![image](https://user-images.githubusercontent.com/25688193/58157836-2ba98c00-7cb4-11e9-9b81-98a1d5aedccb.png)<br>

- Pixelwise feature vector normalization in generator<br>
    生成器と識別器の不適切な競争の結果として、生成器と識別器からの出力信号（＝偽物画像と判別結果）が制御不能になることを防止するために、<br>
    PGGAN では、生成器の各畳み込み層の後の、中間層からの出力（＝特徴ベクトル）に対して、”各ピクセル毎に”、以下のような特徴ベクトルの正規化処理を行う。<br>

    ![image](https://user-images.githubusercontent.com/25688193/58161289-f48aa900-7cba-11e9-8b45-0867b5d1f3ee.png)<br>

	この正規化手法により、（batch norm と同様にして、）学習の際の変化の大きなに応じて、信号を増幅 or 減衰出来るようになるので、結果として学習の安定化が得られる。<br>


<a id="PGGANの実験結果"></a>

### ◎ PGGAN の実験結果

- SWD と MS-SSIM による生成画像の品質評価：<br>
	GAN の生成画像の品質評価を定量的に行うために、SWD [sliced Wasserstein distance] と MS-SSIM [Multi-scale statistical similarity] とによる評価を行う。<br>

    ここで、この SWD [sliced Wasserstein distance] では、結論のみ述べると、本物画像と偽物画像（＝生成画像）の統計的類似性を評価することが出来る。<br>
    望ましい生成器は、生成画像の局所的な画像の構造が、全ての解像度スケールで学習用データである本物画像と類似していると考えられるので、SWD により本物画像と生成画像（＝偽物画像）の類似性を評価することが出来れば、結果として望ましい生成器であるかを判断出来る。<br>

    ![image](https://user-images.githubusercontent.com/25688193/59971502-39567800-95b8-11e9-8dd5-495cbf63f54f.png)<br>

    尚、この SWD [sliced Wasserstein distance] による本物画像と偽物画像の類似度計算は、大まかに以下のように行われる。<br>
    ① Progreesive growing の各解像度スケールに対応したラプラシアンピラミッドのある各解像度レベル（最初は 16 × 16）から、画像の局所的情報をもつ本物画像と偽物画像 descriptors（7×7pixel の近傍）![image](https://user-images.githubusercontent.com/25688193/59971241-268d7480-95b3-11e9-8a5b-0adcaa539c00.png) を取得する。<br>
    ② それぞれの descriptors の平均値と標準偏差を求め これらを正規化する。<br>
    ③ 正規化された descriptors の画像の統計的類似性を SWD を使って計算する。<br>
    ④ これを、Progreesive growing の各解像度スケールに対応した、ラプラシアンピラミッドの各解像度レベル（16 × 16 → 32 × 32 → ・・・ → 128 × 128）で繰り返す。<br>

    一方、MS-SSIM [Multi-scale statistical similarity] では、結論のみ述べると、生成画像の多様性を評価することが出来る。<br>
    
    ![image](https://user-images.githubusercontent.com/25688193/59962722-0e224900-9524-11e9-8f59-ad29f7a47d59.png)<br>

	上表と上図は、解像度 128 × 128 である CelbA データセットと LSUN BEDROOM データセットに対して、SWD と MS-SSIM での測定結果を、従来の GAN 手法と PGGAN の ablation study で、それぞれ定量的（上表）、定性的（上図）に比較したものである。<br>

    比較に使用した、ベースライン手法と PGGAN の ablation study の詳細は以下の通り。<br>
    - (a) : ベースライン手法である WGAN-GP。（生成器で featuring batch normalization、識別器で layer normalization を実施し、ミニバッチサイズ 64）<br>
    - (b) : PGGAN の Progressive growing の仕組みのみを有効化<br>
    - (c) : ミニバッチサイズを 64 → 16 に減らしたもの（※高解像度での処理では、ミニバッチサイズが大きとメモリのサイズが足りなくなることがあるので、ミニバッチサイズが小さいものでも検証）<br>
    - (d) : ハイパーパラメーターを最適にし、batch norm を layer norm に置き換えたもの。<br>
    - (e) : minibatch standard deviation を有効化したもの。<br>
    - (e*) : minibatch discrimination を有効化したもの。<br>
    - (f) : Equalized learning rate を有効化したもの。<br>
    - (g) : Pixelwise feature vector normalization を有効化したもの。<br>
    - (h) : PGGAN における全ての提案手法を有効化したもの。<br>
    
    (a) と (h) では、上図の定性的な品質比較において、明らかに (h) のほうが優れた画像を生成しているが、その MS-SSIM では、両者の数値に大きな違いがなく、MS-SSIM では画像品質をうまく定量化出来ていないことが見てとれる。（※ これは、MS-SSIM が生成画像の多様性を評価する指標であり、本物画像と偽物画像の類似性を評価する指標ではないことによる）<br>
    一方、SWD では、両者の数値に大きな違いがあり、SWD では画像品質をうまく定量化出来ていることが見てとれる。<br>

    (b) では、ベースライン (a) から Progressive growing の仕組みを有効化することで、より鮮明な画像を生成出来ていることが見てとれる。<br>
    (c) では、ミニバッチサイズを減らすことで、生成画像は不自然になり、これは SWD と MS-SSIM の両者の数値にも現れていることが見てとれる。<br>
    minibatch standard deviation を有効化した (e) では、SWD の平均値を改善しているが、従来の minibatch discrimination を有効化した (e*) では、SWD と MS-SSIM ともに値が改善されていないことが見てとれる。<br>
    (f) と (g) では、全体的に、SWD の値が改善していることが見てとれる。<br>
    全ての提案手法を有効化した (h) では、SWD の値も改善し、また生成画像の品質も高くなっていることが見てとれる。<br>

- 学習時間の改善：<br>
    ![image](https://user-images.githubusercontent.com/25688193/59971086-4e2f0d80-95b0-11e9-9785-f2d5e4c3b565.png)<br>

	上図は、Progessive growing の学習時間改善の効果を示すために、横軸を学習時間、縦軸を各ラプラシアンピラミッドの解像度レベルでの SWD の値とし、それぞれ、以下の条件で学習時間を比較した図である。<br>

    (a) : CelebA（128 × 128 の解像度）での Progressive growing なし<br>
    (b) : CelebA（128 × 128 の解像度）での Progressive growing あり<br>
    (c) : 1024 × 1024 の解像度での Progressive growing 有り無し<br>

    尚、点線の垂線は、Progressive growing において、生成画像の解像度が２倍になるフェイズを示している。<br>

    (a), (b) を比較すると、SWD の値が収束するのに、Progressive growing 有りの (b) では、約 1/2 の学習時間で収束出来ていることが見てとれる。また、収束時の SWD の値も Progressive growing 有りの (b) のほうが改善していることも見てとれる。<br>

    更に、高解像度スケール（128 × 128）の青線に注目すると、学習時間短縮の効果が顕著に現れていることが見てとれる。<br>
    これは、Progressive growing 有りのモデルでは、既により小さな解像度スケールでの学習が出来ているので、この学習内容を引き継いですばやく高解像度での学習も行えるためであると考えられる。<br>

    (c) での 1024 × 1024 の解像度での、Progressive growing 有無の学習時間の比較では、フルの解像度（1024 × 1024）に達するまでは、Progressive growing 有りのほうが学習時間が短く、一旦、フルの解像度に達すれば、両者の学習時間は同じになることが見てとれる。<br>

    また、この 1024 × 1024 の解像度での収束するまでの時間は、Progressive growing 有りで約 96 時間、Progressive growing なしで約 520 時間となっており、およそ 5.4 倍の学習時間短縮の効果が得られた。<br>

<!--
- CelebA-HQ での実験結果：<br>
    > 記載中 ...

- LSUN BEDROOM での実験結果：<br>
    > 記載中 ...

- CIFAR-10 での実験結果：<br>
    > 記載中 ...
-->


<a id="PGGANの適用例"></a>

### ◎ PGGAN の適用例

- [PGGAN を利用した手書き文字（MNIST）| CIFAR-10 の自動生成](https://github.com/Yagami360/MachineLearning_Exercises_Python_PyTorch/tree/master/GAN_PGGAN_PyTorch)


<a id="StyleGAN"></a>

## ■ StyleGAN
StyleGAN は、高解像度の画像を生成可能な ProgressiveGAN をベースラインとして、画像の画風変換（Style Transfer）の分野の知見を取り入れたアルゴリズムである。<br>

この StyleGAN のアーキテクチャは、従来の GAN の生成器とは、かなり異なるアーキテクチャになっているが、非常にクオリティの高い画像を、高解像度で生成出来る。<br>
又、単に高解像の画像を生成出来るだけでなく、人物画像の高レベルで大域的な属性（顔の輪郭、眼鏡の有無など）から局所的な属性（しわや肌質など）までを切り分けて、アルゴリズムで制御出来るようになっている。<br>

<a id="StyleGANのアーキテクチャ"></a>

### ◎ StyleGAN のアーキテクチャ
![image](https://user-images.githubusercontent.com/25688193/57349935-00dd1500-7197-11e9-9e03-08d12a33febf.png)<br>

上図は、StyleGAN の全体のアーキテクチャ（上段）と、生成器内部の詳細なアーキテクチャ（下段）を示した図である。<br>
この StyleGAN のアーキテクチャには、主に、以下のような特徴がある。<br>

- Style-Based Generator：<br>
    StyleGAN の生成器（Style-Based Generator）は、ProgressiveGAN の生成器をベースラインとして、画風変換（Style Transfer）の分野の知見を取り入れた、従来の GAN の生成器とは、かなり異なるアーキテクチャになっている。<br>
    具体的には、以下のような構造や特徴をもつ。（詳細は後述）<br>
    - Mapping network f
    - Adaptive Instance Normalization (AdaIN)
    - pixel-wise Noise Input と Stochastic variation
    - ProgressiveGAN から入力層を排除
    - Style Mixing

- Mapping network f（潜在空間 Z から潜在空間 W への写像と潜在表現の獲得）：<br>
    従来の GAN では、入力ノイズ z∈Z（Z:潜在空間）を、そのまま生成器に入力していた。
    しかしながら、この方法では、下図で示すように、画像の特徴量の一部の組み合わせが存在しないようなケースにおいて、入力ノイズ z が存在する潜在空間 Z は、線形ではなく entanglement（もつれ）のある歪んだ空間となる。<br>
    ![image](https://user-images.githubusercontent.com/25688193/57284435-1fd29d00-70ec-11e9-9fdd-a818745d4fa4.png)<br>

	このような entanglement（もつれ、歪み） のある空間では、潜在空間のある部分空間からサンプリングしたベクトル（＝上図では特徴軸上のベクトル）だけを使って画像を生成したときに、その生成画像において、複数の要素が同時に変動してしまう（例えば、性別や肌色などが同時に変わる）。<br>
    そのため、このような entanglement な潜在空間では、狙い通りの画像を生成するのが困難になってしまう問題が存在する。<br>

    ※ 一方で、 disentangle（解きほぐし）されている潜在空間では、潜在空間のある部分空間からサンプリングしたベクトル（＝上図では特徴軸上のベクトル）だけを使って画像を生成したときに、その生成画像において、変動する要素が一つのみである。（例えば、性別だけが変わる、肌色だけが変わる）<br>
    そのため、このような disentangle な潜在空間を獲得出来れば、意図している画像の生成が行いやすくなる。<br>

    このような問題に対応するために、StyleGAN では、この潜在変数である入力ノイズ z をまず、mapping network f という８層のMLPから構成されるネットワークに入力し、中間的な潜在空間 W へと写像して、別の潜在変数 w∈W を獲得した上で、後段のネットワークに入力する。<br>
    ここで、mapping network は、学習されるネットワークであるので、これにより、この mapping network で写像された潜在変数 w は、下図で示すように、画像の特徴量の entanglement（もつれ）の少ない状態で最適化されていることが期待できる。<br>
    ![image](https://user-images.githubusercontent.com/25688193/57284625-8061da00-70ec-11e9-9cd9-780f2ebbeb6e.png)<br>

    尚、StyleGAN では、この mapping network f による潜在空間の entanglement（もつれ）の disentanglement （解きほぐし）の程度を、定量的に評価する指標として、新しい２つの指標（Perceptual path length と Linear separability）を提案している。（詳細は後述）<br>

    > ※ disentanglement と潜在表現の獲得：<br>
    > 直訳すると、（複雑に絡み合った問題の）”解きほぐし” の意味であるが、ここでの意味は、データのもつれを解く最適な表現方法を獲得する（disentanglement）こと。

	尚、この Mapping Networks f の具体的なネットワーク構成は、上図のアーキテクチャ図のように、８つの全結合層（FC）より構成されるネットワークになっている。<br>
    
	※ 公式の実装では、入力潜在変数 z に対して PixelNorm でピクセル単位の正規化処理が行われていることに注意。(https://github.com/NVlabs/stylegan/blob/b061cc4effdcd1da86a0cc6e61e64b575cf35ffa/training/networks_stylegan.py#L416-L418)<br>
    但し、ここでの PixelNorm は、学習の安定化を目的とした生成器の各畳み込み層の後の中間層からの出力である特徴ベクトルに対して行う、”各ピクセル毎の”正規化処理ではなくて、ベースラインとしている PGGAN の 4×4 の解像度スケールでの潜在変数の入力時でも行われている処理の意味での PixelNorm である。<br>
    即ち、shape = 512(C)×1(H)×1(W) の潜在変数に対して、PixelNorm（※shape=512×1×1 なので実質チャンネル次元を正規化しているのみ）して、超球面上の点に変換することで、潜在変数をランダムに変化させても超球面上の点を動くようにし、滑らかな補完を実現しているものと同様の処理になっている。<br>


- 各解像度スケールでの Adaptive Instance Normalization (AdaIN) の採用：<br>
    AdaIN は、Style Transfer の文脈では、Instance Normalization　の発展版で、1つのモデルであらゆるスタイル（画風）への変換を可能にした正規化手法である。<br>
    ※ 詳細は、「[【補足】 Style Transfer における正規化手法（INとAdaIN）](https://github.com/Yagami360/My_NoteBook/blob/master/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6_%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92_%E7%94%9F%E6%88%90%E3%83%A2%E3%83%87%E3%83%AB.md#StyleTransfer%E3%81%AB%E3%81%8A%E3%81%91%E3%82%8B%E6%AD%A3%E8%A6%8F%E5%8C%96%E6%89%8B%E6%B3%95%EF%BC%88IN%E3%81%A8AdaIN%EF%BC%89)」 の項目を参照）<br>

    ![image](https://user-images.githubusercontent.com/25688193/57350788-67affd80-719a-11e9-8c32-60bca8dcc128.png)<br>

    StyleGAN において、この AdaIn は、上図のように、Mapping Network によって得られた中間潜在変数 w に対して、アフィン変換（平行移動変換）を施した後に、このアフィン変換によるスケール値とバイアス値を、それぞれAdaIN の制御パラメーターであるスケール項 ![image](https://user-images.githubusercontent.com/25688193/57304365-28da6300-711a-11e9-8f25-19f6a89fd8ac.png) とバイアス項 ![image](https://user-images.githubusercontent.com/25688193/57304393-37287f00-711a-11e9-94bf-4b78ca958831.png) として用いるという用途で適用される。<br>
    ここで、AdaIN の式は、以下のような式で与えられる。<br>

    ![image](https://user-images.githubusercontent.com/25688193/57304514-722ab280-711a-11e9-8c90-bbbe3b0d973c.png)<br>

	この AdaIN の処理は、特徴マップ単位（＝チャンネル単位）での、正規化処理になっているが、この AdaIN による変換を、各解像度スケール（4×4、8×8、16×16、・・・、1024×1024）の畳み込みの後に行うことで、各解像度スケール（4×4、8×8、16×16、・・・、1024×1024）毎に、”画像全体に渡って大域的に”、スタイル（画風）を変化させることが出来るようになる。<br>

- ピクセル単位の Noise Input と Stochastic variation（確率的変動）：<br>
    人物画像の髪やシワなどは、確率的とみなせる細部の局所的な特徴生成として扱うことが出来る。<br>
	従来の GAN では、このような局所的な特徴生成も（大域的な特徴生成と同様にして）、潜在変数としての入力ノイズを直接生成器に入力することで実現する構造になっている。しかしこの方法では、上記の潜在空間の歪みにより、ノイズの影響を制御することが困難である。<br>

    ![image](https://user-images.githubusercontent.com/25688193/57350819-8615f900-719a-11e9-93c7-2584dc4b592e.png)<br>

    これに対して、StyleGAN では、上図のアーキテクチャ図のように、Synthesis Network g での各畳み込みの直後に、ノイズマップで、ピクセル単位のノイズを直接加えることでこれを実現する。<br>
    このノイズはピクセル単位で適用されるので、髪やシワなどの確率的な変動を、ピクセル単位で局所的に直接制御することが可能になる。<br>
    （※ これに対して、AdaIN は、特徴マップ単位で適用されるので、画風（スタイル）といった画像全体に渡っての大域的な属性を制御している。）<br>

    - ノイズの影響と効果：<br>
        ![image](https://user-images.githubusercontent.com/25688193/57357077-13624900-71ad-11e9-9d4e-dbd2939cfdeb.png)<br>
        上図は、この入力ノイズによる生成画像の効果を示した図である。<br>
        (a) ベースとなる画像<br>
        (b) ベースとなる画像はそのままで、それぞれノイズを変えたとき４つの画像<br>
        (c) ノイズの影響を受けている箇所をグレースケースで表示した画像（白部分が影響大、黒部分が影響小）<br>

        ノイズは、髪の毛という確率的とみなせる局所的な特徴のみを変化させ、顔の向きや輪郭などの、画像全体に渡っての大域的な特徴には影響を与えていないことが見てとれる。<br>
        <br>
        ![image](https://user-images.githubusercontent.com/25688193/57357222-6805c400-71ad-11e9-94eb-7bf64f50420a.png)<br>
        上図は、どの解像度スケールの層にノイズを加えるかでの生成画像の効果を示した図である。<br>
        (a) 全ての解像度スケールの層に、ノイズを加えた場合<br>
        (b) ノイズを加えない場合<br>
        (c) 高解像度の層（64×64 ~ 1024×1024）のみに、ノイズを加えた場合<br>
        (d) 低解像度の層（4×4 ~ 32×32）のみに、ノイズを加えた場合<br>

        ノイズを加えないと、全体的に均一で立体感のない画像が生成されている。<br>
        ノイズを加えると、肌質などの局所的で細かなテクスチャーが加味され、よりリアルな画像が生成されていることが見てとれる。<br>
        そして、ノイズを加える解像度スケールの層に応じて、これら局所的で細かなテクスチャーの、局所性や細かさの程度にも差が出ていることが見てとれる。<br>

- ProgressiveGAN から入力層を除外し、代わりに学習済み定数マップを入力：<br>
    StyleGAN のベースアルゴリズムである  ProgressiveGAN では、乱数入力で生成器への初期入力（4×4）を入力している。<br>

    ![image](https://user-images.githubusercontent.com/25688193/57351073-8793f100-719b-11e9-8eaa-93ceadb81aea.png)<br>

    一方、StyleGAN の生成器では、上図のアーキテクチャ図の Synthesis Network g の開始部分で示しているように、入力層を完全に排除し、そのかわりに、学習済み定数マップ（4×4×512）を入力の開始としている。<br>
    これは、StyleGAN が、生成画像を、前述の潜在変数 w と AdaIN、及びノイズマップによって制御しているために、可変な入力が不要であることによるものである。<br>

- Style Mixing：<br>
    ![image](https://user-images.githubusercontent.com/25688193/57352094-3d147380-719f-11e9-827c-53b9ed64c873.png)<br>

    Style Mixing は、上図のように潜在空間よりサンプリングした２つの潜在変数 ![image](https://user-images.githubusercontent.com/25688193/57352230-b1e7ad80-719f-11e9-9d7d-ca5763974256.png)、及び２つの中間潜在空間 ![image](https://user-images.githubusercontent.com/25688193/57352252-c4fa7d80-719f-11e9-9bab-768bace02642.png) に関して、Synthesis Network g に AdaIN のパラメーターとして入力する際に、ある解像度スケールまでは、z1,w1 を入力し、それ以降の解像度スケールには、z2, w2 を入力するように切り替えるという正規化手法である。<br>
    これにより、Synthesis Network g は、隣接した解像度スケールの層間で画風（スタイル）に相関があるように学習しなくなるので、画風（スタイル）の影響を各解像度スケールの層に局在化させることが可能となる。<br>

    - Style Mixing の効果<br>
        ![image](https://user-images.githubusercontent.com/25688193/57359949-efeecc80-71b3-11e9-963e-73812e4c6709.png)<br>

        上図は、１つの潜在変数 w1 から生成される画像（ソースA）と、別の潜在変数 w2 から生成される画像（ソースB）を、それぞれ縦軸横軸に置き、Style Mixing で潜在変数を w1 → w2 に切り替えたときの画像を示した図である。<br>
        上段の画像は、低解像度のスケール層（4×4 ~ 8×8）で、潜在変数を w1 → w2 に切り替えたときの画像。<br>
        中段の画像は、中解像度のスケール層（16×16 ~ 32×32）で、潜在変数を w1 → w2 に切り替えたときの画像。<br>
        上段の画像は、高解像度のスケール層（64×64 ~ 1024×1024）で、潜在変数を w1 → w2 に切り替えたときの画像。<br>

        低解像度のスケール層で StyleMixing した場合は、ソースBから、顔の向きや輪郭、メガネの有無などの、画像全体に渡っての大域的な特徴が引き継がれていることが見てとれる。<br>
        又、中解像度のスケール層で StyleMixing した場合は、ソースBから、顔の向きや輪郭、髪型などの、画像全体に渡っての大域的な特徴が引き継がれており、ソースAからは、肌質などの局所的な特徴が引き継がれていることが見てとれる。<br>
        又、高解像度のスケール層で StyleMixing した場合は、ソースBから、背景色や髪の色などの、ピクセル単位での局所的な特徴が引き継がれていることが見てとれる。<br>


<a id="潜在空間におけるentanglement（もつれ）のdisentanglement（解きほぐし）の評価"></a>

### ◎ 潜在空間における entanglement（もつれ）の disentanglement （解きほぐし）の評価
先の MappinNetwork での説明で見たように、潜在空間における entanglement と disentanglement  の程度は、意図した画像を生成するための重要な要素である。<br>
従って、これを定量的に評価出来たら有益なのであるが、StyleGAN では、これを定量化する指標として、Perceptual path length と  Linear separabilityの２つの指標を提案している。<br>

※ 尚、これに関連して、特徴が分解されていること（＝disentanglement  な潜在空間）を定量化する指標も、既に考案されているが、この指標を利用するためには、入力画像を潜在変数に写像するエンコーダーが存在していなくてはならない。そのために、StyleGAN のアーキテクチャを変更するのは根本的な解決ではないため、StyleGAN ではこの手法は採用していない。

- Perceptual path length：<br>
    entanglement（もつれ、歪み） のある空間では、各特徴が絡み合ってうまく分解できていないために、下図のように、潜在空間 Z のある端点（１）から別の端点（９）までの線形補間を実施した場合に、途中の潜在空間での値 z で生される生成画像に、非線形な変化（４，５，６）が生じる。<br>
    ![image](https://user-images.githubusercontent.com/25688193/57284523-51e3ff00-70ec-11e9-813d-e2d17b43e28a.png)<br>

	また直感的にも、disentanglement  な潜在空間では、滑らかな変化となり、entanglement な潜在空間では、急激な変化となると考えられる。<br>

    そのため StyleGAN では、このような線形補間において、どれだけ急激に画像が変化するのかを計測することにより、Perceptual path length と呼ばれる潜在空間の entanglement（もつれ、歪み）or disentanglement （解きほぐし）の定量的指標として利用する。<br>

    即ち、潜在空間 Z での全ての両端（上図では１，９）での Perceptual path length の平均 l_z は、以下のような式で定義される。<br>
    ![image](https://user-images.githubusercontent.com/25688193/57367690-6bf11080-71c4-11e9-92f0-e193377c84b5.png)<br>

    又、中間潜在空間 W での Perceptual path length の平均 l_w  は、以下のような式で定義される。<br>
    ![image](https://user-images.githubusercontent.com/25688193/57367739-8a570c00-71c4-11e9-8c43-231d91e31564.png)<br>

    尚、この距離関数 d は、２つのVGG16 の埋め込み（embedding）から得られる特徴間の距離（perceptually-based pairwise image distance）から計算され、これが、画像間のピクセル単位の距離を測っているので、直感的に一致するものとなっている。<br>

- Linear separability：<br>
    先に述べたように、disentangle（解きほぐし）されている潜在空間では、潜在空間のある部分空間からサンプリングしたベクトル（＝上図では特徴軸上のベクトル）だけを使って画像を生成したときに、その生成画像において、変動する要素が一つのみである。（例えば、性別だけが変わる、肌色だけが変わる）<br>
    これは見方を変えると、潜在空間内の各点（＝ある１つの特徴を生成する潜在変数）が、線形超平面で分離可能であることを意味している。<br>

    そのため StyleGAN では、潜在空間の内の点がどれだけうまく線形分離可能であるかを測定することで、Linear separability と呼ばれる潜在空間の entanglement（歪み）or disentanglement （解きほぐし）の定量的指標として利用する。<br>

    より詳細には、以下のような手順で評価する。<br>
    1. 生成された画像にラベル付けするために、２値特徴（例えば、男性の顔と女性の顔）の数に応じて、StyleGAN とは別の補助ネットワーク（分類器）を用意する。<br>

    1. この補助ネットワークを、（人物画像が分類できるように）CelebA-Hデータセットで学習する。<br>

    1. １つの特徴の分類性能を測定するために、StyleGAN から20万枚の画像を生成し、それを先の学習した補助ネットワークを使って分類する。<br>
        そして、分類器のスコアの高かった半分の10万枚のみを、ラベル付き潜在変数として採用する。<br>
        ※ この際のラベリングは、この分類器での分類結果（例えば、男性の顔に分類と女性の顔に分類）を利用してラベリングされる。<br>

    1. これらのラベリングされた潜在変数が、その潜在空間上で、ラベル毎に線形分離可能であるかを、線形SVMで実際に分類してみて評価する。<br>

    1. 分離のしやすさの定量化のために、各特徴に対して、以下の条件付きエントロピーの exp を計算する。<br>

        ![image](https://user-images.githubusercontent.com/25688193/57427662-e1f68580-725f-11e9-8317-e5fc68d13475.png)<br>

        この条件付きエントロピー H(Y│X) は、１つのサンプルの真のクラスを決定するために、どのくらい多くの追加情報が必要であるかを示しており、この値が小さいほど、分類が容易であることを示している。<br>

- Perceptual path length と Linear separability での性能比較結果：<br>

    ![image](https://user-images.githubusercontent.com/25688193/57427769-68ab6280-7260-11e9-975c-000572e39523.png)<br>
    
    上表は、StyleGAN の論文中で提案されている新たなデータセット FFHQ（Flickr-Faces HQ）において、Perceptual path length と Linear separability を計測した表である。<br>
    StyleGAN のベースラインである従来の ProgressiveGAN の生成器（B）と比較して、StyleGAN の生成器（D,E,F）では、Perceptual path length と Linear separability の値が共に、大幅に小さくなっており、潜在空間 Z よりも、中間潜在空間 W のほうが、disentanglement（解きほぐし） の程度が改善していることが見てとれる。<br>
    又、この改善の程度は、StyleGAN にノイズ有りで StyleMixing 無しとしたアーキテクチャが最も改善され、StyleMixing を導入すると、潜在空間の entanglement  の程度を多少強めてしまうことも見てとれる。<br>


<a id="GANimation"></a>

## ■ GANimation
GANimaton は、心理学で用いられている顔の表情表現である Facial Action Coding System (FACS) の知見と、条件付き GAN（cGAN）を拡張したものを組み合わせることで、ある人物顔画像を、目的の表情をした顔画像に自動変換することを実現した image-to-image 手法である。

- Facial Action Coding System (FACS)：<br>
    ![image](https://user-images.githubusercontent.com/25688193/58870110-81415800-86fa-11e9-8b5e-3559de0abb38.png)<br>
    顔の特定の表情筋に由来する Action Units (AUs) と呼ばれるものの動かし方の組み合わせから、人間の顔の表情を表現する仕組み。<br>
    例えば、恐れ [fear] の表情は、<br>
    「AU1 : Inner Brow Raiser（内部の眉の上がり）、AU2 : Outer Brow Raiser（外部の眉の上がり）, AU4 : Brow Lowerer（眉の下がり）、AU5 : Upper Lid Raiser（まぶたの上がり）、AU7 : Lid Tightener（まぶたの締める）、AU20 : Lip Stretcher、AU26 : Jaw Drop（口をあんぐり開ける）」 という AUs の組み合わせで表現する。<br>
    そして、各 AU の信号の大きさにより、感情の程度の大きさを表現する。<br>
    このような FACS により、比較的少ない数（＝30個程度）の特定の表情筋に由来する AUs から、それらの動かし方の組み合わせで、7000 種類以上の表情を表現できるようになる。<br>

GANimation の利点としては、以下のようなものが挙げられる。<br>

- 学習用データに、目標画像や同一人物の異なる表情の画像ペアの必要としない：<br>
    従来の手法では、学習用データに同一人物の異なる表情の画像ペアを必要としていたが、GANimation では、このようなデータは必要とせず、３つのサンプルの組｛入力画像、AUs 状態ベクトル、AUs状態ベクトルの目標ベクトル｝があれば十分である。<br>

- １つの入力画像から、豊富な種類の表情を生成出来る：<br>
    １つの同じ入力画像から、少量の AUs状態ベクトル / AUs パラメーターを通じて、豊富な種類の表情を生成出来る。<br>

- 背景や照明条件に対するロバスト性：<br>
    GANimation では、生成器が attention mask を別途生成することで、顔の表情に関連する特定の部位のみの画像のみを合成することを実現している。<br>
    これにより生成画像において、背景や照明条件が変化しても、最もらしい表情画像が生成でき、この点において SOTA を超えている。<br>


<a id="GANimationのアーキテクチャ"></a>

### ◎ GANimation のアーキテクチャ
まず以下のように、表情変換問題を定式化、モデル化する。<br>

- ![image](https://user-images.githubusercontent.com/25688193/58792488-40c8d800-862f-11e9-8b1a-ad8b6b2fd2c0.png)：任意の表情をもつ入力画像

- ![image](https://user-images.githubusercontent.com/25688193/58792554-5938f280-862f-11e9-93c2-4e4900f0a019.png)：AUs 状態ベクトル
	前述の Facial Action Coding System (FACS) における、Action Units (AUs) の状態を記述するためのベクトルで、このベクトルの各要素 y_n が、n 番目の AUs の大きさを 0~1 の正規化された値で示している。<br>

- ![image](https://user-images.githubusercontent.com/25688193/58792659-97361680-862f-11e9-9a20-3dfd281e8c22.png)：AUs 状態ベクトルの目標ベクトル（下図では笑顔の表情に対応する状態ベクトル）
    生成器に生成させたい表情に対応する状態ベクトル。このベクトルは、ランダムに生成されるものとする。

- ![image](https://user-images.githubusercontent.com/25688193/58792727-b59c1200-862f-11e9-9bee-1c37450fa067.png)：生成器からの出力画像。
    AUs状態ベクトル ![image](https://user-images.githubusercontent.com/25688193/58792837-e4b28380-862f-11e9-9857-2984b495ae48.png)に対応する表情をもつ生成画像。言い換えると、AUs状態ベクトル ![image](https://user-images.githubusercontent.com/25688193/58792837-e4b28380-862f-11e9-9857-2984b495ae48.png)で条件付けされた画像となる。

GANimation の目的は、
入力画像 ![image](https://user-images.githubusercontent.com/25688193/58793215-a4073a00-8630-11e9-9003-654296a737dd.png) を、特定の表情に対応したAUs 状態ベクトルの目標ベクトル ![image](https://user-images.githubusercontent.com/25688193/58792837-e4b28380-862f-11e9-9857-2984b495ae48.png)で条件付けされた生成器からの出力画像 ![image](https://user-images.githubusercontent.com/25688193/58793246-b4b7b000-8630-11e9-8e6c-59f63bb834b4.png) に変換するための写像 ![image](https://user-images.githubusercontent.com/25688193/58793284-c1d49f00-8630-11e9-9d05-e3d70bde765d.png) を学習することである。<br>

この際に必要となる学習用サンプルは、３つのサンプルの組 ![image](https://user-images.githubusercontent.com/25688193/58793581-66ef7780-8631-11e9-91dc-34ac57ac19fb.png) となる。<br>
※ 従来の方法とは異なり、目標画像 ![image](https://user-images.githubusercontent.com/25688193/58793246-b4b7b000-8630-11e9-8e6c-59f63bb834b4.png) や、同一人物の異なる表情の画像ペアを必要としていないことに注目。<br>
※ 実際の学習は、EmotioNet データセットから、この３つのサンプルの組を取り出して学習を行う。<br>

<br>

このような写像![image](https://user-images.githubusercontent.com/25688193/58927035-19d1e980-8788-11e9-88d3-2f144dd348fd.png)を実現するために、GANimation では下図のようなアーキテクチャを構成している。<br>

![image](https://user-images.githubusercontent.com/25688193/58847304-70bebc80-86bd-11e9-8cda-08f84dc63d6b.png)<br>

- 生成器のアーキテクチャ：<br>
    - 条件付きGAN（cGAN）でモデル化：<br>
        生成器 G は、生成器に生成させたい表情に対応する AUs 状態ベクトル ![image](https://user-images.githubusercontent.com/25688193/58792837-e4b28380-862f-11e9-9857-2984b495ae48.png)を条件とする cGAN でモデル化され、![image](https://user-images.githubusercontent.com/25688193/58794803-01e95100-8634-11e9-8a5e-26c332c49c7b.png) で表現される。

    - 双方向 [bidirectionally] の構造を持つ：<br>
        入力画像 ![image](https://user-images.githubusercontent.com/25688193/58793215-a4073a00-8630-11e9-9003-654296a737dd.png) を、目標の表情 ![image](https://user-images.githubusercontent.com/25688193/58792837-e4b28380-862f-11e9-9857-2984b495ae48.png)をした生成画像 ![image](https://user-images.githubusercontent.com/25688193/58793246-b4b7b000-8630-11e9-8e6c-59f63bb834b4.png) に変換するための写像 ![image](https://user-images.githubusercontent.com/25688193/58795252-2560cb80-8635-11e9-8936-ba0d6d741721.png) を行う生成器と、<br>
        生成画像 ![image](https://user-images.githubusercontent.com/25688193/58793246-b4b7b000-8630-11e9-8e6c-59f63bb834b4.png) を、元の入力画像 ![image](https://user-images.githubusercontent.com/25688193/58794917-5987bc80-8634-11e9-92e9-1a9502e55fe6.png) に戻すための写像 ![image](https://user-images.githubusercontent.com/25688193/58795302-49bca800-8635-11e9-8191-33740b64c897.png) を行う生成器の２つが存在する。<br>
        ※ 更に、それぞれの過程で、後述の attention mask と color mask 用の生成器の２つが存在するので、合計４つの生成器が存在することになる。<br>
        ※ 元に戻す変換を行う生成器が必要である理由は、画像の自分が同一人物であることを保証するためである。（詳細は、後述の損失関数の項に含まれる identity loss を参照）<br>

    - attention mask と color mask：<br>
        表情の画像変換を行う際に、表情変換に影響するようなある特定の画像領域のみに変換対象を限定する目的で、上図のように生成器が、attention mask と color mask を生成し、それらを合成することで、最終的なレンダリング画像を生成するようにする。<br>
        ここで、attention mask は、元の画像の各ピクセルが最終的にレンダリングされる画像に、どの程度寄与しているのかを表しており、これにより、表情変換に影響するようなある特定の画像領域のみに、変換対象を限定するすることが出来る。<br>
        一方、color mask は、画像全体に渡っての変換画像の色情報を保持する。<br>

        そして、attention mask と color mask の合成は、以下のような線形補間の式に従って行われる。

        - 入力画像 ![image](https://user-images.githubusercontent.com/25688193/58793215-a4073a00-8630-11e9-9003-654296a737dd.png) を、目標の表情 ![image](https://user-images.githubusercontent.com/25688193/58792837-e4b28380-862f-11e9-9857-2984b495ae48.png)をした生成画像 ![image](https://user-images.githubusercontent.com/25688193/58793246-b4b7b000-8630-11e9-8e6c-59f63bb834b4.png) に変換するための写像 ![image](https://user-images.githubusercontent.com/25688193/58795252-2560cb80-8635-11e9-8936-ba0d6d741721.png) を行う生成器：<br>
            ![image](https://user-images.githubusercontent.com/25688193/58796579-9eaded80-8638-11e9-80b1-9ac8ee2bda97.png)<br>
            ![image](https://user-images.githubusercontent.com/25688193/58796542-8938c380-8638-11e9-9e4e-d704b4d2eb4e.png)<br>

        - 生成画像 ![image](https://user-images.githubusercontent.com/25688193/58793246-b4b7b000-8630-11e9-8e6c-59f63bb834b4.png) を、元の入力画像 ![image](https://user-images.githubusercontent.com/25688193/58794917-5987bc80-8634-11e9-92e9-1a9502e55fe6.png) に戻すための写像 ![image](https://user-images.githubusercontent.com/25688193/58795302-49bca800-8635-11e9-8191-33740b64c897.png) を行う生成器：<br>
            ![image](https://user-images.githubusercontent.com/25688193/58796620-cc933200-8638-11e9-91b2-6441c19efe26.png)<br>
            ![image](https://user-images.githubusercontent.com/25688193/58796680-f3e9ff00-8638-11e9-84cc-348c6fdc837d.png)<br>

        このような attention 構造により、従来の手法とは異なり、画像の内容の背景や照明条件が変化しても、うまく表情の変換が出来るようになる。<br>

    - instance norm で学習を安定化：<br>
        batch norm を使用するよりも、instance norm のほうが学習が安定化したので、instance norm を使用する。<br>

- 識別器のアーキテクチャ：<br>
    ![image](https://user-images.githubusercontent.com/25688193/58847047-77006900-86bc-11e9-8d6a-a0d03345f40a.png)<br>

    - 偽物画像と本物画像を直接的に比較する構造が存在しない：<br>
        一般的な識別器とは異なり、生成器が生成する偽物画像と学習データの本物画像を直接的に比較する構造は存在せず、生成器が生成する従来の偽物画像を入力するのみである。<br>
        それ故に、従来の手法とは異なり、本物画像（＝目標画像）![image](https://user-images.githubusercontent.com/25688193/58793215-a4073a00-8630-11e9-9003-654296a737dd.png) や同一人物の異なる表情の画像ペアを必要としない。<br>
    - AUs 状態ベクトルを推定するための識別器![image](https://user-images.githubusercontent.com/25688193/58796909-7b377280-8639-11e9-966b-ea08eed82d48.png)を並列的に追加：<br>
        上記のように学習データの本物画像を必要としない代わりに、生成器からの生成画像（＝偽物画像）の表情に対応したAUs 状態ベクトル ![image](https://user-images.githubusercontent.com/25688193/58792837-e4b28380-862f-11e9-9857-2984b495ae48.png)を推定（推定値：![image](https://user-images.githubusercontent.com/25688193/58796858-5fcc6780-8639-11e9-834f-58e95b09c796.png)）するための識別器![image](https://user-images.githubusercontent.com/25688193/58796909-7b377280-8639-11e9-966b-ea08eed82d48.png)を並列的に追加する。<br>

    - 識別器として、WGAN-GP ベースの識別器（クリティック）D を使用：<br>
        そのため、識別器（クリティック）の出力は、リプシッツ連続な関数となる。<br>
    - 識別器として、PatchGAN の構造を採用：<br>
        image-to-image のタスクにおいては、画像の高周波成分（＝局所的な特徴量）と低周波成分（＝大域的な特徴量）の両方を捉えることが重要となるが、GANimation においては、この識別器の PatchGAN の構造により、画像の高周波成分（＝局所的な特徴量）を強く捉えることを可能にしている。<br>
        ※ 一方、画像の低周波成分（＝大域的な特徴量）に関しては、後述の損失関数の１つの項である identity loss における、ピクセル単位でのL1ノルムで強く捉えることを可能にしている。<br>

    - 入力画像の feature normalization（特徴量の正規化処理）を除外：<br>
        後述の損失関数における、識別器の WGAN-GP での勾配ペナルティー項 ![image](https://user-images.githubusercontent.com/25688193/58857723-dff9d800-86e0-11e9-9b50-c47f94edce36.png) を計算するときに、識別器の勾配のL2ノルム ![image](https://user-images.githubusercontent.com/25688193/58857787-ff910080-86e0-11e9-8e06-ef92c1cc5a8c.png) が、各入力画像 I^~ に独立してではなく、ミニバッチデータ全体に関して計算されることを回避するために、入力画像の feature normalization（特徴量の正規化処理）を除外する。<br>

<a id="GANimationの損失関数"></a>

### ◎ GANimation の損失関数
GANimation の損失関数は、以下で定義する４つの損失関数（image adversarial loss、attention loss、conditional expression loss、identity loss）の線形結合によって表現される。<br>

![image](https://user-images.githubusercontent.com/25688193/58862277-4d126b00-86eb-11e9-9c36-21c9efa12cf8.png)
※ attention loss は、入力画像から生成画像へのプロセスと、生成画像を元の入力画像に戻すプロセスの両方プロセスで存在することに注意。<br>

1. image adversarial loss：<br>
    ![image](https://user-images.githubusercontent.com/25688193/58858775-4bdd4000-86e3-11e9-9ebc-e0cfb19fbe8e.png)<br>
    ![image](https://user-images.githubusercontent.com/25688193/58852666-65c25700-86d2-11e9-8660-283f0841c822.png)<br>
    識別器が採用している WGAN-GP で用いられる損失関数。<br>
    Earth-Mover 距離の双対表現の式から導出され、識別器の出力がリプシッツ連続性を満たすために、勾配ペナルティーの項が課せられている。<br>
    この損失関数と誤差逆伝播法により、生成画像の分布が、学習データの画像の分布に一致するに動作する。<br>

2. attention loss：<br>
    ![image](https://user-images.githubusercontent.com/25688193/58859076-f7869000-86e3-11e9-9df9-11a20d7b60e6.png)<br>
    ![image](https://user-images.githubusercontent.com/25688193/58858827-74fdd080-86e3-11e9-89eb-283740c4fe98.png)<br>
    ※ 上式と上図は、入力画像から生成画像へのプロセスで定義される attention loss。<br>
    ※ 生成画像を元の入力画像に戻すプロセスでも同様の関係で、attention loss が定義される。<br>

    attention mask の各ピクセル値（＝マスク値）は、0.0 ~ 1.0 のグレースケール値となるが、この attention mask を単純に学習させた場合、学習の過程で各ピクセルのマスク値が簡単に 1.0 の値に飽和してしまう。<br>
    この飽和値 1.0 では、入力画像の全てのピクセルを完全に attention することになるので、![image](https://user-images.githubusercontent.com/25688193/58852944-732c1100-86d3-11e9-9c82-2de294eb12df.png) の関係、即ち、入力画像と生成画像が同じであるような、意味のない結果となってしまう。<br>
    そのため、GANimation では、損失関数に L2重みペナルティーの項を課した attention loss を導入することにより、各ピクセルのマスク値が飽和しない（＝attention がかかり過ぎない）ようにする。<br>

    更に、attention mask と color mask を合成するときに、画像全体に渡っての色の変化がスムーズになるように、画像全体に渡っての全変動正規化項を attention loss に導入することで、attention mask の各隣接ピクセルのマスク値の値が近くなるようにする。<br>

3. conditional expression loss：<br>
    ![image](https://user-images.githubusercontent.com/25688193/58859649-3c5ef680-86e5-11e9-9dff-1d549b17eb1d.png)<br>
    ![image](https://user-images.githubusercontent.com/25688193/58859701-631d2d00-86e5-11e9-8dbd-7d803bf462ac.png)<br>

    → y ̂_g  の部分は、y_r  が正しい？この場合、アーキテクチャ図の識別器のどの部分に y_r  が入力されるのか？（アーキテクチャ図では、y_r  は生成器にのみ入力されているように思える）<br>
	→ y_r  ではなく y ̂_g  とした場合に、D_y (y ̂_g )  は、識別器に並列的に追加されている AUs 状態ベクトルを推定するため構造を、再度識別器に戻していることになるのか？<br>
	→ 論文中での記載では不明のため、公式の実装コードで要検証。<br>

    特定の表情に対応したAUs 状態ベクトルの目標ベクトル ![image](https://user-images.githubusercontent.com/25688193/58792837-e4b28380-862f-11e9-9857-2984b495ae48.png) で条件付けされて生成された、生成器からの出力画像 ![image](https://user-images.githubusercontent.com/25688193/58793246-b4b7b000-8630-11e9-8e6c-59f63bb834b4.png) が、対応する AUs 状態ベクトル ![image](https://user-images.githubusercontent.com/25688193/58796858-5fcc6780-8639-11e9-834f-58e95b09c796.png) に正しく回帰（![image](https://user-images.githubusercontent.com/25688193/58854140-65788a80-86d7-11e9-8066-140a611b79c9.png)）するようにするための損失関数。<br>
    この損失関数により、生成器は、目標の表情に対応した AUs 状態ベクトルをもつ画像を生成出来るようになる。<br>
    ※ この損失関数がないと、単にリアルな顔画像を生成するのみになってしまう。

    この AUs 状態ベクトルの回帰のための損失関数（＝AUs回帰損失）には、「生成器が生成した偽物画像がもつ AUs 状態ベクトルが、入力した AUs 状態ベクトル![image](https://user-images.githubusercontent.com/25688193/58792837-e4b28380-862f-11e9-9857-2984b495ae48.png)に正しく回帰出来ているかの項」 と「本物画像がもつ AUs 状態ベクトルが、正しく回帰出来ているかの項」という２つの項が含まれる。

4. identity loss：<br>
    ![image](https://user-images.githubusercontent.com/25688193/58861161-b80e7280-86e8-11e9-9fce-8a97a7214ff1.png)<br>

    ![image](https://user-images.githubusercontent.com/25688193/58861286-015ec200-86e9-11e9-9c39-4ea0427ad86a.png)<br>
    入力画像 ![image](https://user-images.githubusercontent.com/25688193/58793215-a4073a00-8630-11e9-9003-654296a737dd.png) と、再変換した出力画像 ![image](https://user-images.githubusercontent.com/25688193/58861424-4e429880-86e9-11e9-859d-d7f02ce20656.png) が同一人物であることを要請するために、入力画像 ![image](https://user-images.githubusercontent.com/25688193/58793215-a4073a00-8630-11e9-9003-654296a737dd.png) と再変換した出力画像 ![image](https://user-images.githubusercontent.com/25688193/58861424-4e429880-86e9-11e9-859d-d7f02ce20656.png) との間のピクセル単位でのL1ノルムで定義した損失関数。<br>

    image-to-image のタスクにおいては、画像の高周波成分（＝局所的な特徴量）と低周波成分（＝大域的な特徴量）の両方を捉えることが重要となる。<br>
    この内、画像の高周波成分（＝局所的な特徴量）に関しては、前述の識別器での PatchGAN の構造により、強く捉えることを可能にしている。<br>
    一方、画像の低周波成分（＝大域的な特徴量）に関しては、この identity loss における、ピクセル単位でのL1ノルムで強く捉えることを可能にしている。<br>


<a id="GANimationの実験結果"></a>

### ◎ GANimation の実験結果
以下の表は、GANimation の論文中での実験の実験条件を示している。<br>
![image](https://user-images.githubusercontent.com/25688193/58864647-125f0180-86f0-11e9-9489-f4256326e50c.png)<br>

- attention mask と color mask のレンダリング結果：<br>

    ![image](https://user-images.githubusercontent.com/25688193/58872041-214cb080-86fe-11e9-8b53-8679ec453cbd.png)<br>
    上図は、AUs の種類（AU1,AU2,...）に対応する各AUs 状態ベクトルから生成された attention mask と color mask 、及びそれらの合成画像を示した図である。<br>
    各AUs の種類に対応する表情が、うまく生成出来ていることがわかる。<br>

- ２つの表情の間の補間能力と照明条件等に対するロバスト性：<br>
    ![image](https://user-images.githubusercontent.com/25688193/58932430-a1c2ee00-879e-11e9-9962-6aafe35f89fe.png)<br>
    上図は、AUs 状態ベクトル y_r で表される元の画像 ![image](https://user-images.githubusercontent.com/25688193/58793215-a4073a00-8630-11e9-9003-654296a737dd.png) から、（変換させたい表情に対応した）AUs 状態ベクトルの目標ベクトル ![image](https://user-images.githubusercontent.com/25688193/58792837-e4b28380-862f-11e9-9857-2984b495ae48.png) で表される生成画像 ![image](https://user-images.githubusercontent.com/25688193/58793246-b4b7b000-8630-11e9-8e6c-59f63bb834b4.png) との間で、線形補間 ![image](https://user-images.githubusercontent.com/25688193/58932377-79d38a80-879e-11e9-8a7a-5d22b0b82f1b.png) の式に従って生成される補間画像の様子を示した図である。

    元の画像から目標表情へのスムーズな補間が出来ていることが見て取れる。<br>
    又、不自然な照明条件（４段目、５段目）や、アバターのような非現実世界の画像（６段目）に対しても、うまく表情変換が出来ていることが見て取れる。<br>

- 他の手法（CycleGAN, StarGAN等）との比較：<br>
    ![image](https://user-images.githubusercontent.com/25688193/58925481-a2e62200-8782-11e9-9b69-664531097358.png)<br>
    上図は、他の表情変換手法（DIAT, CycleGAN, IcGAN, StarGAN）との比較結果を示した図である。<br>
    公正な比較のために、RaFD データセットにおける、同一人物の離散的な感情のカテゴリ （例えば、幸せ、悲しみ、恐怖など）をレンダリングするタスクにおいて、（これらの手法の中で最も最近の研究である）StarGAN によって、学習されたこれらの手法の結果を適用した？<br>
    又、DIAT と CycleGAN は、条件付けの構造を持たないアルゴリズムなので（cGANの亜種ではない）、同一人物の元の感情（ソース）と目標感情（ターゲット）の全ての可能なペアに対して、独立的に学習させている。<br>

    上図からわかるように、GANimation でのレンダリング結果が、視覚的な正確性と空間的な解像度のトレードオフにおいて、最も高いクオリティーの画像を生成していることが見て取れる。<br>
    ※ GANimation では、生成画像のクオリティーが最も高いだけではなく、これら他の手法（DIAT, CycleGAN, IcGAN, StarGAN）と比較して、同一人物の異なる表情の画像ペア（＝ここでは、同一人物の離散的な感情のカテゴリ）を必要としていない点にも注目。<br>

    - 生成画像の表情バリエーション：<br>
        ![image](https://user-images.githubusercontent.com/25688193/58930099-0b3dff00-8795-11e9-8fc4-99fff7ab1d51.png)<br>
        上図は、ある１つの入力画像（最上段左隅）から、14 個の AUs状態ベクトル / AUs パラメーターを通じて生成された、同一人物のいくつかの種類の表情画像である。<br>
        GANimation では、ある１つの入力画像から、少数の AUs状態ベクトル / AUs パラメーターを通じて豊富な種類の同一人物の表情を生成出来ていることが分かる。<br>

    - wild な画像に対する変換：<br>
        ![image](https://user-images.githubusercontent.com/25688193/58931324-455dcf80-879a-11e9-97fe-fb0b12ad3f7a.png)<br>
        上図は、顔画像領域でトリミングされていない wild な画像に対する表情変換の結果を示した図である。<br>
        顔画像領域でトリミングされていない wild な画像であっても、うまく表情変換出来ていることが見て取れる。<br>

        但し、この変換においては、以下のような３ステップで構成されるトリミング処理を導入していることに注意。<br>

        1. 画像内の顔の位置を特定し、トリミングするために、顔検出器を使用する。<br>
        2. attention mask と color mask の合成式<br>
            ![image](https://user-images.githubusercontent.com/25688193/58796579-9eaded80-8638-11e9-80b1-9ac8ee2bda97.png)<br>
            を用いて、表情変換を、顔をトリミングした領域に適用する。<br>
        3. 生成された顔を、画像内の元の位置に戻す。<br>

    - GANimation の限界：<br>
        ![image](https://user-images.githubusercontent.com/25688193/58937572-c1154780-87ad-11e9-88aa-af02852a4078.png)<br>

    	上図は、いくつかの特徴的な画像に対しての表情変換タスクにおいて、その成功事例（➀ ~ ⑥）と失敗事例（⑦ ~ ⑩）を示した図である。各画像に対して以下のようなことが見てとれる。<br>

    	➀ 人間のような彫刻での顔画像 → 入力画像のアーティスティックな効果を保持したたまま表情変換出来ている。<br>
    	➁ メガネをかけているリアルではない顔画像 → attention map がメガネのような人工物をどのように避けているのかに注目。<br>
	    ➂ ひげなどの不均一なテクスチャーをもつ画像 → ひげを追加 or 削除することによって、テスクチャを均質化しようとしていない点に注目。<br>
	    ④ 非現実的な顔テクスチャーでの顔画像 → テクスチャーに影響を与えることなしに、表情を変換している点に注目。<br>
	    ➄ 非標準的な照明や色をもつ顔画像 → 非標準的な照明や色に対してもロバストになっている。<br>
	    ⑥ 顔のイラスト → このような単純なイラストであっても、うまく表情変換出来ている。<br>
	    ⑦ 入力画像が極端な表情をもつ顔画像 → このようなケースでは、attention メカニズムがうまく機能せず、うまく表情変換出来ない。<br>
    	⑧ 眼帯で顔の一部が隠れている顔画像 → うまく表情変換出来ていない。<br>
	    ⑨ サイクロプスのような実在しない擬人的な顔を持つ生物 → うまく表情変換出来ていない。<br>
        ⑩ 動物の顔画像 → 動物ではなく人間のような表情変換になっている。<br>

        尚、論文中では、このような失敗事例（⑦ ~ ⑩）が発生するのは、学習データの数が十分でなかったためと推測している。<br>

<a id="補足事項"></a>

## ■ 補足事項

<a id="KLダイバージェンス"></a>

### ◎ KLダイバージェンス [Kullback-Leibler(KL) diviergence]
２つの確率分布 P,Q 間の距離を表す指標として、KLダイバージェンスと呼ばれる以下のような指標が考えられる。<br>

![image](https://user-images.githubusercontent.com/25688193/56255740-ee743c00-6100-11e9-9e09-3574daab5b09.png)<br>

※ ここで、定義から分かるように、KLダイバージェンスは、２つの確率分布 P,Q に対して、対称ではない。（![image](https://user-images.githubusercontent.com/25688193/56255798-25e2e880-6101-11e9-9ff6-5abfdd6e8b82.png)）これは、例えば、A地点からB地点までの距離と、その反対のB地点からA地点までの距離が異なることを意味している。従って、距離の公理を満たしていないことになり、厳密には距離ではないことになるが、便宜上、統計的距離という。<br>

以下の図は、２つの確率分布 P,Q の具体的な確率分布の形状として、いくつかの正規分布でKLダイバージェンスを求めた図である。<br>

![image](https://user-images.githubusercontent.com/25688193/57173202-12f14780-6e67-11e9-9b26-9d036487fa6a.png)<br>

２つの確率分布 P,Q が完全に一致するとき、０の値となっており、<br>
２つの確率分布 P,Q が重ならない部分が大きくなるにつれ、大きな値となっていることが分かる。<br>

#### ☆ KLダイバージェンスとエントロピーの関係
KLダイバージェンスは、クロスエントロピーとエントロピーの差で記述することも出来る。<br>
即ち、エントロピーは<br>
![image](https://user-images.githubusercontent.com/25688193/56255899-95f16e80-6101-11e9-9791-95c012937999.png)<br>
クロスエントロピーは、<br>
![image](https://user-images.githubusercontent.com/25688193/56255930-ab669880-6101-11e9-9e78-d06bfecee5c4.png)<br>
であることより、<br>
![image](https://user-images.githubusercontent.com/25688193/56255946-bb7e7800-6101-11e9-9633-beb2a3426628.png)<br>
となり、従って、<br>
![image](https://user-images.githubusercontent.com/25688193/56255976-d6e98300-6101-11e9-9b53-a2304f892bc7.png)<br>
の関係が成り立つことがわかる。<br>


<a id="JSダイバージェンス"></a>

### ◎ JSダイバージェンス [Jensen-Shannon(JS) divergence]
KLダイバージェンスは、距離の公理の一つである対称性の性質を満たさない（![image](https://user-images.githubusercontent.com/25688193/56255798-25e2e880-6101-11e9-9ff6-5abfdd6e8b82.png)）ので、扱いづらいという問題があった。<br>
（※この対称性の性質を満たさないことは、例えば、A地点からB地点までの距離と、その反対のB地点からA地点までの距離が異なることを意味している。　）

JSダイバージェンスは、対称性の性質を満たすように、KLダイバージェンスを使って、以下のように定義される２つの確率分布 P,Q 間の距離指標である。<br>
![image](https://user-images.githubusercontent.com/25688193/56256830-3d23d500-6105-11e9-8d7d-4f101841b809.png)<br>

以下の図は、２つの確率分布 P,Q の具体的な確率分布の形状として、いくつかの正規分布でKLダイバージェンスとJSダイバージェンスを求めた図である。<br>

![image](https://user-images.githubusercontent.com/25688193/57173174-92cae200-6e66-11e9-929f-00a4bd409a50.png)<br>

黄色枠で示した確率分布は、２つの確率分布 P,Q の平均値である M である。<br>
この平均分布 M は、各々の確率分布 P,Q から見て対称であるが、<br>
JSダイバージェンスでは、その定義式より、（対称である）平均分布 M と各々の確率分布 P, Q とのKLダイバージェンスを計算することで、対称性の性質を満たすようにしていることがわかる。<br>
２つの確率分布 P,Q が重ならない部分が大きくなるにつれ、JSダイバージェンスは小さな値となっていることが分かる。<br>

<a id="Inceptionscore"></a>

### ◎ Inception score
GAN においては、生成画像を人目ではなく、如何にして自動的に評価するのか？というのが１つの課題となっている。<br>
Inception score は、このような問題を解決するために考案された、GANの生成画像を評価するための代表的な評価値の１つである。<br>

Inception score では、Inception モデルと呼ばれる画像識別モデルでの予想確率を用いて、スコアの計算が行われる。<br>
より詳細には、画像データを与えない場合の、Inception モデルのラベル y の予想確率 p(y) と、i 番目の画像データ x_i を入力した場合の、Inception モデルのラベル y の予測確率 ![image](https://user-images.githubusercontent.com/25688193/58064442-1271e400-7bbd-11e9-9ddc-bf85f2bfa048.png) を用いて、以下のように、その２つの確率分布間 ![image](https://user-images.githubusercontent.com/25688193/58064470-3a614780-7bbd-11e9-84b8-6266117564dd.png) のKL-ダイバージェンスの和で定義される。<br>

![image](https://user-images.githubusercontent.com/25688193/58064512-58c74300-7bbd-11e9-9989-fc7b1c5f8ac0.png)


ここで、Inception score は、上記のように、ISモデルの２つの予想確率分布間のKL-ダイバージェンスの和で定義されているので、２つの確率分布の差異が大きくなるほど、ISスコアも大きくなるようになっている。<br>

![image](https://user-images.githubusercontent.com/25688193/58064568-94620d00-7bbd-11e9-93ff-d98eeabfe97c.png)

上図は、あるケースにおける２つの確率分布 ![image](https://user-images.githubusercontent.com/25688193/58064470-3a614780-7bbd-11e9-84b8-6266117564dd.png) を形状を示した図である。<br>
p(y) は、画像データという条件を与えない場合の Inception モデルからの予想確率となっているので、一般的に、ある特定のクラスラベルの確からしさが分からず、一様分布に近い平坦な形状となっている。<br>
一方、![image](https://user-images.githubusercontent.com/25688193/58064442-1271e400-7bbd-11e9-9ddc-bf85f2bfa048.png) は、ある画像データ x_i を指定した場合の Inception モデルからの予想確率となっているので、その分ある特定のクラスラベルの確からしさが高まり、凹凸のある形状になっている。<br>
この際に、![image](https://user-images.githubusercontent.com/25688193/58064442-1271e400-7bbd-11e9-9ddc-bf85f2bfa048.png) において、ある特定のクラスラベルでの確からしさが高まり、凹凸が激しくなるほど、２つの分布間の差異が大きくなるので、ISスコアも大きくなる。（左図）<br>
逆に、ある特定のクラスラベルでの確からしさそれほど高まらず、比較的平坦になるほど、２つの分布間の差異が小さくなるので、ISスコアも小さくなる。（右図）<br>
これは言い換えると、「Inception モデルで識別しやすい画像であるほど、ISスコアが大きくなる。」 ことを意味している。<br>


<a id="EMD"></a>

### ◎ Earth-Mover距離（Wassertein距離）
Earth-Mover距離（Wassertein距離）は、２つの確率分布間の距離指標の１つであるが、下図のように、確率分布を１つの山とみなすと、片方の確率分布の山をもう片方の確率分布の山に，土を輸送して変形させるための最小の労力であるという最適輸送問題ともみなせる。<br>
（※それ故、Earth-Mover という名前がついている。）<br>

![image](https://user-images.githubusercontent.com/25688193/56466456-69b15700-644d-11e9-869d-6c6ebf66279d.png)<br>

![image](https://user-images.githubusercontent.com/25688193/56466469-88afe900-644d-11e9-9a29-4f37e14e1b4b.png)<br>

![image](https://user-images.githubusercontent.com/25688193/56466479-bdbc3b80-644d-11e9-9a23-c143b4f06f8d.png)<br>

そして、Earth-Mover距離（EMD,Wassertein距離）は、この 「（輸送する労力）＝（輸送する土の量）×（輸送する距離）」 の関係において、全ての組み合わせで総和した以下のような式で定義できる。<br>

![image](https://user-images.githubusercontent.com/25688193/56477963-17704480-64e6-11e9-9396-a72c7cae56af.png)<br>

２つの確率分布の山を一致させるさせるためには、無数の方法が考えられるが、そららの方法の中で最小の労力（＝最小のEMD）で済むような、最適な輸送方法を考えるのが、考えるべき最適輸送問題となる。<br>

そして、この最適輸送問題は、一般的な線形計画法を用いて解くことが出来る。<br>

しかし、結論から述べると、この最適輸送問題を、単純に線形計画法の主形式で解く方法では、GANのように確率変数 x,y の次元が大きい場合には、その計算量が現実的ではなくなる。<br>
一方、この線形計画問題の双対形式で与える式では、現実的に計算可能な式になっている。<br>

そして、この線形計画法の双対形式での、
Earth-Mover距離（EMD,Wassertein距離）の式は、以下のようになる。（式の導出略）<br>

![image](https://user-images.githubusercontent.com/25688193/56466815-c151c180-6451-11e9-8474-4fb0e160df4e.png)<br>

※ 関数 f のリプシッツ連続性の要件は、線形計画問題おける制約条件から発生している。<br>


<a id="UNet"></a>

### ◎ U-Net
セマンティックセグメンテーション（領域抽出）のタスクでは、局所的な特徴と、画像全体の特徴の両方を、元の画像上で捉えることが重要となる。<br>
CNN では、畳み込み層が物体の局所的な特徴を抽出し、プーリング層が物体の全体的な位置情報をぼかしている役割を持っていた。<br>
そのため、層が深くなるほど、抽出される特徴は、より局所的になり、物体の全体的な位置情報はより曖昧になるので、従来のCNNベースの手法（VGG16など）では、局所的な特徴量と、画像全体の特徴の両方を捉えることが困難となる。<br>

U-Net は、セマンティックセグメンテーションのタスクにおけるこの問題を解決するために、全結合層を利用してしない Fully Convolutional Network（FCN）のアーキテクチャをベースとして、ダウンサンプリングのパス（contractiong path）とアップサンプリングのパス（extractiong path）、及び、skip connection の構造を加えることで、この 「（セマンティックセグメンテーションで重要となる）局所的な特徴量と、画像全体の特徴の両方を捉えること」 を実現する。<br>

※ U-Net は、このように、画像のセマンティックセグメンテーションでの用途で考察された手法であり、医療画像セグメンテーションのコンペで当時の SOTA を達成している。<br>
※ 尚、この U-Net は、オートエンコーダー（AE）としての側面もある。<br>
※ 又、この U-Net は、pix2pix の生成器側のネットワークとして採用されている。<br>


<a id="UNetのアーキテクチャ"></a>

#### ☆ U-Net のアーキテクチャ
![image](https://user-images.githubusercontent.com/25688193/56942254-969ff100-6b54-11e9-81e2-f9ffe31eef12.png)<br>

上図は、U-Net のアーキテクチャ図である。<br>
この U-Net のアーキテクチャには、主に、以下のような特徴がある。<br>

- ダウンサンプリングとアップサンプリング（Encoder と Decoder）
    - 概要で述べたように、セマンティックセグメンテーションのタスクにおいては、物体の局所的な特徴と、物体の全体的な位置情報を同時に捉えることが重要となるが、U-Net では、max pooling によるダウンサンプリング側（Encoder側）の深い層で局所的な特徴を捉え、浅い層で、全体的な位置情報を捉えること出来るようにしている。
    - そして、up-conv によるアップサンプリング側（Decoder側）で、特徴を保持したまま、画像の解像度を復元し、出力する。
    - これらの一連の処理は、Encoder（符号器）と Decoder（復号器）に相当する処理となっているため、U-Net は、一種のオートエンコーダー（AE）ともみなせる。

- skip connections
    - U-Net の主な特徴の１つは、Encoder と Decoder との間に、skip connections と呼ばれる構造を持つことである。（※ この構造の効果は、ResNet に似ている。）
    - これは、CNNでの、Encoder-Decoder のように全ての情報をボトルネックまでダウンサンプリングさせるのではなく、共通の特徴量（＝局所的な特徴と物体の全体的な位置情報）は Encoder-Decoder 間を、skip connections 経由でスキップさせて、上図のアーキテクチャ図の下側のボトルネック部分を回避することで、共通の特徴量におけるデータ欠損を回避させるというものである。
    - より詳細には、Encoder 側の深い層ほど局所的特徴が強く、浅い層ほど全体的な位置情報が強くなっているので、深い層での skip connections は、局所的特徴が強い特徴マップをスキップさせ、浅い層での skip connections は、全体的な位置情報が強い特徴マップをスキップさせることになるが、Decoder 側では、特徴を保持したままアップサンプリングが可能であるので、結果として、「（セマンティックセグメンテーションで重要となる）局所的な特徴量と、画像全体の特徴の両方を捉えること」 を実現できるというものである。
    - 尚、Encoder 側からのスキップした特徴マップと、Decoder側からのアップサンプリングされた特徴マップをマージする際には、Encoder 側の特徴マップの中央部分を crop（切り出し）して、両者のサイズを一致させた上でマージを行う。これは、Encoder 側では、padding=0 の畳み込みで特徴サイズが（ダウンサンプリングとは違う意図で）小さくなっているために、Decoder 側で２倍間隔でアップサンプリングされた特徴マップと、サイズが一致しないくなってしまうためである。

- 全結合層を利用していない。
    - U-Net では、全結合層は使用されていない。
    - このような畳み込みのみからなるネットワークは、 Fully Convolutional Network（FCN）と呼ばれるが、U-Net は、この Fully Convolutional Network（FCN）のアーキテクチャをベースとして設計されている。
    - U-Net で全結合層が使用されていない理由としては、CNN での画像分類のアーキテクチャの出力（この場合は最後の層が全結合層になる必要がある）とは異なり、ネットワークの出力が画像であることが１つの理由として考えられる。
    - <font color="Pink">別の理由としては、overlap-tile strategy によって、任意の大きな画像のシームレスなセグメンテーションを許可するため？（元論文での記載より）</font>

<a id="UNetの学習方法">

#### ☆ U-Net の学習方法

> 記載中...


<a id="UNetの適用例">

#### ☆ U-Net の適用例

- [UNet によるセマンティックセグメンテーションを利用して、衛星画像から地図を生成する。](https://github.com/Yagami360/MachineLearning_Exercises_Python_PyTorch/tree/master/UNet_PyTorch)


<a id="Minibatchdiscrimination"></a>

### ◎ Minibatch discrimination
GAN における課題の１つにモード崩壊という問題が存在する。

この現象は、GAN のアーキテクチャにおいて、識別器に、生成器からの生成画像の多様性を認識できる構造がないために発生してしまう問題であるとみなすことも出来る。<br>
※ この多様性を知る構造が識別器に備わっていれば、多様性を増大するような方向に、誤差逆伝播法で勾配を生成器に伝搬すれば、多様性のある生成画像が自動生成出来て、モード崩壊を回避出来ることになる。<br>

Minibatch discrimation では、この考えに基づき、識別器に、ミニバッチデータ全体の多様性を、ミニバッチ間でのデータの距離から判断するようにする。<br>

以下の図は、この Minibatch discrimation での全体的な処理の流れを示した図である。<br>

![image](https://user-images.githubusercontent.com/25688193/58080159-9ba10f00-7bed-11e9-8cee-82301b043669.png)<br>
![image](https://user-images.githubusercontent.com/25688193/58080209-b4112980-7bed-11e9-995d-b295e7f8e244.png)<br>

上図に示した、Minibatch discriminator における、各処理の詳細は、以下のようになる。<br>

1. 各ミニバッチデータの画像 ![image](https://user-images.githubusercontent.com/25688193/58080387-15d19380-7bee-11e9-96a5-01484e16606d.png) に対して、識別器の中間層からの出力（＝特徴ベクトル）<br>
    ![image](https://user-images.githubusercontent.com/25688193/58080415-25e97300-7bee-11e9-9110-ea0540850a13.png)<br>
    を得る。<br>

2.  この各中間層からの出力 ![image](https://user-images.githubusercontent.com/25688193/58080415-25e97300-7bee-11e9-9110-ea0540850a13.png) に、３階のテンソル ![image](https://user-images.githubusercontent.com/25688193/58080454-3dc0f700-7bee-11e9-9f3f-944e94fcc959.png) を乗算して、B×C の行列<br>
    ![image](https://user-images.githubusercontent.com/25688193/58080504-503b3080-7bee-11e9-84af-09a3fa3b54e7.png)<br>
    を得る。（※ ここで、テンソルの次元数 B,C は学習可能なハイパーパラメータである。）<br>

3. 画像 x_i における行列 M_i の b行目のベクトル M_(i,b)  と、<br>
    別の画像 x_j における行列 M_j の b行目のベクトル M_(j,b) とのL1ノルムを<br>
    ![image](https://user-images.githubusercontent.com/25688193/58080670-955f6280-7bee-11e9-96ff-fb083b46d166.png)<br>
    で計算することで、２つの画像間の距離を計算する。<br>

4. 計算した２つの画像間の距離 ![image](https://user-images.githubusercontent.com/25688193/58080787-cb9ce200-7bee-11e9-8a90-b4db79dd357a.png) に対して、ミニバッチデータ内の他の全ての画像 ![image](https://user-images.githubusercontent.com/25688193/58080973-29312e80-7bef-11e9-81e5-f70424c28d48.png) に関しての和を取る。<br>
    ![image](https://user-images.githubusercontent.com/25688193/58080855-ee2efb00-7bee-11e9-825b-25ea51f52181.png)<br>
    ※ この計算結果は、ある画像とミニバッチ内でのその他の画像間の距離となっているが、これは見方を変えると、ミニバッチ内でのデータの多様性を示していることになる。従って、この処理（と後述の5）により、識別器にデータの多様性を認識できる構造が入っていることになる。<br>

5. 画像 x_i における行列 M_i の 行数 b=1~B に関して同じような計算を行い、それらを合わせてベクトル化する。<br>
    ![image](https://user-images.githubusercontent.com/25688193/58081744-8b3e6380-7bf0-11e9-9d6a-848bf90597f3.png)<br>

6. 先に得ていた識別器の中間層からの出力 f(x_i) と結合（concat）したもの<br>
    ![image](https://user-images.githubusercontent.com/25688193/58081651-6518c380-7bf0-11e9-877e-8fd289225af2.png)<br>
    を、次の層への入力とする。<br>


<a id="StyleTransferにおける正規化手法（INとAdaIN）"></a>

### ◎ Style Transfer における正規化手法（INとAdaIN）

- Instance Normalization（IN）:<br>
    BatchNorm のように、バッチサイズ単位で正規化するのではなく、特徴マップ単位（＝画像の縦横幅単位）で正規化する手法。<br>

    ![image](https://user-images.githubusercontent.com/25688193/57304779-ed8c6400-711a-11e9-9b8a-d1e88e6ecb23.png)<br>

    具体的には、以下の式で与えられる。<br>
    ![image](https://user-images.githubusercontent.com/25688193/60065219-0f2ec280-973e-11e9-82e9-8f609b096a01.png)<br>

    Style Transfer のタスクにおいては、Batch Normalization をこの Instance Normalization に変えることで、Style Transfer の質において大きな向上が達成されることが報告されている。

    ※ 論文「Instance Normalization: The Missing Ingredient for Fast Stylization」(https://arxiv.org/abs/1607.08022)<br>

    - Style Transfer での Instance Normalization による定性的な品質改善比較：
        ![image](https://user-images.githubusercontent.com/25688193/60065251-3b4a4380-973e-11e9-8918-d8c756965171.png)<br>

    - Style Transfer での Instance Normalization による定量的な品質改善比較：<br>
        ![image](https://user-images.githubusercontent.com/25688193/60065431-d2af9680-973e-11e9-8d6e-85a2a38c6bf8.png)<br>

- Adaptive Instance Normalization (AdaIN) :<br>
    ![image](https://user-images.githubusercontent.com/25688193/60065520-17d3c880-973f-11e9-81d6-2b280ce9e381.png)<br>

    AdaIN は、上図のように、StyleTransfer における変換タスクを実現するネットワークにおいて、instance norm の係数ベクトル（＝アフィンパラメーター）![image](https://user-images.githubusercontent.com/25688193/60066314-aa756700-9741-11e9-81b0-532ac38ae1ec.png) を、Style 画像での平均値と分散値 ![image](https://user-images.githubusercontent.com/25688193/60066343-c6790880-9741-11e9-9be3-6e7d3ce81141.png) に置き換えたものになっている。<br>
    ※ 言い換えると、AdaIN は、instance norm に対してスタイル特徴の統計量（＝平均値、分散値）の正規化を利用してスタイル変換を行っている。<br>

    ![image](https://user-images.githubusercontent.com/25688193/60066434-2374be80-9742-11e9-81a5-39a26e943c9e.png)<br>

    ※ このネットワークと AdaIN は、論文 「Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization」(https://arxiv.org/abs/1703.06868) で提案されたもの。

    StyleTrasnfer の文脈での instance norm では係数ベクトル（＝アフィンパラメーター）![image](https://user-images.githubusercontent.com/25688193/60066314-aa756700-9741-11e9-81b0-532ac38ae1ec.png) で、正規化された入力をどのようにスケーリング、平行移動するかを学習していたのに対して、この AdaIN では、これをスタイル画像（＝画風）の平均値と分散値で置き換えるので、パラメーターの学習は不要となる。但し、別途スタイル画像（＝画風）の平均値と分散値で計算する必要は出てくる。

    この AdaIN を用いることで、下図の比較結果のように、Style Transfer においてスタイル品質、速度（訓練時/推論時）、柔軟性（一度の訓練で適用できるスタイルの数）の面で高いレベルのトレードオフが得られる。<br>
    ※ 論文「Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization」では、ネットワーク構成や AdaIN の導入の他に、損失関数の工夫も提案しており、このトレードオフの向上はそれら全ての工夫で得られるものであるが、ここでは AdaIN の意味について着目して説明しているのでそれらの詳細は述べない。<br>

    - StyleTranfer での AdaIN による生成画像の定性的比較：<br>
        ![image](https://user-images.githubusercontent.com/25688193/60066702-eeb53700-9742-11e9-8c85-a0921e4ff45f.png)<br>

    - StyleTranfer での AdaIN による生成画像の定量的比較結果：<br>
        ![image](https://user-images.githubusercontent.com/25688193/60066760-12787d00-9743-11e9-808f-8ef465d42813.png)<br>


<a id="参考"></a>

## ■ 参考

### ◎ 参考サイト

**強調文字**付きは、特に参考にさせたもらったサイトです。<br>

- 全般
    - [**GAN（と強化学習との関係）**](https://www.slideshare.net/masa_s/gan-83975514)
    - [IIBMP2016 深層生成モデルによる表現学習](https://www.slideshare.net/pfi/iibmp2016-okanohara-deep-generative-models-for-representation-learning)
    - [タカハシ春の GAN 祭り！〜 一日一GAN(๑•̀ㅂ•́)و✧ 〜- ABEJA Arts Blog](https://tech-blog.abeja.asia/entry/everyday_gan)

- VAE
    - [**Variational Autoencoder徹底解説**](https://qiita.com/kenmatsu4/items/b029d697e9995d93aa24)
    - [**猫でも分かるVariational AutoEncoder**](https://www.slideshare.net/ssusere55c63/variational-autoencoder-64515581)
    - 実装
        - - [PyTorch (11) Variational Autoencoder - 人工知能に関する断創録](http://aidiary.hatenablog.com/entry/20180228/1519828344)

- GAN
    - [**【元論文】[1406.2661] Generative Adversarial Networks**](https://arxiv.org/abs/1406.2661)
    - [**今さら聞けないGAN（1）　基本構造の理解**](https://qiita.com/triwave33/items/1890ccc71fab6cbca87e)
    - [**Generative Adversarial Networks(GAN)を勉強して、kerasで手書き文字生成する - 緑茶思考ブログ**](http://yusuke-ujitoko.hatenablog.com/entry/2017/05/08/010314)
    - [GANの論文を読んだ自分なりの理解とTensorflowでのGANの実装メモ](http://owatank.hatenablog.com/entry/2018/04/20/180151)

- DCGAN
    - [**【元論文】[1511.06434] Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks**](https://arxiv.org/abs/1511.06434)
    - 実装
        - [**DCGAN Tutorial — PyTorch Tutorials 1.0.0.dev20190327 documentation**](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html?highlight=dataloader)
        - [GitHub - znxlwm/pytorch-MNIST-CelebA-GAN-DCGAN: Pytorch implementation of Generative Adversarial Networks (GAN) and Deep Convolutional Generative Adversarial Networks (DCGAN) for MNIST and CelebA datasets](https://github.com/znxlwm/pytorch-MNIST-CelebA-GAN-DCGAN)
        - [**PyTorch (12) Generative Adversarial Networks (MNIST) - 人工知能に関する断創録**](http://aidiary.hatenablog.com/entry/20180304/1520172429)

- Conditional GAN（cGAN）
    - [**【元論文】[1411.1784] Conditional Generative Adversarial Nets**](https://arxiv.org/abs/1411.1784)
    - [**今さら聞けないGAN（6） Conditional GANの実装**](https://qiita.com/triwave33/items/f6352a40bcfbfdea0476)
    - 実装
        - [GitHub/znxlwm/pytorch-MNIST-CelebA-cGAN-cDCGAN](https://github.com/znxlwm/pytorch-MNIST-CelebA-cGAN-cDCGAN)
        - [**PyTorch で Conditional GAN をやってみる**](http://cedro3.com/ai/pytorch-conditional-gan/)

- Wasserstein GAN
    - [**【元論文】[1701.07875] Wasserstein GAN**](https://arxiv.org/abs/1701.07875)
    - [**Read-through: Wasserstein GAN**](https://www.alexirpan.com/2017/02/22/wasserstein-gan.html)
    - [**From GAN to WGAN**](https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html)
    - [GAN — Wasserstein GAN & WGAN-GP](https://medium.com/@jonathan_hui/gan-wasserstein-gan-wgan-gp-6a1a2aa1b490)
    - [**Wasserstein GAN と Kantorovich-Rubinstein 双対性 - Qiita**](https://qiita.com/mittyantest/items/0fdc9ce7624dbd2ee134)
    - [**[DL輪読会]Wasserstein GAN/Towards Principled Methods for Training Generative Adversarial Networks**](https://www.slideshare.net/DeepLearningJP2016/dlwasserstein-gantowards-principled-methods-for-training-generative-adversarial-networks)
    - [Wasserstein GAN（WGAN）でいらすとや画像を生成してみる - 緑茶思考ブログ](http://yusuke-ujitoko.hatenablog.com/entry/2017/05/20/145924)
    - [Wasserstein GAN [arXiv:1701.07875] – ご注文は機械学習ですか？](http://musyoku.github.io/2017/02/06/Wasserstein-GAN/)
    - [Wasserstein GAN (WGAN) - DeepLearningを勉強する人](http://wanwannodao.hatenablog.com/entry/2017/02/28/051353)
    - 実装
        - [**GitHub - martinarjovsky/WassersteinGAN**](https://github.com/martinarjovsky/WassersteinGAN)

- pix2pix
    - [**【元論文】Image-to-Image Translation with Conditional Adversarial Networks**](https://arxiv.org/abs/1611.07004)
    - [**pix2pixでポップアートから写真を復元してみた (追記あり)**](https://qiita.com/hiromu1996/items/38f1bd5a78336fa8ca25)
    - [**深層学習を利用した食事画像変換で飯テロ**](https://qiita.com/negi111111/items/6d6f19edc060a91662ec)
    - [**[DL輪読会]Image-to-Image Translation with Conditional Adversarial Networks**](https://www.slideshare.net/DeepLearningJP2016/dlimagetoimage-translation-with-conditional-adversarial-networks)
    - [Image-to-Image Translation with Conditional Adversarial Nets](https://phillipi.github.io/pix2pix/)
    - [Image-to-Image Translation in Tensorflow](https://affinelayer.com/pix2pix/)
    - 実装
        - [GitHub/junyanz/pytorch-CycleGAN-and-pix2pix](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)
        - [**GitHub/eriklindernoren/PyTorch-GAN**](https://github.com/eriklindernoren/PyTorch-GAN#pix2pix)

- U-Net
    - [**【元論文】U-Net: Convolutional Networks for Biomedical Image Segmentation**](https://arxiv.org/abs/1505.04597)
    - [**Deep learningで画像認識⑨〜Kerasで畳み込みニューラルネットワーク vol.5〜**](https://lp-tech.net/articles/5MIeh)
    - [U-Net：ディープラーニングによるSemantic Segmentation手法](https://blog.negativemind.com/2019/03/15/semantic-segmentation-by-u-net/)
    - [U-Net: Convolutional Networks for Biomedical Image Segmentationの紹介](https://www.slideshare.net/KCSKeioComputerSocie/unet-convolutional-networks-for-biomedicalimage-segmentation?ref=https://blog.negativemind.com/2019/03/15/semantic-segmentation-by-u-net/)
    - [オートエンコーダーとしてのU-Net（自己符号化から白黒画像のカラー化まで）](https://qiita.com/koshian2/items/603106c228ac6b7d8356)
    - [Semantic Segmentation — U-Net (Part 1)](https://medium.com/@keremturgutlu/semantic-segmentation-u-net-part-1-d8d6f6005066)
    - 実装
        - [**GitHub/GunhoChoi/Kind-PyTorch-Tutorial12_Semantic_Segmentation/**](https://github.com/GunhoChoi/Kind-PyTorch-Tutorial/tree/master/12_Semantic_Segmentation)

- Minibatch discrimation
    - [**Improved Techniques for Training GANs [arXiv:1606.03498] – ご注文は機械学習ですか？**](http://musyoku.github.io/2016/12/23/Improved-Techniques-for-Training-GANs/)
    - [Toy ProblemでGANのmode collapseを可視化 - Qiita](https://qiita.com/haru-256/items/b9584d404da3d9ea51e0)

- CycleGAN
    - [**【元論文】[1703.10593] Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks**](https://arxiv.org/abs/1703.10593)
    - [CycleGAN Project Page](https://junyanz.github.io/CycleGAN/)
    - [CycleGANについて](https://www.slideshare.net/yoheiokawa/cyclegan-88149870)
    - [GANで犬を猫にできるか~cycleGAN編(1)~ - Qiita](https://qiita.com/itok_msi/items/b6b615bc28b1a720afd7)
    - 実装
        - [GitHub/junyanz/pytorch-CycleGAN-and-pix2pix](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)

- StarGAN
    - [**【元論文】[1711.09020] StarGAN: unified generative adversarial networks for multi-domain image-to-image translation**](https://arxiv.org/abs/1711.09020)
    - [**[DL輪読会]StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation**](https://www.slideshare.net/DeepLearningJP2016/dlstargan-unified-generative-adversarial-networks-for-multidomain-imagetoimage-translation)
    - [【論文メモ:StarGAN】StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation](https://tsunotsuno.hatenablog.com/entry/stargan)
    - 実装
        - [GitHub/yunjey/stargan](https://github.com/yunjey/stargan)

- SAGAN
    - [**【元論文】Self-Attention Generative Adversarial Networks**](https://arxiv.org/abs/1805.08318)
    - [[DL輪読会]Self-Attention Generative Adversarial Networks](https://www.slideshare.net/DeepLearningJP2016/dlselfattention-generative-adversarial-networks-100362745)
    - 実装
        - [GitHub/brain-research/self-attention-gan](https://github.com/brain-research/self-attention-gan)

- ProgressiveGAN
    - [**【元論文】[1710.10196] Progressive Growing of GANs for Improved Quality, Stability, and Variation**](https://arxiv.org/abs/1710.10196)
    - [**【Progressive Growing of GANs for Improved Quality, Stability, and Variation】を読んだのでまとめる - St_Hakky’s blog**](https://www.st-hakky-blog.com/entry/2017/11/22/173112)
    - [【論文メモ:PGGAN】Progressive Growing of GANs for Improved Quality, Stability, and Variation - Re:ゼロから始めるML生活](https://tsunotsuno.hatenablog.com/entry/pggan)
    - [GAN — Progressive growing of GANs – Jonathan Hui – Medium](https://medium.com/@jonathan_hui/gan-progressive-growing-of-gans-f9e4f91edf33)

- StyleGAN
    - [**【元論文】[1812.04948] A Style-Based Generator Architecture for Generative Adversarial Networks**](https://arxiv.org/abs/1812.04948)
    - [**StyleGAN「写真が証拠になる時代は終わった。」 - Qiita**](https://qiita.com/Phoeboooo/items/7be15acb960837adab21)
    - [**StyleGAN: A Style-Based Generator Architecture for Generative Adversarial Networks - 機械学習奮闘記**](https://sakuma-dayo.hatenablog.com/entry/2019/03/06/201429)
    - [**Style-based GANs – Generating and Tuning Realistic Artificial Faces | Lyrn.AI**](https://www.lyrn.ai/2018/12/26/a-style-based-generator-architecture-for-generative-adversarial-networks/)
    - Style Transfer
        - [**ニューラルネットワークでStyle Transferを行う論文の紹介 - Qiita**](https://qiita.com/kidach1/items/0e7af5981e39955f33d6)
        - [**Arbitrary Style Transfer in Real-Time With Adaptive Instance Normalization の紹介**](https://www.slideshare.net/HHiroto/arbitrary-style-transfer-in-realtime-with-adaptive-instance-normalization-84174047)
    - 実装
        - [GitHub/NVlabs/stylegan](https://github.com/NVlabs/stylegan)
        - [PyTorchでStyleGAN - Qiita](https://qiita.com/t-ae/items/afc969c48450507dc421)

- GANimation
    - [**【元論文】[1807.09251] GANimation: Anatomically-aware Facial Animation from a Single Image**](https://arxiv.org/abs/1807.09251)
    - [要約: GANimation: Anatomically-aware Facial Animation from a Single Image - toaruharunohi’s diary](http://toaruharunohi.hatenablog.com/entry/2018/09/14/134748)
    - 実装
        - [GitHub/albertpumarola/GANimation](https://github.com/albertpumarola/GANimation)

- その他
    - [**KL divergenceとJS divergenceの可視化**](http://yusuke-ujitoko.hatenablog.com/entry/2017/05/07/200022)
    - [正規分布間のKLダイバージェンス](https://qiita.com/ceptree/items/9a473b5163d5655420e8)
    - [**bluewidz nota: Inception score**](http://bluewidz.blogspot.com/2017/12/inception-score.html)

