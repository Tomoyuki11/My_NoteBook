# 強化学習 [Reinforcement Learning]（執筆中）
機械学習の一種である強化学習について勉強したことをまとめたノート（忘備録）です。随時追記中。<br>

- 【参考サイト】
    - [今さら聞けない強化学習（1）：状態価値関数とBellman方程式](https://qiita.com/triwave33/items/5e13e03d4d76b71bc802)
    - [強化学習について学んでみた。（まとめ） - いものやま。](http://yamaimo.hatenablog.jp/entry/2016/01/11/200000)
    - [第３章： 強化学習の基礎理論](http://sysplan.nams.kyushu-u.ac.jp/gen/edu/RL_intro.html#Foundations)<br>
    - [これから強化学習を勉強する人のための「強化学習アルゴリズム・マップ」と、実装例まとめ](https://qiita.com/sugulu/items/3c7d6cbe600d455e853b)
    - [ゼロからDeepまで学ぶ強化学習](https://qiita.com/icoxfog417/items/242439ecd1a477ece312)
    - [dissecting reinforcement learning](https://github.com/mpatacchiola/dissecting-reinforcement-learning)


## 目次 [Contents]

1. [概要](#概要)
    1. [強化学習の各種アルゴリズムの関係](#強化学習の各種アルゴリズムの関係)
    1. [強化学習で特有の用語説明](#強化学習で特有の用語説明)
1. [強化学習のモデル化](#強化学習のモデル化)
    1. [エージェントと環境の相互作用](#エージェントと環境の相互作用)
    1. [エピソード的タスクと連続タスク](#エピソード的タスクと連続タスク)
    1. [環境のマルコフ性](#環境のマルコフ性)
    1. [マルコフ決定過程（MDP）](#マルコフ決定過程（MDP）)
        1. マルコフ決定過程の例
    1. [価値関数](#価値関数)
    1. [状態価値関数](#状態価値関数)
    1. [行動価値関数](#行動価値関数)
    1. [状態価値関数と行動価値関数の関係](#状態価値関数と行動価値関数の関係)
    1. [ベルマン方程式](#ベルマン方程式)
        1. [ベルマン最適方程式とグリーディーな選択](#ベルマン最適方程式とグリーディーな選択)
1. 代表的な古典的アルゴリズムの比較
1. [ベルマン方程式を解くための動的計画法](#ベルマン方程式を解くための動的計画法)
    1. [ベルマンの方程式の厳密な解法と反復法](#ベルマンの方程式の厳密な解法と反復法)
    1. [方策評価](#方策評価)
        1. [反復方策評価](#反復方策評価)
    1. [方策改善](#方策改善)
        1. [方策改善定理](#方策改善定理)
        1. [方策改善定理とグリーディー方策](#方策改善定理とグリーディー方策)
        1. [方策改善のアルゴリズム](#方策改善のアルゴリズム)
    1. [方策反復法 [policy iteration]](#方策反復法)
    1. [価値反復法 [value iteration]](#価値反復法)
    1. 方策勾配法
        1. 方策勾配法の適用例
    1. [非同期動的計画法の概要](#非同期動的計画法の概要)
    1. [一般化方策反復（GPI）](#一般化方策反復（GPI）)
    1. 動的計画法の効率
1. [モンテカルロ法](#モンテカルロ法)
    1. [初回訪問モンテカルロ法](#初回訪問モンテカルロ法)
    1. [逐一訪問モンテカルロ法](#逐一訪問モンテカルロ法)
    1. [モンテカルロ-ES法](#モンテカルロ-ES法)
    1. [方策オン型モンテカルロ法](#方策オン型モンテカルロ法)
    1. [方策オフ型モンテカルロ法](#方策オフ型モンテカルロ法)
    1. モンテカルロ法による価値推定の適用例
1. [TD学習（時間的差分学習）](#TD学習（時間的差分学習）)
    1. TD(0)法
    1. [Sarsa（方策オン型TD制御）](#Sarsa)
        1. Sarsa の適用例
    1. [Q学習 [Q-learning]（方策オフ型TD制御）](#Q学習)
        1. Q 学習の適用例
    1. アクタークリティック手法
    1. [nステップTD法](#nステップTD法)
    1. [TD(λ)法](#TD(λ)法)
        1. [TD(λ)の前方観測的な見方](#TD(λ)の前方観測的な見方)
        1. [TD(λ)の後方観測的な見方と適格度トレース](#TD(λ)の後方観測的な見方と適格度トレース)
        1. 前方観測的見方と後方観測的見方の等価性
        1. [Sarsa(λ)](#Sarsa(λ))
        1. [Q(λ)](#Q(λ))
        1. アクタークリティック手法と適格度トレース
1. [関数による価値関数の近似（関数近似手法）](#関数による価値関数の近似（関数近似手法）)
    1. 線形関数での近似
    1. ニューラルネットワークでの関数近似による価値推定
1. 深層学習の構造を取り込んだアルゴリズム
    1. DQN [Deep Q-learning]
1. 補足事項
    1. [【外部リンク】機械学習](http://yagami12.hatenablog.com/entry/2017/09/17/111400)
    1. [【外部リンク】最適化問題](http://yagami12.hatenablog.com/entry/2017/09/17/101739)
    1. [【外部リンク】グラフ理論](http://yagami12.hatenablog.com/entry/2017/09/17/110406)
        1. [【補足（外部リンク）】確率過程とマルコフ連鎖](http://yagami12.hatenablog.com/entry/2017/09/17/110406#ID_3)
1. [参考文献](#参考文献)
    1. [使用コード](#使用コード)

---

<!--
<a id="概要"></a>

## ■ 概要
> 記載中...



<a id="強化学習の各種アルゴリズムの関係"></a>

### ◎ 強化学習の各種アルゴリズムの関係
> 記載中...

<a id="強化学習の各種アルゴリズムの関係"></a>

### ◎ 強化学習で特有の用語説明
> 記載中...

-->

<a id="強化学習のモデル化"></a>

## ■ 強化学習のモデル化

<a id="エージェントと環境の相互作用"></a>

### ◎ エージェントと環境の相互作用
![image](https://user-images.githubusercontent.com/25688193/50441622-e9676300-093e-11e9-8179-a4a7deefa11c.png)<br>

上図は、強化学習で取り扱うシステムを図示したものである。<br>
このシステムは、以下のように、エージェントと環境との相互作用でモデル化される。<br>

- エージェントとは、意思決定と学習（※将来に渡る収益の期待値を最大化するような学習）を行う行動主体である。<br>
- 環境とは、エージェントが相互作用を行う対象であり、エージェントの行動 a を反映した上で、エージェントに対して、状態 s と、その状態での報酬 r を与える。<br>
- エージェントの行動 a は、確率分布で表現される行動方策 [policy] ![image](https://user-images.githubusercontent.com/25688193/50441875-081a2980-0940-11e9-9edb-7faff8a50fa5.png) に基づいて選択される。<br>
	ここで、![image](https://user-images.githubusercontent.com/25688193/50441875-081a2980-0940-11e9-9edb-7faff8a50fa5.png) は、![image](https://user-images.githubusercontent.com/25688193/50441923-3d267c00-0940-11e9-997f-f25cd5dd12ca.png) ならば ![image](https://user-images.githubusercontent.com/25688193/50441977-5a5b4a80-0940-11e9-8306-2669589b23e1.png) となる確率を表す。<br>

- エージェントと環境は、継続的に相互作用を行う。<br>
	より詳細には、離散時間 t=0,1,2,... の各々の時間 t おいて、相互作用を行う。<br>
- エージェントがその意思決定目的とする収益は、時間経過 t=0,1,2,.. での時間経過によって、その値が減衰するものとする。<br>
	つまり、時間 t 時点から見て、将来 t+1,t+2,... にわたっての報酬 r の差し引きを考慮した割引収益<br>
	![image](https://user-images.githubusercontent.com/25688193/50442003-752dbf00-0940-11e9-9449-9c2d8391e466.png)<br>
	で考える。<br>
	※ 割引収益で考えるのは、同じ収益値ならば、将来受け取るよりも現時点で受け取るほうがよい値であることの妥当性もあるが、別の理由として、終端状態の存在しない連続タスクにおいて、割引なしの場合の将来にわたっての収益の和が、t→∞ の時間経過により、無限大に発散してしまい、エージェントの目的である将来にわたっての収益の最大化が、数学的に取り扱いずらい問題になってしまうことを防ぐという理由もある。<br>
- そして、エージェントは、時間経過 t=0,1,2,.. での相互作用によって、この将来にわたっての割引収益 ![image](https://user-images.githubusercontent.com/25688193/50442058-a1494000-0940-11e9-9af7-aaa8a2c8b836.png) の期待値 E[R] を最大化するように、自身の行動方策 ![image](https://user-images.githubusercontent.com/25688193/50441875-081a2980-0940-11e9-9edb-7faff8a50fa5.png) を学習しながら、行動する。<br>
	※ 一般的に、強化学習の枠組みにおいて、時間に依存する行動方策 ![image](https://user-images.githubusercontent.com/25688193/50442104-cc339400-0940-11e9-9471-d4864775f324.png) ではなく、時間 t によらない定常方策 ![image](https://user-images.githubusercontent.com/25688193/50442137-e40b1800-0940-11e9-8133-9b607406bb93.png) が用いられる。<br>
	（価値関数の満たすべき方程式を与えるベルマンの方程式は、定常方策 ![image](https://user-images.githubusercontent.com/25688193/50442137-e40b1800-0940-11e9-8133-9b607406bb93.png) を前提とした方程式となっている。）<br>

- ここで、エージェントと環境の相互作用の順序は、以下のようになる。<br>
	① エージェントは、環境から状態![image](https://user-images.githubusercontent.com/25688193/50442183-161c7a00-0941-11e9-95e2-c20d8df60afd.png)を受け取る。（＝エージェントの状態が![image](https://user-images.githubusercontent.com/25688193/50442183-161c7a00-0941-11e9-95e2-c20d8df60afd.png)になる。）<br>
	② エージェントは、現在の状態![image](https://user-images.githubusercontent.com/25688193/50442183-161c7a00-0941-11e9-95e2-c20d8df60afd.png)に基づき、行動方策 ![image](https://user-images.githubusercontent.com/25688193/50441875-081a2980-0940-11e9-9edb-7faff8a50fa5.png) に従って、行動![image](https://user-images.githubusercontent.com/25688193/50442241-58de5200-0941-11e9-8757-a4c9bddb8c28.png)を選択する。<br>
	③ 時間が１ステップ t→t+1 経過後に、エージェントは、エージェントの行動![image](https://user-images.githubusercontent.com/25688193/50442241-58de5200-0941-11e9-8757-a4c9bddb8c28.png)の結果として、報酬 ![image](https://user-images.githubusercontent.com/25688193/50442311-a6f35580-0941-11e9-90f9-2afbb39360c9.png) を受取る。<br>
	※ 時間 t で環境が受け取った行動![image](https://user-images.githubusercontent.com/25688193/50442241-58de5200-0941-11e9-8757-a4c9bddb8c28.png)による即時報酬という意味で、![image](https://user-images.githubusercontent.com/25688193/50442280-84f9d300-0941-11e9-8afb-968821fa5805.png)ではなく ![image](https://user-images.githubusercontent.com/25688193/50442311-a6f35580-0941-11e9-90f9-2afbb39360c9.png) を用いる。<br>
    ※ つまり、状態は行動を選択するための判断材料となり、報酬はその行動を評価するための判断材料となる。<br>

- 尚、強化学習の文脈において、エージェントと環境との相互作用の一連の時系列単位を、エピソード [Episode] という。<br>
	このエピソードは、エージェントの初期状態から始まって、環境との相互作用により終端状態まで移行するまでのの過程を、１回のエピソードとして扱う。<br>
	例えば、迷路探索問題においては、エージェントが、迷路のスタート地点からゴールまで辿りつくまでの一連の過程が、１回のエピソードになる。<br>
	強化学習による学習（＝行動方策の学習や価値関数の学習など）では、１回のエピソード内の各時間ステップ t→t+1 度に学習を行う手法（例えば、価値反復法）もあれば、１回のエピソードが完了する毎に学習を行う手法（例えば、方策反復症）も存在するので、エピソードと時間ステップを明確に区別しておく必要がある。<br>

> 【Memo】　エージェントと環境の境界線はどのように設定すべきか？<br>
> このようにモデル化される強化学習のシステムにおいて、実際上のタスクでは、どのようにエージェントと環境との線引を行えばよいのか？という問題は残る。<br>
> これは例えば、ロボット制御タスクにおいては、ロボットを構成するモーターやセンサーなどの物理的デバイスは、その内部の電子回路など自体は、エージェントであるロボット自身からは制御できにので、エージェントの一部というよりも、むしろ環境の一部とみなしたほうが適切であるのでは？と考えられる。<br>
> 同様の例として、人体制御タスクにおいては、人体を構成する筋肉や骨格や感覚器などは、エージェントの一部というよりも、むしろ環境の一部とみなしたほうが適切であるのでは？と考えられる。<br>
> この問題の一般的な基準を与える回答としては、エージェントがに任意に変更出来ないものは、エージェントの外部にあると考え、それらを環境とみなすという解決策が考えられる。<br>

このエージェントと環境の相互作用を、時間や相互作用の順序を含まて考える場合においては、以下のようなバックアップ線図で表現したほうがわかりやすい。<br>
例えば、以下の図は、マス目上に仕切られた迷路探索問題（移動方向が上下左右の４方向）における、バックアップ線図である。<br>
但し、この問題では、状態遷移関数 ![image](https://user-images.githubusercontent.com/25688193/50538553-af9f9200-0bb4-11e9-9442-301138dd7d21.png) が確率１で決定論的に定まるために、状態遷移関数による分岐が存在しないことに注意（青線部分）。<br>

![image](https://user-images.githubusercontent.com/25688193/50538389-8c73e300-0bb2-11e9-8cdc-bd78db544897.png)<br>

より一般的には、以下のバックアップ線図のように、状態遷移関数 ![image](https://user-images.githubusercontent.com/25688193/50538553-af9f9200-0bb4-11e9-9442-301138dd7d21.png) による分岐が存在する。<br>

![image](https://user-images.githubusercontent.com/25688193/50540536-ebe3ea00-0bd6-11e9-86c8-df661d23c4f5.png)<br>

尚、バックアップ線図を用いての詳細な議論は、後述の価値関数～ベルマンの方程式で議論する。<br>
※ バックアップ線図という名前は、価値関数の満たすべき方程式を記述するベルマンの方程式が、前回と今回の価値関数の値の再帰的関係（＝バックアップ）で記述されるためである。<br>


<a id="エピソード的タスクと連続タスク"></a>

### ◎ エピソード的タスクと連続タスク
> 記載中...


<a id="環境のマルコフ性"></a>

### ◎ 環境のマルコフ性
強化学習で生じるエージェントと環境との相互作用の過程（＝確率過程となる）において、時間 t で状態![image](https://user-images.githubusercontent.com/25688193/50442183-161c7a00-0941-11e9-95e2-c20d8df60afd.png)にあるエージェントによって選択された行動![image](https://user-images.githubusercontent.com/25688193/50442241-58de5200-0941-11e9-8757-a4c9bddb8c28.png)に対して、次の時間 t+1 において環境がどのように応答するのかという問題を考える。<br>

この問題の最も一般的なモデル化では、以前に生じた全ての事象 ![image](https://user-images.githubusercontent.com/25688193/50513328-7eeb2a00-0ada-11e9-80f9-e242fb707b51.png) に依存した条件付き同時確率分布でモデル化する。<br>
即ち、<br>
![image](https://user-images.githubusercontent.com/25688193/50513339-90cccd00-0ada-11e9-9796-779541f37c2d.png)<br>

しかしながら、最も一般的なモデルでは、過去の全ての事象に依存するために、問題設定や計算が複雑になるという問題が存在する。<br>
従って、強化学習のモデル化においては、一般的に、環境に対してマルコフ性の性質を仮定する。<br>

ここで、このマルコフ性とは、「ある状態![image](https://user-images.githubusercontent.com/25688193/50442183-161c7a00-0941-11e9-95e2-c20d8df60afd.png)から、別の状態 ![image](https://user-images.githubusercontent.com/25688193/50513369-bce84e00-0ada-11e9-9dd0-4307c3e2ceba.png) に移行する確率が、それ以前の経路 t によらず、現在の状態のみ![image](https://user-images.githubusercontent.com/25688193/50442183-161c7a00-0941-11e9-95e2-c20d8df60afd.png)で決まる。」という性質である。<br>
今の場合、環境に対してマルコフ性の性質を仮定するので、（環境がエージェントに与える）状態 s と報酬 r が対象となる。式で書くと、<br>
![image](https://user-images.githubusercontent.com/25688193/50513421-09cc2480-0adb-11e9-8bd1-d1931879f7fe.png)<br>
である。<br>

このように環境がマルコフ性の性質を満たすとき、現在の状態![image](https://user-images.githubusercontent.com/25688193/50442183-161c7a00-0941-11e9-95e2-c20d8df60afd.png)と行動![image](https://user-images.githubusercontent.com/25688193/50442241-58de5200-0941-11e9-8757-a4c9bddb8c28.png)さえ与えられてば、１ステップ t→t+1 間での環境による遷移確率 ![image](https://user-images.githubusercontent.com/25688193/50513466-57e12800-0adb-11e9-944a-0e37b676d907.png) から、次のステップ t+1 での状態 ![image](https://user-images.githubusercontent.com/25688193/50513369-bce84e00-0ada-11e9-9dd0-4307c3e2ceba.png) と行動 ![image](https://user-images.githubusercontent.com/25688193/50513450-32ecb500-0adb-11e9-9099-6a772a891938.png) を予測することが出来るので、これらの反復処理を逐次繰り返すことにより、過去の全ての履歴が与えられた場合と同様にして、将来における状態と行動を予測できる構造が織り込まれていることになる。（それ故に、マルコフ性の条件が重要である。）<br>
式で書くと、<br>
![image](https://user-images.githubusercontent.com/25688193/50513433-1fd9e500-0adb-11e9-8385-603a511642c9.png)<br>
である。<br>


<a id="マルコフ決定過程（MDP）"></a>

### ◎ マルコフ決定過程（MDP）
先に見たように、強化学習は、エージェント環境が各時間ステップ t=0,1,2,... で継続的に相互作用するシステムにおいて、エージェントが目標を示す数値である割引収益を最大化するように行動方策を学習制御するような過程 ![image](https://user-images.githubusercontent.com/25688193/50513530-b9a19200-0adb-11e9-8453-602482bab801.png) であって、更に、環境に対してマルコフ性の性質を仮定してモデル化された。<br>

このようなモデルは、マルコフ決定過程 [MDP:Markov decision process] の枠組みで厳密にモデル化出来る。
このマルコフ決定過程は、マルコフ性とエルゴード性を持つ確率過程であって、選択可能な行動、及び、報酬を追加したものである。<br>
言い換えると、マルコフ決定過程は、マルコフ連鎖に対して、選択可能な行動、及び、報酬を追加したものである。<br>

- エルゴード性（定常性）：<br>
    任意の初期状態からスタートした確率過程が、無限時間経過後には、最初の状態とは無関係になる性質。<br>
    マルコフ連鎖は、このエルゴード性の性質も合わせ持つ。<br>

<br>

![image](https://user-images.githubusercontent.com/25688193/50538944-a7972080-0bbb-11e9-86ce-31699b9f41c4.png)<br>

> 【Memo】<br>
> 「速習 強化学習」の本に書いてあるマルコフ決定過程の定義、状態集合、行動集合、遷移確率カーネルの３つの構造の組から定義されてるのに対し、他の文献では、状態集合、行動集合、遷移確率、報酬関数４つの組から定義されてものが多いが、３つの構造の１つの遷移確率カーネルが、状態遷移と報酬関数を与えるから、報酬関数を新たに３つの組とは別に追加しなくても well-defined となる。<br>

このマルコフ決定過程による過程は、以下の遷移グラフで図示するとわかりやすい。<br>
但し、この遷移グラフでは、行動方策による行動の選択の分岐は表現できていないことと、時間ステップ t に対する ![image](https://user-images.githubusercontent.com/25688193/50513611-23ba3700-0adc-11e9-89af-9b8d7e64254d.png) のダイナミクス ![image](https://user-images.githubusercontent.com/25688193/50513625-33398000-0adc-11e9-9038-3148ec0c7335.png) は表現していないこと注意。（添字の位置に注意）<br>

![image](https://user-images.githubusercontent.com/25688193/50513600-100ed080-0adc-11e9-912d-c3b3a41abe56.png)<br>

時間ステップによるダイナミクス、行動方策による行動の選択の分岐と、遷移確率による状態の遷移の分岐を含めて表現するには、以下のバックアップ線図で図示するとわかりやすい。<br>

![image](https://user-images.githubusercontent.com/25688193/50540536-ebe3ea00-0bd6-11e9-86c8-df661d23c4f5.png)<br>

- 【参考サイト】<br>
    - [マルコフ決定過程 - Wikipedia](https://ja.wikipedia.org/wiki/%E3%83%9E%E3%83%AB%E3%82%B3%E3%83%95%E6%B1%BA%E5%AE%9A%E9%81%8E%E7%A8%8B)<br>



<a id="価値関数"></a>

### ◎ 価値関数
> 記載中...

<!--
- 定常方策<br>
	マルコフ決定過程のマルコフ性より、行動方策 ![image](https://user-images.githubusercontent.com/25688193/50535412-08f3cb00-0b8d-11e9-99a9-48cb8d5b501c.png) は、全ての過程の履歴に依存した ![image](https://user-images.githubusercontent.com/25688193/50535431-435d6800-0b8d-11e9-8896-b0d52bbaee87.png) ではなく、時間ステップ間 t→t+1 にのみ依存する。このような行動方策を定常方策 ![image](https://user-images.githubusercontent.com/25688193/50535449-81f32280-0b8d-11e9-9b09-529f91684bc7.png) という。<br>
    以降の議論では、行動方策というと、定常方策であることを前提とする。<br>
-->

- 状態価値関数：![image](https://user-images.githubusercontent.com/25688193/50574476-a6b8f580-0e2c-11e9-9452-59c5584dd01d.png)<br>
    状態 s で現在の行動方策 ![image](https://user-images.githubusercontent.com/25688193/50574479-bdf7e300-0e2c-11e9-8270-089af93d1e67.png) に従いつづけた（＝定常方策）ときに得られる価値。<br>

- 行動価値関数：![image](https://user-images.githubusercontent.com/25688193/50574484-df58cf00-0e2c-11e9-8bc5-5ad14a34838b.png)<br>
    状態 s で行動 a を選択し、その後は既存の行動方策 ![image](https://user-images.githubusercontent.com/25688193/50574479-bdf7e300-0e2c-11e9-8270-089af93d1e67.png) に従いつづけた（＝定常方策）ときに得られる価値。<br>

<br>

- 【参照サイト】<br>
    - [今さら聞けない強化学習（1）：状態価値関数とBellman方程式 - Qiita](https://qiita.com/triwave33/items/5e13e03d4d76b71bc802)<br>
    - [今さら聞けない強化学習（3）：行動価値関数とBellman方程式 - Qiita](https://qiita.com/triwave33/items/8966890701169f8cad47)<br>
    - [人工知能概論 第6回 多段決定(2) 強化学習.](https://slidesplayer.net/slide/11477412/)<br>


<a id="状態価値関数"></a>

### ◎ 状態価値関数
先に述べたように、強化学習をマルコフ決定過程でモデル化するにあたって、エージェントの目的は、自身の行動則 π により目的関数である割引収益<br>
![image](https://user-images.githubusercontent.com/25688193/50535390-cfbb5b00-0b8c-11e9-9f32-ee8b8be25f14.png)<br>
の期待値を最大化することであったが、これを時間に依存しない行動方策である定常方策での収益に限定された状態価値関数の概念を用いて、モデル化することを考える。<br>

![image](https://user-images.githubusercontent.com/25688193/50902908-9c67ae80-145f-11e9-941f-ce62137f801d.png)<br>

以下の図は、バックアップ線図に状態価値関数 ![image](https://user-images.githubusercontent.com/25688193/50538680-330db300-0bb6-11e9-8f4f-3aa37c5878e2.png) を記した図である。<br>

![image](https://user-images.githubusercontent.com/25688193/50540754-03bd6d00-0bdb-11e9-804a-7a92b0438156.png)<br>

このバックアップ図より、状態価値関数の定義にある将来に対する割引収益の期待値 ![image](https://user-images.githubusercontent.com/25688193/49739267-c9943600-fcd4-11e8-9f8a-88ee2de9e046.png) は、青枠内での（将来における）報酬 r の平均をとったものになっていることが分かる。<br>

更に、この将来の割引総和に関しての期待値は、上図から分かるように、行動方策での分岐（赤線）の和の部分 ![image](https://user-images.githubusercontent.com/25688193/49739452-44f5e780-fcd5-11e8-935e-dfacc1315420.png) と遷移関数での分岐（青線）の和の部分 ![image](https://user-images.githubusercontent.com/25688193/50540677-9b21c080-0bd9-11e9-9b33-f532231b154b.png) で各々平均値をとったものに対応しているので、以下の関係が成り立つことが分かる。<br>
![image](https://user-images.githubusercontent.com/25688193/50540682-ac6acd00-0bd9-11e9-8260-30441b653835.png)<br>
尚、この関係式を上図のバックアップ線図から直感的にではなく、式変形で示すと以下のようになる。<br>
![image](https://user-images.githubusercontent.com/25688193/50540685-c60c1480-0bd9-11e9-9677-1ce92a9f5c13.png)<br>

ここで、この関係式は、状態価値関数に関しての方程式になっており、（状態価値関数に対しての）ベルマンの方程式という。<br>
そして、このベルマンの方程式は不動点方程式であるので、この方程式を解けば、原理的には不動点としての状態価値関数の最適解が一意に存在することになる（詳細は後述）<br>


<a id="行動価値関数"></a>

### ◎ 行動価値関数
マルコフ決定過程において、価値関数の一種として、定常方策 ![image](https://user-images.githubusercontent.com/25688193/49683687-38726300-fb0c-11e8-8de2-b79447dcff37.png) の元での、状態 ![image](https://user-images.githubusercontent.com/25688193/49683696-4a540600-fb0c-11e8-984c-e6f2eccdeb94.png) にいて行動 ![image](https://user-images.githubusercontent.com/25688193/49683701-62c42080-fb0c-11e8-8690-62119c18f03d.png) を選択する価値を表わす、行動価値関数 ![image](https://user-images.githubusercontent.com/25688193/49683704-72dc0000-fb0c-11e8-841c-0c42230bcc5b.png) なるものを考えることも出来る。<br>

![image](https://user-images.githubusercontent.com/25688193/50902937-b6a18c80-145f-11e9-815a-41f66bef84cf.png)<br>

以下の図は、バックアップ線図に状態価値関数 ![image](https://user-images.githubusercontent.com/25688193/50538680-330db300-0bb6-11e9-8f4f-3aa37c5878e2.png) を記した図である。<br>

![image](https://user-images.githubusercontent.com/25688193/50540791-d1f8d600-0bdb-11e9-94aa-039af2de8b79.png)<br>

このバックアップ線図より、状態行動関数の定義にある割引収益の期待値 ![image](https://user-images.githubusercontent.com/25688193/49915218-bb623780-fed7-11e8-8daf-fe4499115302.png) は、青枠内での報酬 r の平均をとったものになっていることが分かる。<br>


<a id="状態価値関数と行動価値関数の関係"></a>

### ◎ 状態価値関数と行動価値関数の関係
価値関数には、状態価値関数と行動価値関数の２つが存在するが、これら２つの関数は、互いに結びつく。<br>
このことをバックアップ線図を元に見ていく。<br>

以下の図は、バックアップ線図に状態価値関数 ![image](https://user-images.githubusercontent.com/25688193/50538680-330db300-0bb6-11e9-8f4f-3aa37c5878e2.png) と行動価値関数 ![image](https://user-images.githubusercontent.com/25688193/49683704-72dc0000-fb0c-11e8-841c-0c42230bcc5b.png) とを記した図である。<br>

![image](https://user-images.githubusercontent.com/25688193/50540879-7b8c9700-0bdd-11e9-833c-88b86ab42533.png)<br>

このバックアップ線図から、以下のような関係が成り立つことが分かる。<br>

![image](https://user-images.githubusercontent.com/25688193/50540839-6d8a4680-0bdc-11e9-97dc-735c210affd5.png)<br>
→ 将来の期待利得である状態価値関数 ![image](https://user-images.githubusercontent.com/25688193/49739008-2c390200-fcd4-11e8-8c93-0609a67c8915.png) は、（現在の期待利得である）行動価値関数 ![image](https://user-images.githubusercontent.com/25688193/49683704-72dc0000-fb0c-11e8-841c-0c42230bcc5b.png) に対して、行動方策に関しての分岐（赤枠部分）で和をとった形で表現できる。<br>

![image](https://user-images.githubusercontent.com/25688193/50540841-84c93400-0bdc-11e9-8675-580dc0a713e2.png)<br>

以上の関係式をまとめると、以下のようになる。<br>

![image](https://user-images.githubusercontent.com/25688193/50546866-3c5c5580-0c72-11e9-96a1-e1fc87df116a.png)<br>

- （証明略）バックアップ線図より成り立つことが分かる。<br>


<a id="ベルマン方程式"></a>

### ◎ ベルマン方程式
次の問題は、これら最適価値関数 ![image](https://user-images.githubusercontent.com/25688193/49915291-185ded80-fed8-11e8-8eff-72bd73109142.png),![image](https://user-images.githubusercontent.com/25688193/49916303-b0120a80-fedd-11e8-8590-12a538918666.png) を如何にして求めるかということである。<br>
最適価値関数 ![image](https://user-images.githubusercontent.com/25688193/49915291-185ded80-fed8-11e8-8eff-72bd73109142.png),![image](https://user-images.githubusercontent.com/25688193/49916303-b0120a80-fedd-11e8-8590-12a538918666.png) は、以下のベルマン方程式と呼ばれる、最適解に関しての不動点方程式から求めることが原理的には可能となる。<br>
※ ”原理的には可能” と表現したのは、ベルマン方程式が最適価値関数に関しての不動点方程式となっており、収束先の不動点としての最適解を持つことが保証されるが、実際上この方程式を直接解くことは困難であり、代わりに動的計画法（価値反復、方策反復など）の手法で近似解を得るようにすることが多いため。<br>

まずは、最適価値関数のうち、最適状態価値関数 ![image](https://user-images.githubusercontent.com/25688193/49916303-b0120a80-fedd-11e8-8590-12a538918666.png) に対してのベルマン方程式を考える。<br>

![image](https://user-images.githubusercontent.com/25688193/50903008-e0f34a00-145f-11e9-9600-449b33449ef0.png)<br>

- （証明略）先の「状態価値関数」の項目を参照<br>

<br>

![image](https://user-images.githubusercontent.com/25688193/50903046-f8323780-145f-11e9-8df0-1dce456d7928.png)<br>

- （証明略）先の「状態価値関数」の項目を参照<br>

<br>

同様にして、最適行動価値関数 ![image](https://user-images.githubusercontent.com/25688193/49915291-185ded80-fed8-11e8-8eff-72bd73109142.png) に対してもベルマン方程式が成り立ち、この不動点方程式の収束解（＝不動点）としての最適行動価値関数の値の存在性が保証される。<br>

![image](https://user-images.githubusercontent.com/25688193/50903086-113ae880-1460-11e9-848c-22b812712d27.png)<br>

- （証明略）先の「状態価値関数と行動価値関数の関係」の項目を参照<br>

<br>

- 【参考サイト】<br>
    - [Q-learningの収束性](https://qiita.com/ashigirl966/items/573d533180df49021f28)<br>


<a id="ベルマン最適方程式とグリーディーな選択"></a>

#### ☆ ベルマン最適方程式とグリーディーな選択
一般的に、コンピューターサイエンスの分野におけるグリーディーの意味とは、「長期的・大域的には、より良い代替案・最適解を見つかる可能性を考慮せずに、短期的・局所的な情報の中でのみ、代替案・最適解を見つけるような検索方針・意思決定手続き」のこのとを指す。<br>

今考えている、最適価値関数 ![image](https://user-images.githubusercontent.com/25688193/50548596-f06dd880-0c92-11e9-8f2f-8831c8f089b3.png) は、ベルマン最適方程式<br>
![image](https://user-images.githubusercontent.com/25688193/50548622-0aa7b680-0c93-11e9-853e-3ba507cae13a.png)<br>
の解として与られるが、この２つの方程式は、いずれも、１ステップ間 ![image](https://user-images.githubusercontent.com/25688193/50548641-70943e00-0c93-11e9-9f7f-c296533f8431.png) での短期的なグリーディーな選択 ![image](https://user-images.githubusercontent.com/25688193/50548645-7ee25a00-0c93-11e9-82ea-ce42e67f3d07.png) を取りさえすれば、１エピーソード間の長期的な意味でも最適解に到達できることを意味している。<br>
言い換れば、１ステップ間でのその時点での短期的で局所的なグリーディーな選択さえ続けていれば、エピーソード間での大域的最適解が得られることを意味している。<br>

これは、価値関数は、”将来”に対して報酬の期待値として定義しているために、将来の全ての可能な挙動がもららす報酬が、既に考慮されていることに起因する。<br>


> 【Memo】ベルマン最適方程式と大域的最適解、価値関数の定義について<br>
> ベルマン最適方程式、ベルマン最適作用素が縮小写像になってて、不動点としての価値関数の最適解を持つという見方でだけで見ると、まあそうだよなあ～くらいの感想になるけど、<br>
> このベルマン最適方程式の解としての最適価値関数が、大域的最適解であって、これは１ステップ間の局所的でグリーディーな行動選択を繰り返しさえすれば得れるということ。<br>
> そして、この性質（局所的→大域的）は、状態価値関数が、将来の全ての過程による報酬を考慮した定義になっていることに起因している（逆に言えば、そうなるように状態価値関数を定義している）という見方で見ると、理論構築、モデル化のうまさに感心する。<br>


<a id="ベルマン方程式を解くための動的計画法"></a>

## ■ ベルマン方程式を解くための動的計画法
> 要書き換え...

先のベルマンの方程式<br>
![image](https://user-images.githubusercontent.com/25688193/50536344-b5867a80-0b96-11e9-8c24-a4f95f038217.png)<br>
が示す重要な性質は、「ある状態や行動の価値 ![image](https://user-images.githubusercontent.com/25688193/49739008-2c390200-fcd4-11e8-8c93-0609a67c8915.png), ![image](https://user-images.githubusercontent.com/25688193/49683704-72dc0000-fb0c-11e8-841c-0c42230bcc5b.png) は、その後の状態や行動の価値 ![image](https://user-images.githubusercontent.com/25688193/49915737-6673f080-feda-11e8-97e7-a3e6a573cb44.png), ![image](https://user-images.githubusercontent.com/25688193/50039721-0caa2c80-007a-11e9-8f8b-b1700677dc38.png) を用いて表すことが出来る。」という、ブートストラップ性の性質である。<br>
（※機械学習で用られるデータ分割手法の１つであるブートストラップ法と異なるものであることに注意）<br>

このような価値関数のブートストラップ性を利用して、価値関数を反復的に枝分かれをたどりながら逐次計算していく手法を総称して、動的計画法という。<br>
※ この動的計画法で計算可能になるためには、ベルマンの方程式に出てくる状態遷移関数 ![image](https://user-images.githubusercontent.com/25688193/50536348-c9ca7780-0b96-11e9-9f54-67ee92954561.png) が既知であることが条件になる。言い換えると、エージェントの環境のダイナミクスが既知であることが条件になる。<br>
※ これに対して、環境のダイナミクスが既知でなくとも有効な価値関数の計算手法として、価値推定法（モンテカルロ法による価値推定法など）がある。（詳細は、後述）<br>

ここで、このベルマンの方程式は、価値関数に対しての不動点方程式にもなっており、この不動点方程式が導く不動点としての最適価値関数の存在の一意性は、価値反復法 [value iteration] や方策反復法 [policy iteration] などの動的計画法のアルゴリズムで、最適解が得られることの根拠となる。<br>


<a id="ベルマンの方程式の厳密な解法と反復法"></a>

### ◎ ベルマンの方程式の厳密な解法と反復法
> 要書き換え...

まずは、価値反復法や方策反復法などの動的計画法による近似手法を用いず、ベルマンの方程式の式に従って、１エピーソード間を再帰的に総当たりで価値関数を求めていく方法の問題点を見てみる。<br>

![image](https://user-images.githubusercontent.com/25688193/50538389-8c73e300-0bb2-11e9-8cdc-bd78db544897.png)<br>

> 記載中...

方策評価をする（＝ベルマン方程式を解く）にあたっては、厳密な解法では、ベルマン方程式を、状態集合 S 個の未知変数 s∈S を持つ、S 個の連立一次方程式とみなし、これを解くことになるが、この厳密な解法では、一般的に計算量が膨大となり、現実的でない。<br>
そこで、反復法による近似解を用いる。<br>

<br>

反復法は、ベルマンの方程式を満たす価値関数、及びそのときの最適行動方策を、ベルマンの方程式の式にそのままに従って再帰的に総当たり計算するのではなく、今の状態と次の状態の２層間のみの繰り返し計算により近似的に求める動的計画法の一種である。<br>
より詳細には、<br>
- 今の状態 s と次の状態 s' の２層間のみでの計算を、k=0,1,2,... で更新しながら反復する。<br>
- 最適解に近づくまで、この更新処理 k=0,1,2,... を続ける。<br>
- 得られた最適解の近似から、最適行動方針を算出する。<br>
というのが基本的なアイデアである。<br>

![image](https://user-images.githubusercontent.com/25688193/50572784-2766fa00-0e0b-11e9-8c8b-0fcbb1ce55ab.png)<br>

反復法には、それぞれ価値反復法と方策反復法が存在するが、両者の違いは、価値関数の更新方針の違いにある。即ち、<br>

- 価値反復法では、価値関数の更新方針は、グリーディーな方策（＝価値関数をmax化する行動：![image](https://user-images.githubusercontent.com/25688193/49993657-98648000-ffca-11e8-8048-e6e8f69202f2.png)）に基づく更新。<br>
- 方策反復法での価値関数の更新方針は、行動方策に関しての全ての分岐 ![image](https://user-images.githubusercontent.com/25688193/49993632-81259280-ffca-11e8-975b-627395dd1696.png) に基づく更新。<br>
    従って、グリーディーで max のみを選択する価値反復法に比べて、計算量は増加する。<br>

<br>

- 【参考サイト】<br>
    - [今さら聞けない強化学習（5）：状態価値関数近似と方策評価](https://qiita.com/triwave33/items/bed0fd7a2b56ee8e7c29#_reference-085df97779eb2480be0f)<br>


<a id="方策評価"></a>

### ◎ 方策評価
状態価値関数 ![image](https://user-images.githubusercontent.com/25688193/50572772-fd153c80-0e0a-11e9-8470-2744fff5f836.png) は、状態 s において、定常方策 ![image](https://user-images.githubusercontent.com/25688193/50574565-1e3b5480-0e2e-11e9-9d9d-fcdc6204b91e.png) に従いつづけた際の価値を表している。<br>
従って、任意の定常方策 ![image](https://user-images.githubusercontent.com/25688193/50572762-d9ea8d00-0e0a-11e9-8faa-32f9a7cf8155.png) に対する、状態価値関数 ![image](https://user-images.githubusercontent.com/25688193/50572772-fd153c80-0e0a-11e9-8470-2744fff5f836.png) を求めることは、ある定常方策の評価指数となりうる価値を求めて、方策を評価していることになるので、（動的計画法の文脈では）”方策評価” という。<br>

状態価値関数についてのベルマン方程式<br>
![image](https://user-images.githubusercontent.com/25688193/50572779-0e5e4900-0e0b-11e9-9c4d-0910b01dc82a.png)<br>
は、ある行動方策 π のもとでの、状態価値関数を解としているので、ベルマン方程式を解くことは、方策評価となる。<br>


<a id="反復方策評価"></a>

#### ☆ 反復方策評価
![image](https://user-images.githubusercontent.com/25688193/50572784-2766fa00-0e0b-11e9-8c8b-0fcbb1ce55ab.png)<br>

上図のように、状態価値関数に対してのベルマン方程式を、更新規則とした更新式<br>
![image](https://user-images.githubusercontent.com/25688193/50574655-c6055200-0e2f-11e9-8d60-8c55bfdd3fae.png)<br>
から生成される近似列 ![image](https://user-images.githubusercontent.com/25688193/50574668-095fc080-0e30-11e9-90d8-5a936f8d73d5.png) を考える。<br>
すると、この近似列 ![image](https://user-images.githubusercontent.com/25688193/50574668-095fc080-0e30-11e9-90d8-5a936f8d73d5.png) は、k → ∞の極限で、![image](https://user-images.githubusercontent.com/25688193/50572807-9d6b6100-0e0b-11e9-91e6-562566b8a54f.png) に収束することが知られている（証明略）。<br>
このアルゴリズムは、ベルマン方程式を繰り返し解くこと、即ち、方策評価を繰り返し行い、価値関数を近似しているので、反復方策評価という。<br>

ここで、この反復方策評価のアルゴリズムには、以下のような特徴が存在する。<br>

- スイープ操作と完全バックアップ<br>
    一般的に、現在評価している行動方策 π の元で、実現可能な全ての１ステップ遷移（＝時間ステップ t→t+1 での遷移）に対して、バックアップ処理（＝時間ステップでの遷移後から遷移前の逆方向への更新処理）を行うことを完全バックアップという。<br>
    今の反復方策評価では、全ての可能な状態 ![image](https://user-images.githubusercontent.com/25688193/50572815-c4299780-0e0b-11e9-9c65-70a52285c37e.png) を sweep 操作しながら、１ステップ遷移 ![image](https://user-images.githubusercontent.com/25688193/50572822-d4da0d80-0e0b-11e9-919f-55dd7b5ca276.png) 毎に、１度ずつバックアップ処理（ ![image](https://user-images.githubusercontent.com/25688193/50572825-ea4f3780-0e0b-11e9-992e-6da6e107cb86.png) ）を行っているので、完全バックアップ処理を行っていることになる。（下図の迷路探索問題の例を参照）<br>
    ![image](https://user-images.githubusercontent.com/25688193/50572933-6ac26800-0e0d-11e9-9ab9-1a713c1e62df.png)<br>
    cf : サンプルバックアップ<br>
    <br>
- その場更新型のコード実装<br>
    反復方策評価で取り扱うインデックス k に対する更新式（＝逐次近似式）<br>
    ![image](https://user-images.githubusercontent.com/25688193/50572860-8bd68900-0e0c-11e9-8cb3-ed901bb5f32e.png)<br>
    をコード実装する際に、<br>
    単純に考えると、新しい価値関数の配列 ![image](https://user-images.githubusercontent.com/25688193/50572922-ff789600-0e0c-11e9-8a64-90bf60627366.png) と元の配列 ![image](https://user-images.githubusercontent.com/25688193/50572924-2931bd00-0e0d-11e9-8af2-8c7bdfbff633.png) の２つの配列を確保し、それらを逐次更新していく実装が考えられるが、<br>
    別の実装方法として、価値関数の配列を１つだけ確保し、その時点 k での新しい価値関数 ![image](https://user-images.githubusercontent.com/25688193/50572861-9abd3b80-0e0c-11e9-875f-217476556858.png) を、古い価値関数 ![image](https://user-images.githubusercontent.com/25688193/50572914-d48e4200-0e0c-11e9-95e1-d10dec96d54f.png) に直接上書きする実装方法も考えられる。<br>
    後者の方法では、新しいデータをすぐに利用するので、２つの配列を用いる方法よりも、通常は速く収束するというメリットが存在する。<br>

以上の事項をアルゴリズムとしてまとめると、以下のようになる。<br>

![image](https://user-images.githubusercontent.com/25688193/50903178-59f2a180-1460-11e9-8d49-07e9ccad518e.png)<br>


<a id="方策改善"></a>

### ◎ 方策改善
従来の行動方策から、別の新しい行動方策に変更することで、得られる価値がより良くなるのかを？という方策改善の問題を考える。<br>
この方策改善は、<br>
① 状態 s において、従来の行動方策 π に従いつづけた際の価値である状態価値関数 ![image](https://user-images.githubusercontent.com/25688193/50580760-1a431d00-0e96-11e9-98ea-7f6cbdfe935d.png) と、<br>
② 状態 s から、新しい行動方策 π' に従って、一度だけ行動 a を選択して、その後は従来行動方策 π に従いつづけた際の価値である行動価値関数 ![image](https://user-images.githubusercontent.com/25688193/50589246-3a400400-0ec9-11e9-921c-bee22c53e1db.png)<br>
という２つの値との大小比較<br>
![image](https://user-images.githubusercontent.com/25688193/50589262-4a57e380-0ec9-11e9-9bb6-b41cfa16c026.png)<br>
から、計量的に取り扱うことが出来る。<br>

- 【参考サイト】<br>
    - [今さら聞けない強化学習（6）：反復法による最適方策](https://qiita.com/triwave33/items/59768d14da38f50fb76c)<br>


<a id="方策改善定理"></a>

#### ☆ 方策改善定理
このことを一般的に述べたものが、以下の方策改善定理と呼ばれるものになる。<br>

![image](https://user-images.githubusercontent.com/25688193/50589289-65c2ee80-0ec9-11e9-8d94-61248e01f862.png)<br>

- （証明）<br>
    状態価値関数と行動価値関数の関係より、<br>
    ![image](https://user-images.githubusercontent.com/25688193/50589355-a7ec3000-0ec9-11e9-8d93-51bcccd50235.png)<br>
    の関係が成り立つので、以下のような時間ステップ t に関しての漸化式が成り立つ。<br>
    ![image](https://user-images.githubusercontent.com/25688193/50589372-b89ca600-0ec9-11e9-9d7c-594ec9bf19b0.png)<br>
    従って、![image](https://user-images.githubusercontent.com/25688193/50589407-d4a04780-0ec9-11e9-8a07-1a9e6af85b5c.png) の関係が成り立つので、定理が成り立つことが分かる。<br>


<a id="方策改善定理とグリーディー方策"></a>

#### ☆ 方策改善定理とグリーディー方策
ここで、以下のように定義される新しい行動方策としてのグリーディー方策<br>
![image](https://user-images.githubusercontent.com/25688193/50593589-41244200-0edc-11e9-8637-4e229e91ba1b.png)<br>
は、方策改善定理にある新たな方策 π′ の条件 ![image](https://user-images.githubusercontent.com/25688193/50589481-2517a500-0eca-11e9-8bdf-5f2c9f5f1621.png) を満たし、方策改善になっている。（証明略）<br>
つまり、１ステップ間 t→t+1 で、グリーディー方策で行動を選択することで、方策が改善されていくことが保証される。<br>


<a id="方策改善のアルゴリズム"></a>

#### ☆ 方策改善のアルゴリズム
このグリーディー方策による方策改善をアルゴリズムとしてまとめると、以下のようになる。<br>

![image](https://user-images.githubusercontent.com/25688193/50593645-72047700-0edc-11e9-903e-bf9d74557915.png)<br>


<a id="方策反復法"></a>

### ◎ 方策反復法 [policy iteration]
先にみた方策評価と方策改善のプロセスを繰り返すことで、方策と価値価値関数を次々と更新し、最終的には、最適行動方策と最適価値関数に収束させるようなアルゴリズムを、方策反復法という。<br>

![image](https://user-images.githubusercontent.com/25688193/50594042-3cf92400-0ede-11e9-9dea-84772a441d19.png)<br>

- 【参考サイト】<br>
    - [強化学習について学んでみた。（その12）](http://yamaimo.hatenablog.jp/entry/2015/09/06/200000)<br>


<a id="価値反復法"></a>

### ◎ 価値反復法 [value iteration]
先に見てきた方策反復法では、そのアルゴリズム中に含まれる方策評価のプロセスにおいて、全ての状態に対してのスイープ操作による完全バックアップ処理を行うために、方策改善のプロセスの段階に移行するまで時間がかかるという欠点が存在する。<br>

そこで、価値反復法と呼ばれるアルゴリズムでは、最適解への収束性を失うことなく、１回のスイープ操作度に、方策改善を行うようにして、方策反復法よりも速く方策改善が行えるようにしている。<br>
更に、価値反復法では、状態価値関数の推定値 V(s) の更新式として<br>
![image](https://user-images.githubusercontent.com/25688193/50592761-dd4c4a00-0ed8-11e9-9cf9-34f3513d0274.png)<br>
というベルマン最適方程式に準じた形の更新式を利用する。<br>

この更新式は、先の方策反復法での、状態価値関数の更新式<br>
![image](https://user-images.githubusercontent.com/25688193/50592786-f05f1a00-0ed8-11e9-98e5-6b3573b7c975.png)<br>
と比較すると、<br>
価値反復法では、行動方策に関しての全ての分岐 ![image](https://user-images.githubusercontent.com/25688193/50592834-21d7e580-0ed9-11e9-835c-3c75ff156b05.png) の情報に従って更新を行うのではなくて、グリーディーな行動選択 ![image](https://user-images.githubusercontent.com/25688193/50593104-46808d00-0eda-11e9-8902-2451d812fa66.png) に従って、更新を行っていることになるので、<br>
分岐の総和をとる方策反復法に比べて、グリーディーで max のみを選択するために、１ステップあたりの計算量は、価値反復法に比べて減少するというメリットが存在する。<br>

以上のことをアルゴリズムとしてまとめると、以下のようになる。<br>

![image](https://user-images.githubusercontent.com/25688193/50594065-54381180-0ede-11e9-8971-59250bb2a8f9.png)<br>


- 【参考サイト】<br>
    - [強化学習について学んでみた。（その13）](http://yamaimo.hatenablog.jp/entry/2015/09/07/200000)<br>


<a id="非同期動的計画法の概要"></a>

## ◎ 非同期動的計画法の概要
この非同期動的計画法のアルゴリズムの詳細はここでは述べない。以下はその概要説明<br>

これまで見てきた各種DP手法（＝方策反復法、価値反復法）では、状態集合 S 全体に対して、スイープ操作とバックアップ処理が必要であるために、多くの実際上の強化学習タスクのように、状態の数が莫大になるようなケースにおいて、計算量とメモリが爆発的に増加してしまうという欠点が存在する。<br>

非同期動的計画法のアルゴリズムでは、この問題を解決するために、全状態をスイープ操作するのではなくて、ある状態の状態価値関数の更新に関係のある状態を任意に選別して、それらを元に状態価値関数を更新（＝バックアップ）する。<br>
このように、選択する状態を選別することで、動的計画法による最適化にあまり関係のない状態の価値関数の更新処理を省くことが出来るため、計算量やメモリ消費量を大幅に削減することが出来る。<br>


<a id="一般化方策反復（GPI）"></a>

## ◎ 一般化方策反復（GPI）
一般化方策反復（GPI）とは、これまで見てきた様々な強化学習手法でなぜ最適解が得られるのかを、共通の枠組みで解釈するための考え方・概念のことである。（※アルゴリズムではないことに注意）<br>

これまで見てきた強化学習手法では、<br>
例えば、方策反復法では、方策評価のループが完了→方策改善に移行。<br>
価値反復法では、１回の方策評価→１回の方策改善に移行。<br>
といったように、方策評価のプロセスと方策改善のプロセスにおいて、その粒度の違いは存在するものの、方策評価と方策改善の２つのプロセスの相互作用で、最適解に収束させていくアルゴリズムであった。<br>

一般化方策反復（GPI）では、このような強化学習手法に共通する、方策評価と方策改善の２つのプロセスの相互作用による最適解への収束過程を、一般的概念（共通概念）として扱う。<br>
これにより、ほとんど全ての強化学習手法は、（明確な方策と価値関数、及びそれによる方策評価、方策改善プロセスを持つために、）この一般化方策反復の枠組みで、定性的に解釈することが出来る。<br>

以下の図は、一般化方策反復の枠組みで、方策評価と方策改善のプロセスが互いに相互作用しながら、最適解に収束していく様子を図示した模式図である。<br>

![image](https://user-images.githubusercontent.com/25688193/50631021-36be8280-0f86-11e9-8873-8d3d81d36122.png)<br>

- 安定性：<br>
	一般化方策反復の枠組みでの安定性とは、方策評価と方策改善のプロセスにおいて、これ以上変化が生じず、価値関数と方策が共に最適になっていることを意味している。<br>
	ここで、価値関数が安定になるのは、現在の方策との ”整合性” が取れている場合である。<br>
    又、方策が安定になるのは、行動方策が、現在の価値関数に対してのグリーディ方策になっている場合のみである。<br>


<a id="モンテカルロ法"></a>

## ■ モンテカルロ法による価値推定、方策評価と方策改善
モンテカルロ法とは、数値計算やシミュレーションなどにおいて、ランダムな乱数をサンプリングすることで数値計算（例えば、積分計算など）を行う手法の総称であるが、今考えているマルコフ決定過程における価値関数の推定問題、方策評価と方策改善問題にも応用出来る。<br>

先に見たように、動的計画法と呼ばれる価値関数のブートストラップ性を利用して、反復的に枝分かを辿りながら価値関数を計算していく手法では、ベルマンの方程式に出てくる状態遷移関数 ![image](https://user-images.githubusercontent.com/25688193/50039765-09637080-007b-11e9-8694-e28e343c2bb7.png) が既知でなくてはならないという問題があった。<br>
これに対して、モンテカルロ法による価値関数の推定法では、ブートストラップ性を利用せず、シミュレーションの経験的に価値推定を推定するため、この状態遷移関数が既知でなくてもよいというメリットが存在する。<br>
※ 但し、状態遷移関数が既知でなくてもよいのは、状態価値関数ではなく行動価値関数を推定するアルゴリズムに限定される。（詳細は後述）<br>
※ 又、収益の分散が大きい場合、モンテカルロ法による価値関数の推定は、信頼度が低くなってしまうという問題がある。<br>

<br>

下図は、動的計画法とモンテカルロ法による手法の違いを示したバックアップ線図である。<br>
![image](https://user-images.githubusercontent.com/25688193/50652076-a81f2500-0fc8-11e9-9127-6d4268c0dc87.png)<br>

動的計画法の各種手法は、更新に寄与するノードが、後に続く全ての可能な遷移経路となるので、動的計画法の各種手法のバックアップ線図は、可能な遷移経路全てで分岐していた。<br>

一方、モンテカルロ法では、シミュレーション上で１つのエピソードが試行され、経験的に１つの経路が試行されるだけなので、モンテカルロ法のバックアップ線図は、上図の太線部分で示された１つのエピソードの開始から終端までの１つのサンプリングされた経路となる。<br>


ここで、状態価値関数は、<br>
![image](https://user-images.githubusercontent.com/25688193/50636865-059c7d00-0f9b-11e9-9c9f-5baa5479c22f.png)<br>
のように期待利得に関する期待値で定義されるものであったことを考えると、モンテカルロ法で価値推定を行う場合には、シミュレーションに従って状態を訪問した後に観測した収益を平均化すれば良いことが分かる。<br>
そうすることで、より多くの訪問回数を重ねるにつれて、平均値が期待値に収束するようになる。<br>

- 【参考サイト】<br>
    - [今さら聞けない強化学習（7）：モンテカルロ法で価値推定](https://qiita.com/triwave33/items/0c8833e6b899c26b208e#_reference-523997a713762bb0a83c)<br>
    - [強化学習について学んでみた。（その14）](http://yamaimo.hatenablog.jp/entry/2015/09/30/200000)<br>
    - [強化学習について学んでみた。（その16）](http://yamaimo.hatenablog.jp/entry/2015/10/02/200000)<br>


<a id="初回訪問モンテカルロ法"></a>

### ◎ 初回訪問モンテカルロ法による価値推定
初回訪問モンテカルロ法は、あるエピソードにおいて、状態 s への初回訪問の結果で発生した収益のみを平均値するアルゴリズムで、以下のようになる。<br>
※ このアルゴリズムは、状態価値関数の推定を行っているだけで、方策評価と方策改善は行っていないことに注意。<br>

![image](https://user-images.githubusercontent.com/25688193/50676554-4d251680-1038-11e9-8dff-94b28c17670a.png)<br>

<a id="逐一訪問モンテカルロ法"></a>

### ◎ 逐一訪問モンテカルロ法による価値推定
逐一訪問モンテカルロ法は、エピソード群全体での、状態 s への全ての訪問後に発生した収益に対して、平均化処理を行い、その平均値で状態価値関数の推定を行う。<br>
※ このアルゴリズムは、状態価値関数の推定を行っているだけで、方策評価と方策改善は行っていないことに注意。<br>


<a id="モンテカルロ-ES法"></a>

### ◎ モンテカルロ-ES法
モンテカルロ法による価値推定、方策評価が有効であるためには、以下のような事項を考慮する必要がある。<br>
そして、これらの事項を考慮したモンテカルロ法を、モンテカルロ-ES法 [ES:Exploring Starts] という。<br>

- 状態価値関数ではなく行動価値関数で方策評価：<br>
    状態遷移関数 ![image](https://user-images.githubusercontent.com/25688193/50039765-09637080-007b-11e9-8694-e28e343c2bb7.png) が与えられていて、モデルのダイナミクスが明確なときは、<br>
    ![image](https://user-images.githubusercontent.com/25688193/50676409-8610bb80-1037-11e9-8771-5815123238cb.png)<br>
    の関係式に従って、算出した状態価値関数の推定値 V(s) から、行動方策 π を算出することが出来る。<br>
	逆に、状態遷移関数 ![image](https://user-images.githubusercontent.com/25688193/50039765-09637080-007b-11e9-8694-e28e343c2bb7.png) が不明で、モデルのダイナミクスが不明なときは、この関係式に従って、状態価値関数の推定値 V(s) から、行動方策 π を算出することが出来ないので、状態価値関数ではなくて行動価値関数を推定値 Q(s,a) を算出し、そこから<br>
    ![image](https://user-images.githubusercontent.com/25688193/50676459-c7a16680-1037-11e9-9653-d35cd0b68be5.png)<br>
    の関係式に従って、行動方策を算出する必要がある。<br>
    モンテカルロ法のメリットの１つは、先に述べたようにモデルのダイナミクスが不明であっても、価値推定が行えることにあるので、このメリットを享受するためには、状態価値関数ではなく、行動価値関数で方策評価する必要がある。<br>

- 到達されない状態行動対と開始点検索の仮定：<br>
	モンテカルロ法では、シミュレーションに従って経験的にエピソードの試行を行うが、行動方策が確率１の決定論的な行動方策であった場合、或いは、ほとんど決定論的な行動方策であった場合、エピソードの試行回数を多くしても、モンテカルロ法のシミュレーション上で到達されない状態や行動が出てきてしない、結果として、平均化すべき収益がなくなるので、収益の平均値として算出される行動価値関数の推定値も正しく算出出来なくなるという問題が存在する。<br>
	この問題を解決するための１つの方法として、全ての状態とそれに続く行動セット（＝状態行動対）を、エピソードの開始点に取ることが出来るという、開始点検索の仮定をおく方法が考えられる。<br>
    これにより、無限回のエピソードの試行において、全ての状態行動対が訪問され、正しく行動価値関数の推定値が算出可能となることが保証される。<br>

- 無限個のエピソードを試行する必要があることへの対応：<br>
    モンテカルロ法による価値推定が、正しい値に収束するためには、原理的には、無限回のエピソードを試行する必要がある。<br>
	但し、（これまで見てきた動的計画法の手法と同じように）推定値の近似値を得るという目的であれば、無限回のエピソードを試行せずとも、十分回数の多いエピソードの試行回数で近似推定値を得ることが出来る。<br>
    或いは無限回のエピソードを試行することを避ける別の方法としては、方策評価が完了して方策改善を行う手順ではなくて、方策評価と方策改善を交互に逐次行っておく方法が考えられる。<br>
    （例えば、動的計画法の手法の１つである価値反復法では、１回の方策評価→１回の方策改善を繰り返すアルゴリズムになっている。）<br>
    モンテカルロ-ES法では、この方法を採用しており、１エピーソード単位で方策評価と方策改善の交互に行う。つまりは、各エピーソード完了後に、観測された収益から算出された行動価値関数の推定値で、方策評価を行い、エピーソード中に訪問された全ての状態において、方策改善が行われるという具合である。<br>

これらの事項を考慮したモンテカルロ法を、モンテカルロ-ES法といい、以下のようなアルゴリズムとなる。<br>

![image](https://user-images.githubusercontent.com/25688193/50676577-6f1e9900-1038-11e9-8434-948b21dd9773.png)<br>


<a id="方策オン型モンテカルロ法"></a>

### ◎ 方策オン型モンテカルロ法
先のモンテカルロ-ES法では、決定論的な行動方策において、モンテカルロシミュレーション上で、実際には到達しない状態行動対が存在するために、利得の平均値の計算が出来ず、結果として、収益の平均値として算出される行動価値関数の推定値も正しく算出出来なくなるという問題を回避するために、開始点検索（＝全ての状態行動対を、エピソードの開始点に設定出来る）の仮定を行っていた。<br>

しかしながら、この開始点検索の仮定は、実際上のタスクにおいて、非現実的なものであので、次に、開始点検索の仮定を除外することを考える。<br>
この方法には、大きく分けて２つの方法（方策オン型モンテカルロ法と方策オフ型モンテカルロ法）があるが、いづれもエージェントに、到達しない状態行動対を選び続けさせるというのが基本的なコンセプトになっている。<br>
※ ここでのオン・オフの意味は、モンテカルロシミュレーションでエピソードの系列（＝状態行動対）を観測する際に、方策評価、方策改善で対象となる行動方策を「使う」/「使わない」ということを意味している。<br>

まずは、方策オン型モンテカルロ法について見ていく。<br>

<br>

方策オン型モンテカルロ法では、決定論的な行動方策を全てにおいて完全に排除して、常に全ての状態 ∀s∈S と行動 ∀a∈A において ![image](https://user-images.githubusercontent.com/25688193/50682651-e9f4ad80-1052-11e9-908a-bb7b9ffff6b8.png) というソフトなもの置き換える。<br>
これにより、常に ![image](https://user-images.githubusercontent.com/25688193/50682687-009b0480-1053-11e9-8b31-4a21adcfdada.png) なので、到達されない状態行動対は存在しなくなり、開始点検索の仮定は必要なくなる。<br>
そして、上記のように方策をソフトなものに置き換えた上で、行動方策を、決定論的になる最適方策に向けて、徐々に移行させていくようにする。<br>

方策をソフトなものに置き換えるやり方は、いくつか変種があるが、ここでは、ε-greedy 手法を用いる方法を見ていく。<br>

ε-グリーディ方策では、以下の式で与えられる行動方策 ![image](https://user-images.githubusercontent.com/25688193/50687984-ca678000-1066-11e9-9091-91c9d0d9c06a.png) である。<br>
![image](https://user-images.githubusercontent.com/25688193/50721383-f1bf5b00-1101-11e9-8cc2-962005cf9c39.png)<br>

先に見た一般化方策（GPI）による共通の知見では、方策改善が行えるためには、行動方策がグリーディ方策に移行しなくてはならないが、これは、移行の過程で、該当方策がずっとグリーディ方策であることは要求しておらず、グリーディ方策に移行することだけが要求されている。<br>

今考えている方策オン型モンテカルロ法の場合では、単純にεーグリーディ方策へと移行するだけである？<br>
行動価値関数 ![image](https://user-images.githubusercontent.com/25688193/50721389-07cd1b80-1102-11e9-999e-244a455abbe9.png) に関するどのような ε-グリーディ方策も、従来のソフト方策に関して、改善された方策になっている。このことは方策改善定理によって保証されている。<br>
即ち、任意の状態 ∀s∈S 、新しい ε-グリーディー方策 π′、従来のソフト方策 π に対して、<br>

となり、方策改善定理の条件が成り立つので、新しい εーグリーディー方策 π′ と従来のソフト方策 π に対して、![image](https://user-images.githubusercontent.com/25688193/50721400-30551580-1102-11e9-9cde-8a69a8ebb475.png) となり、![image](https://user-images.githubusercontent.com/25688193/50721406-44991280-1102-11e9-90cc-9b0d9d5ffe49.png) となる。<br>
つまり、行動価値関数 ![image](https://user-images.githubusercontent.com/25688193/50721389-07cd1b80-1102-11e9-999e-244a455abbe9.png) に関するどのような ε-グリーディ方策も、従来のソフト方策に関して、改善された方策になっている。<br>
![image](https://user-images.githubusercontent.com/25688193/50721375-d6ece680-1101-11e9-8ec2-d2b888db50a8.png)<br>

<br>

以上のことを踏まえて、先のモンテカルロ-ES法から、開始点検索の仮定を取り除き、決定論的な行動方策をソフト方策に置き換ると、以下のような方策オン型モンテカルロ法のアルゴリズムが得られる。<br>

![image](https://user-images.githubusercontent.com/25688193/50721413-65f9fe80-1102-11e9-84e9-e24605c97d4e.png)<br>


<a id="方策オフ型モンテカルロ法"></a>

### ◎ 方策オフ型モンテカルロ法
開始点検索の仮定を外しても、価値推定を正しく行うことが出来る別の方法として、方策オフ型モンテカルロ法が存在する。<br>

この方策オフ型モンテカルロ法では、行動方策を、次の２つの方策 π′,π に分ける。<br>
- 挙動方策 π′ [behavior policy]：エピーソードを生成するための方策<br>
- 推定方策 π [estimation policy]：方策評価・方策改善に使用する方策<br>

このように方策を２つの方策に分けた場合は、挙動方策と推定方策とで得られる収益が異なるので、如何にしてエピソードを生成する挙動方策から、推定方策を使って方策評価や方策改善を行えばよいのか？というのが課題となる。<br>

この課題を解決するために、挙動方策と推定方策それぞれで、実際に生成されるエピソードの系列が実現される確率を比較し、それらを収益で評価（＝価値関数で評価）するときの重みとして利用することを考える。<br>

![image](https://user-images.githubusercontent.com/25688193/50761541-02accf80-12ae-11e9-8923-2b2feff21623.png)<br>

そのためにまず、上図のように、挙動方策と推定方策２つの方策 π,π′ に対して、エピソード全体に渡っての、以下のような確率を導入する。<br>

![image](https://user-images.githubusercontent.com/25688193/50761863-ce85de80-12ae-11e9-9e55-1ecf4582929e.png)<br>

今のモンテカルロ法の場合、この２つの確率 ![image](https://user-images.githubusercontent.com/25688193/50761938-fa08c900-12ae-11e9-990b-4112da28c31d.png) は、状態遷移関数 ![image](https://user-images.githubusercontent.com/25688193/50761964-09881200-12af-11e9-9699-9d60674fa353.png) が不明なので、未知の変数であるが、その確率の比 ![image](https://user-images.githubusercontent.com/25688193/50761984-160c6a80-12af-11e9-82a8-71370818403d.png) をとると、<br>
![image](https://user-images.githubusercontent.com/25688193/50762126-7a2f2e80-12af-11e9-8898-184fd855559b.png)<br>
となるので、状態遷移確率によらないので、モデルのダイナミクスが不明であっても、２つの行動方策（推定方策、挙動方策）から計算可能な値となる。<br>

次に、挙動方策 π′ で収益 R が観測されたときに、推定方策 π でも収益 R が観測される確率を考えると、この確率は、確率の比 ![image](https://user-images.githubusercontent.com/25688193/50761984-160c6a80-12af-11e9-82a8-71370818403d.png) で表現されるので、推定方策 π のもとでの収益の合計 ![image](https://user-images.githubusercontent.com/25688193/50763504-46ee9e80-12b3-11e9-9ec7-92a6a48e4c4d.png) と行動価値関数の推定値 Q(s,a) は、以下のようなアルゴリズムで求まることが分かる。<br>
![image](https://user-images.githubusercontent.com/25688193/50763520-55d55100-12b3-11e9-8c71-860362ebeda6.png)<br>

ここまでの事項をアルゴリズムとしてまとめると、以下のような方策オフ型モンテカルロ法のアルゴリズムが得られる。<br>

![image](https://user-images.githubusercontent.com/25688193/50764509-2d028b00-12b6-11e9-82ca-0e662508170c.png)<br>


<a id="TD学習（時間的差分学習）"></a>

## ■ TD学習（時間的差分学習）
先のモンテカルロ法では、ブーストラップ性を用いないため、計算量が大きく、又、収益の分散が大きい場合に、モンテカルロ法による価値関数の推定は、信頼度が低くなってしまうという問題が存在する。<br>
一方、動的計画法では、状態遷移確率の形が具体的に与えられていないと、計算が出来ないとという問題が存在する。<br>

この両者（MC法、DP法）の利点を織り込んだ最適価値関数の推定手法として、TD学習と呼ばれる手法が存在する。<br>
具体的には、このTD学習では、状態遷移確率の具体的な形を必要せず、なおかつ、ブートストラップ性を用いて、確率過程の終端まで待たずに、価値関数の更新をオンライ的に逐次行うことが出来る。<br>

まず、オンライン型の推定値の計算の一般的な議論の例として、平均値の計算の取扱いについて見てみる。
報酬 ![image](https://user-images.githubusercontent.com/25688193/50372449-d177a280-0611-11e9-8721-b0385816271b.png) の平均値計算は、以下のようにして計算できる。<br>
![image](https://user-images.githubusercontent.com/25688193/50372454-e0f6eb80-0611-11e9-8b73-1f8778b0e417.png)<br>
この平均値計算式の形式では、新しいデータが追加されるたびに、和の操作を再度実行する必要があるために、計算負荷が大きくなりやすいという問題が存在する。即ち、<br>
![image](https://user-images.githubusercontent.com/25688193/50372466-0d126c80-0612-11e9-86a9-e56599079ada.png)<br>
従って、この式を、前回の平均値と最新のデータから、新しい平均値を算出するという漸近式の表現に書き直すと、<br>
![image](https://user-images.githubusercontent.com/25688193/50372472-2c10fe80-0612-11e9-995c-dec39cf37e0f.png)<br>
即ち、平均値の計算を、漸近式の形で表現すると、以下のような形式となる。<br>

- 新しい推定値 ← 古い推定値＋ステップサイズ ✕ [最新の更新データ ー 古い推定値]<br>

この漸近式の表現では、新しいデータに対して、都度平均値計算をし直す必要はなく、オンライン型の手続きが可能となり、計算負荷が軽減されるといったメリットが存在する。<br>

<br>

次に、この漸近式の表現での価値関数の推定方法を、先の逐次訪問モンテカルロ法による価値推定の方法に適用してみることを考える。<br>
先のモンテカルロ法による価値推定の方法では、各状態 s に対する得られた収益のリスト Returns(s) を平均化したもので価値関数の推定を行っていたが、これを漸近式の形で表現すると、<br>

![image](https://user-images.githubusercontent.com/25688193/50373326-b8c2b900-0620-11e9-8263-32ab0a598a00.png)<br>

この収益 ![image](https://user-images.githubusercontent.com/25688193/50373274-aeec8600-061f-11e9-984b-a741f664ff03.png) を目標値とする漸近式の形で表現した、逐次訪問モンテカルロ法による価値推定手法を、アルファ不変MC法という。<br>
尚、このアルファ不変MC法は、漸近式の形で表現されているが、報酬の平均値である収益 ![image](https://user-images.githubusercontent.com/25688193/50373274-aeec8600-061f-11e9-984b-a741f664ff03.png) がエピソードの完了までまたないと計算できないため、漸近式のメリットであるオンライン型の手続きのメリットは享受できない。<br>

<br>

TD学習では、エピソードの完了までまたないと計算できない収益 ![image](https://user-images.githubusercontent.com/25688193/50373274-aeec8600-061f-11e9-984b-a741f664ff03.png) ではなく、次のステップでの価値関数と報酬の推定値 ![image](https://user-images.githubusercontent.com/25688193/50373355-3be40f00-0621-11e9-8fb4-fa66eb1cf1f2.png) を使用する。<br>
即ち、![image](https://user-images.githubusercontent.com/25688193/50810918-63401900-134f-11e9-8c32-4b632e0d326f.png) の１ステップでの関係式を、先の不変MC法の式 ![image](https://user-images.githubusercontent.com/25688193/50373474-f45e8280-0622-11e9-8298-7d58f29bc1bd.png) の収益 ![image](https://user-images.githubusercontent.com/25688193/50373274-aeec8600-061f-11e9-984b-a741f664ff03.png) の項に入力した式<br>

![image](https://user-images.githubusercontent.com/25688193/50810484-38ed5c00-134d-11e9-8b26-c566551e2c2e.png)<br>
により、価値関数の推定値の更新を行う。<br>
このような直近の価値関数の推定値を用いる最も単純なTD法を、TD(0)法と呼ぶ。<br>
このTD(0)法では、不変MC法と比較すると、![image](https://user-images.githubusercontent.com/25688193/50373489-32f43d00-0623-11e9-9219-a98a40f764a4.png) を目標値として更新を行い、又、不変MC法とは異なり、エピソードの完了まで待たなくとも、次のステップ t+1 まで待つだけでよい。<br>
※ この TD(0)法では、１ステップでのバックアップ処理の式になっているので、１ステップTD法であるとも言える（詳細は後述）<br>

![image](https://user-images.githubusercontent.com/25688193/50902697-ed2ad780-145e-11e9-9645-1ed40eba4b4a.png)<br>

> 【Memo】<br>
> - TD法の収束性<br>
>   TD法のアルゴリズムでは、如何なる定常方策 π に対しても、ステップサイズ α が十分小さな値定数であれば、推定値 V(s) が価値関数 ![image](https://user-images.githubusercontent.com/25688193/50376139-6a2b1400-064c-11e9-8438-31d254a79957.png) へ収束されることが保証されている。<br>
>	但し、この収束性の保証は、ほとんどの場合、テーブル形式のTD法（＝テーブルTD(0)法）で適用されるものである。<br>
> - TD法とMC法の収束速度<br>
>   TD法とMC法は、両方とも正しい価値関数の値に収束するが、どちらがより速く収束するのか？という疑問に対する数学的な証明は、現時点では未解決である。<br>
>	しかしながら、例えば、ランダムウォークのような確率的なタスクにおいては、TD法のほうが、アルファ不変MCより速く収束することが知られている。（詳細略）<br>

- 【参考サイト】<br>
    - [今さら聞けない強化学習（9）: TD法の導出](https://qiita.com/triwave33/items/277210c7be4e47c28565)<br>
    - [強化学習について学んでみた。（その18）](http://yamaimo.hatenablog.jp/entry/2015/10/15/200000)



<a id="Sarsa"></a>

### ◎ Sarsa（方策オン型TD制御）
ここでは、TD法を制御問題に適用する方法について見ていく。<br>
TD法を制御問題に適用した手法には、大きく分けて、方策オン型TD制御手法と方策オフ型TD制御手法の２つが存在する。<br>
Sarsa は、このうち方策オン型TD制御手法に分類される手法の１つであり、先のTD法と ε-greedy 手法を組み合わせた手法である。<br>
より具体的には、行動価値関数の推定をTD法によって行い、エピソードの更新（＝行動選択）を ε-greedy な行動方策によって行う手法である。<br>

<br>

まず、行動選択に関する ε-greedy 手法について見ていく。<br>
行動価値関数 Q(s,a) に関してグリーディな行動方策とは、![image](https://user-images.githubusercontent.com/25688193/50381321-74d6bf00-06c7-11e9-9cc1-638686addf68.png) となるような行動 a のことであったが、このグリーディーな行動方策では、常にその時点での最適解を模索するため、局所的最適解に陥る恐れが存在する。<br>
そこで、局所的最適解を脱出して大域的最適解に移動できるように、リスクをとって、一定の確率 ε でランダムな行動をとるよな行動方策を考える。この行動方策を ε-greedy 手法という。<br>

<br>

Sarsa では、行動価値関数の推定をTD法によって行うが、TD法による行動価値関数の更新と推定は、以下のような漸近式で表現されるのであった。<br>
![image](https://user-images.githubusercontent.com/25688193/50381318-583a8700-06c7-11e9-854f-aedaef25bb78.png)<br>

記号の簡単化のため、t の添字なしで書き換えると、以下のようになる。<br>
![image](https://user-images.githubusercontent.com/25688193/50888440-002ab100-1439-11e9-87f3-6caebf530664.png)<br>

Sarsa では、この更新式により、推定する行動価値関数の更新を行う。<br>
※ Sarsa というアルゴリズムの名前は、上式の推定式の右辺の変数 s,a,r′,s′,a′ の頭文字を繋げたものに由来する。<br>
そして、エピソードの更新（＝行動選択）は、ε-greedy 手法に従った行動方策によって行う。<br>

以下の図は、Sarsa のバックアップ線図である。<br>

![image](https://user-images.githubusercontent.com/25688193/50774861-f2a9e580-12d7-11e9-9ae6-5c950e527c85.png)<br>

上図から分かるように、この Sarsa は、ε-greedy な行動方策によってエピソードの更新（＝行動選択）が行われ、それに伴って次の状態 s',a' が定まり、価値関数の更新が行われることになる。<br>
即ち、Sarsa では、実際に進む行動（＝エピソードを更新する行動）と価値関数の推定値の更新に用いる行動が同様となる。<br>
そして、このような制御手法を、方策オン型TD制御という。<br>

この Sarsa をアルゴリズムとしてまとめると、以下のようになる。<br>

![image](https://user-images.githubusercontent.com/25688193/50902640-c79dce00-145e-11e9-9322-369de05a1988.png)<br>
![image](https://user-images.githubusercontent.com/25688193/50774897-0ead8700-12d8-11e9-8722-05e0b808ab1e.png)<br>


<a id ="Sarsaの適用例"></a>

#### ☆ Sarsa の適用例

- （例）Sarsa を利用した単純な迷路探索問題<br>
    > 記載中...

    尚、本適用例の実装コードは、以下のサイトに保管してあります。<br>
    [GitHub/Yagami360/ReinforcementLearning_Exercises/MazeSimple_Sarsa/](https://github.com/Yagami360/ReinforcementLearning_Exercises/tree/master/MazeSimple_Sarsa)


<a id="Q学習"></a>

### ◎ Q学習 [Q-learning]（方策オフ型TD制御）
先に見た Sarsa は、エピソードの更新（＝行動選択）と価値関数の更新の双方が、共に同じ ε-greedy な行動方策に従って行われる方策オン型TD制御アルゴリズムであった。<br>

一方、Q学習は、エピソードの更新（＝行動選択）と価値関数の更新の双方が、必ずしも同じ行動方策になるとは限らない方策オン型TD制御アルゴリズムになっている。<br>

まず、Q学習における、価値関数の推定値の更新式は、以下のようになる。<br>
![image](https://user-images.githubusercontent.com/25688193/50888533-3c5e1180-1439-11e9-88cb-acec148af417.png)<br>
この式から分かるように、Sarsa とは異なり、Q学習では、価値関数の値が max となるようなグリーディーな行動選択を一律に行う。<br>
（※ Sarsa では、ε-greedy な行動方策に従って、一定のランダム性を織り込んだ行動選択であった。）<br>
一方、エピソードの更新（＝行動選択）は、Sarsa と同様にして、ε-greedy 手法に従った行動方策によって行う。<br>

以下の図は、Q学習のバックアップ線図である。<br>

![image](https://user-images.githubusercontent.com/25688193/50775177-ccd11080-12d8-11e9-8eeb-3309a6ba22e5.png)<br>

上図から分かるように、（推定値の更新式に含まれる項である）![image](https://user-images.githubusercontent.com/25688193/50384493-afad1700-0708-11e9-9600-941f4df96415.png) によるグリーディーな行動選択と、ε-greedy な行動方策によるエピソードの更新（＝行動選択）とは、必ずしも一致するとは限らない。<br>
即ち、Q学習では、実際に進む行動（＝エピソードを更新する行動）と価値関数の推定値の更新に用いる行動が異なる。<br>
そして、このような制御手法を、方策オフ型TD制御という。<br>

このQ学習をアルゴリズムとしてまとめると、以下のようになる。<br>

![image](https://user-images.githubusercontent.com/25688193/50902587-a50bb500-145e-11e9-8801-ae3ef35386cf.png)<br>
![image](https://user-images.githubusercontent.com/25688193/50775278-0e61bb80-12d9-11e9-9c6d-e159d9bbe0ce.png)<br>

> 【Memo】<br>
> - SarsaとQ学習のの収束性の比較<br>
>   行動選択に関しては、SarsaとQ学習は共に ε-greedy によるランダム性をもった行動選択となるが、価値関数の推定値の更新に関しては、Sarsa が ε-greedy ランダム性が更新式に入る一方で、Q学習では、![image](https://user-images.githubusercontent.com/25688193/50384493-afad1700-0708-11e9-9600-941f4df96415.png) のようにランダム性を持たない一律のグリーディーな行動選択となる。<br>
>   従って、ランダム性がない分、Q学習のほうが Sarsa より収束が早い傾向がある。<br>


- 【参考サイト】<br>
    - [今さら聞けない強化学習（10）: SarsaとQ学習の違い](https://qiita.com/triwave33/items/cae48e492769852aa9f1#_reference-745dd3cb44ecaa9c9752)<br>
    - [強化学習について学んでみた。（その19）](http://yamaimo.hatenablog.jp/entry/2015/10/16/200000)


<a id ="Q学習の適用例"></a>

#### ☆ Q学習の適用例

- （例）Q学習を利用した単純な迷路探索問題<br>
    > 記載中...

    尚、本適用例の実装コードは、以下のサイトに保管してあります。<br>
    [GitHub/Yagami360/ReinforcementLearning_Exercises/MazeSimple_Qlearning/](https://github.com/Yagami360/ReinforcementLearning_Exercises/tree/master/MazeSimple_Qlearning)


<a id="nステップTD法"></a>

### ◎ nステップTD法
先のTD(0)では、価値関数の更新式（＝漸化式）![image](https://user-images.githubusercontent.com/25688193/50813583-39d8ba80-135a-11e9-8b28-e7975febabb1.png) の式での収益 R おいて、１ステップでの関係式 ![image](https://user-images.githubusercontent.com/25688193/50811225-c2eaf400-1350-11e9-8a6d-e27bf744e286.png) を代入した式<br>
![image](https://user-images.githubusercontent.com/25688193/50813606-4bba5d80-135a-11e9-8ebd-52ee8bfea10f.png)<br>
つまりは、１ステップバックアップで価値関数の推定を行っていた。<br>
一方、モンテカルロ法では、エピソードの開始から終端までの全ての系列に基づいてバックアップを行っていた。<br>

nステップTD法は、この１ステップバックアップであるTD(0)法と、全ステップバックアップであるモンテカルロ法の中間のバックアップであり、n ステップバックアップで価値関数の推定を行う（下図参照）<br>
※ 尚、先の TD(0) 学習は、このnステップTD法の範疇にあり、n=1 のときの手法となる。<br>

![image](https://user-images.githubusercontent.com/25688193/50812093-aa7cd880-1354-11e9-8827-c8da138ef3e1.png)<br>

式で書くと、TD学習における、価値関数の更新式（＝漸化式）<br>
![image](https://user-images.githubusercontent.com/25688193/50813583-39d8ba80-135a-11e9-8b28-e7975febabb1.png)<br>
の収益 R おいて、TD(0) 法では、１ステップでの関係式 ![image](https://user-images.githubusercontent.com/25688193/50811225-c2eaf400-1350-11e9-8a6d-e27bf744e286.png) を代入していたのに対して、
nステップTD法では、nステップ先での関係式<br>
![image](https://user-images.githubusercontent.com/25688193/50813756-c1bec480-135a-11e9-9775-a6b29d5feaff.png)<br>
に拡張して代入する。即ち、価値関数の推定値の更新式は、<br>
![image](https://user-images.githubusercontent.com/25688193/50813785-dbf8a280-135a-11e9-8646-88d414c25763.png)<br>
となる。<br>


<a id="TD(λ)法"></a>

### ◎ TD(λ)法
> 記載中...


<a id="TD(λ)の前方観測的な見方"></a>

#### ☆ TD(λ)の前方観測的な見方
一般的に、TD法におけるバックアップ<br>
![image](https://user-images.githubusercontent.com/25688193/50830843-60174e00-138c-11e9-9e0e-f3c0922a6190.png)<br>
の収益項（＝目標値）![image](https://user-images.githubusercontent.com/25688193/50831087-24c94f00-138d-11e9-8ddf-cd7c069e543e.png)は、nステップ収益に対してのみだけではなく、nステップ収益の平均値に対しても適用でき、この平均化された収益で、上式に従って価値推定を行うことが出来る。<br>

TD(λ) アルゴリズムは、このような、nステップバックアップを平均化した収益で価値推定を行うアルゴリズムである。<br>
詳細には、各 n ステップ収益 ![image](https://user-images.githubusercontent.com/25688193/50831112-3c083c80-138d-11e9-95dd-d83354dc87c7.png) を ![image](https://user-images.githubusercontent.com/25688193/50831167-61954600-138d-11e9-80b2-bdcd55e62a2e.png) で重み付け平均化した収益<br>
![image](https://user-images.githubusercontent.com/25688193/50832622-1d587480-1392-11e9-88a9-742f039a20f7.png)<br>
を収益として利用して、価値推定を行う。<br>
※ この式において、λ=0 とすると、![image](https://user-images.githubusercontent.com/25688193/50833402-64476980-1394-11e9-81d1-60fe6f168bd0.png) となるので１ステップ収益となり、TD(0) に帰着する。<br>
※ 又、λ=1 とすると、![image](https://user-images.githubusercontent.com/25688193/50833442-7d501a80-1394-11e9-9a5c-797444fa146c.png) となり、アルファ不変MCに帰着する。<br>

この重み付けの様子を、バックアップ線図とともに図示したものが以下の図である。<br>
![image](https://user-images.githubusercontent.com/25688193/50832750-7b855780-1392-11e9-8dbe-dea119e73f94.png)<br>

ここで、この重み付け平均化した利得 ![image](https://user-images.githubusercontent.com/25688193/50874105-0a7f8780-1406-11e9-9290-b6f8e7a54df5.png) の計算は、現在の時間 t から将来の時間 t+1,t+2,... に渡っての重み付け平均化処理であり、時間の経過方向 t→t+1→t+2→... に沿って観測しているので、前方観測的見方となっていることが分かる。<br>


<a id="TD(λ)の後方観測的な見方と適格度トレース"></a>

#### ☆ TD(λ)の後方観測的な見方と適格度トレース
先に見たTD(λ)の前方観測的見方では、<br>
![image](https://user-images.githubusercontent.com/25688193/50875424-a1027780-140b-11e9-8213-63ac5ad072c8.png)<br>
の式に従って、重み付け平均化された収益 ![image](https://user-images.githubusercontent.com/25688193/50874105-0a7f8780-1406-11e9-9290-b6f8e7a54df5.png) を算出したが、これは、現在の時間 t から将来の時間 t+1,t+2,... に渡っての重み付け平均化処理であり、時間の経過方向 t→t+1→t+2→... に沿って観測しているので、前方観測的見方となっている。<br>

しかしながら、前方観測的見方の方法では、時間が n ステップ経過後でないと、実際に利益 R_t^((n) )  が計算できないので、価値推定を行えないという問題が存在する。<br>

この問題を解決するために、前方観測的見方を反転し、時間の経過方向とは逆の方向からみた後方観測的見方からTD(λ)を考え直す。<br>
より詳細には、今考えているTD(λ)において、後方の時間ステップ t+n から見た、TD誤差（＝現在の推定価値との差分）![image](https://user-images.githubusercontent.com/25688193/50882880-9ce55280-1429-11e9-969d-118b3ba459ac.png) が、時間ステップ t での価値推定 ![image](https://user-images.githubusercontent.com/25688193/50879411-20994200-141e-11e9-9127-d45200ac8651.png) にどの程度（の重みで）影響を与えているのか？ということを考える。<br>

このことを示したのが以下の図である。<br>
> 図を要追加...

図から分かるように、TD(λ)において、各時間ステップ t+1,t+2,... でのTD誤差は、nステップ前の価値の推定に、![image](https://user-images.githubusercontent.com/25688193/50879475-67873780-141e-11e9-9d02-cc334a893ac1.png) の重みで影響を与えているということが分かる。<br>

従って、![image](https://user-images.githubusercontent.com/25688193/50879475-67873780-141e-11e9-9d02-cc334a893ac1.png) に比例した重みで影響を与えるような以下のような適格度トーレスなるものを考える。<br>

![image](https://user-images.githubusercontent.com/25688193/50879553-a4532e80-141e-11e9-810e-f751702b9728.png)<br>

- この適格度トーレスは、<br>
    「時間ステップ t において、次の状態 ![image](https://user-images.githubusercontent.com/25688193/50879575-b765fe80-141e-11e9-9392-32410b2404ff.png) を観測した際に、ある状態 s∈S の価値 V(s) に反映させるための、現在状態との価値の差分 ![image](https://user-images.githubusercontent.com/25688193/50879374-0b241800-141e-11e9-8b4d-6c565c68f6eb.png) に対しての重み」<br>
    を表しており、全ての状態 ∀s∈S に対して定義される。<br>
- より端的・包括的に言い換えると、<br>
    「強化事象が発生したときに、各状態が学習上の変化を受けることが、”適格” であることの度合い」<br>
    を表しており、その意味で、適格度トレースという。<br>

この適格度トレースは、価値関数に反映させるための、TD誤差 ![image](https://user-images.githubusercontent.com/25688193/50882880-9ce55280-1429-11e9-969d-118b3ba459ac.png) に対しての重みになっているので、この２つ（＝TD誤差と適格度トレース）を乗算した式から、TD(λ)の価値推定（＝価値関数の更新）を行う。<br>
即ち、<br>
![image](https://user-images.githubusercontent.com/25688193/50883038-25fc8980-142a-11e9-860a-dac9da3f8917.png)<br>

<br>

- 【参考サイト】<br>
    - [強化学習について学んでみた。（その25）](http://yamaimo.hatenablog.jp/entry/2015/12/12/200000)<br>


<!--
<a id="前方観測的見方と後方観測的見方の等価性"></a>

#### ☆ 前方観測的見方と後方観測的見方の等価性
> 記載中...
-->

<a id="Sarsa(λ)"></a>

#### ☆ Sarsa(λ)
Sarsa に対して、TD(λ) と同様にして、適格度トレースを導入したものを Sarsa(λ) という。<br>
これは、単純に、Sarsaにおける価値推定の更新式<br>
![image](https://user-images.githubusercontent.com/25688193/50887888-aa093e00-1437-11e9-8423-c35c3b0af521.png)<br>
を、適格度トレースを使用した以下のような価値推定の更新式に置き換えるものである。<br>
![image](https://user-images.githubusercontent.com/25688193/50887916-bbeae100-1437-11e9-981f-9f2057fa83d1.png)<br>

この Sarsa(λ) をアルゴリズムとしてまとめると、以下のようになる。<br>

![image](https://user-images.githubusercontent.com/25688193/50902549-80afd880-145e-11e9-8c2f-2c5993cf3449.png)<br>


<a id="Q(λ)"></a>

#### ☆ Q(λ)
Q学習に対しても、適格度トレースの考えを適用することを考える。<br>
このような手法は、Q(λ) と呼ばれ、”Watkins のQ(λ)” と ”Peng のQ(λ)” の２つの手法が存在する。<br>

ここで、Q学習は、（推定値の更新式に含まれる項である）![image](https://user-images.githubusercontent.com/25688193/50902154-7b05c300-145d-11e9-856d-59d61fbe1354.png) によるグリーディーな行動選択と、ε-greedy な行動方策によるエピソードの更新（＝行動選択）とが、必ずしも一致するとは限らない方策オフ型TD制御であった。<br>
そのため、方策オン型TD制御である Sarsa(λ) で単純に適格度トレースを導入出来たのとは異なり、Q学習に適格度トレースを導入する際には、特別な対応を考える必要がある。<br>

ここで、このQ学習に適格度トレースを導入する際の特別な対応というのは、具体的には以下のような事項となる。<br>

- Q(λ)におけるバックアップ処理のステップ数：<br>
    Q(λ)におけるバックアップ処理では、価値関数の更新でのグリーディ行動と、εーgreedy な行動選択とが一致しない（＝非グリーディ行動が選択）場合は、その時点で終了させる。<br>
    最初の非グリーディ行動を nステップ目の a_(t+n)  として、式で表現すると、<br>
    ![image](https://user-images.githubusercontent.com/25688193/50906295-23b92000-1468-11e9-832e-c1f42d163d35.png)<br>
    一方、常にグリーディ行動が選択され続けた場合は、エピソードの終端までバックアップ処理する。<br>
    エピソードの終端を t=T として、式で表現すると、<br>
    ![image](https://user-images.githubusercontent.com/25688193/50906336-35022c80-1468-11e9-9d3b-4ae5466e1591.png)<br>

> 記載中...


<a id="関数による価値関数の近似（関数近似手法）"></a>

## ■ 関数による価値関数の近似（関数近似手法）
これまで見てきた各種強化学習アルゴリズムでは、いずれも、離散的な状態空間 ![image](https://user-images.githubusercontent.com/25688193/51009809-fa49e280-1594-11e9-8758-79028000c2a3.png) 、及び、離散的な行動空間 ![image](https://user-images.githubusercontent.com/25688193/51009845-1baace80-1595-11e9-9729-8fd26deba066.png) に対して、<br>
状態価値関数 V(s) の各要素<br>
![image](https://user-images.githubusercontent.com/25688193/51009871-32e9bc00-1595-11e9-83d5-9b413b617683.png)<br>
或いは、行動価値関数 Q(s,a)  の各要素<br>
![image](https://user-images.githubusercontent.com/25688193/51009888-4137d800-1595-11e9-956e-936ed24aa20c.png)<br>
の推定を行っていた。<br>

※ 下図は、状態価値関数 V(s) の離散的な各状態 s∈S に対しての、各要素の推定値の模式図。行動価値関数 Q(s,a) では、離散的な各状態行動対 (s,a)∈S×A に対しての３次元プロットになる。<br>

![image](https://user-images.githubusercontent.com/25688193/51009936-6a586880-1595-11e9-848a-f3a7bf4feeeb.png)<br>

そして、このような処理を実現するために、これらの離散的な状態空間、離散的な行動空間に対しての、価値関数の各要素の推定値<br>
![image](https://user-images.githubusercontent.com/25688193/51009998-9e338e00-1595-11e9-945c-eaedcddb123e.png)<br>
をそのままメモリ上に保管し、価値関数の推定値の更新処理を行っていた。<br>
このような方式を、テーブル法という。<br>

しかしながら、このテーブル法では、多くの実際上の強化学習のタスクにあるように、状態数や行動数が膨大な数存在する場合、或いは、状態や行動が離散的ではなく連続である場合において、膨大なメモリを消費しい、又計算コストも膨大になるという問題が存在する。<br>


> 記載中...


---

<a id="参考文献"></a>

## ■ 参考文献

- 強化学習
    - [amazonで詳細を見る](https://www.amazon.co.jp/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-Richard-S-Sutton/dp/4627826613?SubscriptionId=AKIAJMYP6SDQFK6N4QZA&amp&tag=cloudstudy09-22&amp&linkCode=xm2&amp&camp=2025&amp&creative=165953&amp&creativeASIN=4627826613)

- 速習 強化学習 ―基礎理論とアルゴリズム―
    - [amazonで詳細を見る](https://www.amazon.co.jp/%E9%80%9F%E7%BF%92-%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-%E2%80%95%E5%9F%BA%E7%A4%8E%E7%90%86%E8%AB%96%E3%81%A8%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0%ク%80%95-Csaba-Szepesvari/dp/4320124227)

- つくりながら学ぶ! 深層強化学習 ~PyTorchによる実践プログラミング<br>
    - [amazonで詳細を見る](https://www.amazon.co.jp/dp/4839965625/ref=asc_df_48399656252542427/?tag=jpgo-22&creative=9303&creativeASIN=4839965625&linkCode=df0&hvadid=295723231663&hvpos=1o2&hvnetw=g&hvrand=4934601497265078375&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=1009648&hvtargid=pla-527403650097&th=1&psc=1)<br>


<a id="使用コード"></a>

### ◎ 使用コード

- [GitHub/Yagami360/ReinforcementLearning_Exercises](https://github.com/Yagami360/ReinforcementLearning_Exercises)<br>