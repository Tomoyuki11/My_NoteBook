# 強化学習 [Reinforcement Learning]
機械学習の一種である強化学習について勉強したことをまとめたノート（忘備録）です。随時追記中。<br>


## 目次 [Contents]

1. [概要](#概要)
1. [【補足（外部リンク）】確率過程とマルコフ連鎖](http://yagami12.hatenablog.com/entry/2017/09/17/110406#ID_3)
1. [マルコフ決定過程（MDP）](#マルコフ決定過程（MDP）)
    1. [マルコフ決定過程による強化学習のモデル化](#マルコフ決定過程による強化学習のモデル化)
    1. マルコフ決定過程の例
    1. [価値関数](#価値関数)
        1. [状態価値関数](#状態価値関数)
        1. [行動価値関数](#行動価値関数)
        1. [状態価値関数と行動価値関数の関係](#状態価値関数と行動価値関数の関係)
    1. [ベルマン方程式](#ベルマン方程式)
    1. マルコフ決定過程を解くための動的最適化法
1. 補足事項
    1. [【外部リンク】機械学習](http://yagami12.hatenablog.com/entry/2017/09/17/111400)
    1. [【外部リンク】最適化問題](http://yagami12.hatenablog.com/entry/2017/09/17/101739)
    1. [【外部リンク】グラフ理論](http://yagami12.hatenablog.com/entry/2017/09/17/110406)
1. [参考文献](#参考文献)
    1. [使用コード](#使用コード)

---

<a id="概要"></a>

## ■ 概要
> 記載中...

---

<a id="マルコフ決定過程（MDP）"></a>

## ■ マルコフ決定過程（MDP）
強化学習は、概要でみたように、意思決定主体（＝エージェント）と外部環境が相互作用するシステムにおいて、教師なし学習の状況の下、試行錯誤を通じて、目標を示す数値（＝報酬）を最大化するようにシステムを学習制御する枠組みであるが、これをマルコフ決定過程 [MDP:Markov decision process] でモデル化する。<br>

このマルコフ決定過程は、マルコフ性とエルゴード性を持つ確率過程であって、選択可能な行動、及び、報酬を追加したものである。<br>
言い換えると、マルコフ決定過程は、マルコフ連鎖に対して、選択可能な行動、及び、報酬を追加したものである。<br>

- マルコフ性：<br>
    ある状態 ![image](https://user-images.githubusercontent.com/25688193/49641452-85463300-fa53-11e8-9596-7e8b4b082bce.png) から、別の状態 ![image](https://user-images.githubusercontent.com/25688193/49641500-a575f200-fa53-11e8-8467-eaaf3a8e866f.png) に移行する確率が、それ以前の経路 t によらず、現在の状態のみ ![image](https://user-images.githubusercontent.com/25688193/49641452-85463300-fa53-11e8-9596-7e8b4b082bce.png) で決まる性質。マルコフ連鎖は、このマルコフ性を性質を持つ。<br>

- エルゴード性（定常性）：<br>
    任意の初期状態 ![image](https://user-images.githubusercontent.com/25688193/49641553-d5bd9080-fa53-11e8-9c45-e3771976401b.png) からスタートした確率過程が、無限時間経過後には、最初の状態とは無関係になる性質。マルコフ連鎖は、このエルゴード性の性質も合わせ持つ。<br>

<br>

![image](https://user-images.githubusercontent.com/25688193/49641972-208bd800-fa55-11e8-9bce-c325cbe3b59f.png)<br>
![image](https://user-images.githubusercontent.com/25688193/49697862-cb3ffa00-fbff-11e8-9cab-5a50d0a77470.png)<br>
![image](https://user-images.githubusercontent.com/25688193/49697644-62578280-fbfd-11e8-82b7-2c931ca73fcc.png)<br>

> 【Memo】<br>
> 「速習 強化学習」の本に書いてあるマルコフ決定過程の定義、状態集合、行動集合、遷移確率カーネルの３つの構造の組から定義されてるのに対し、他の文献では、状態集合、行動集合、遷移確率、報酬関数４つの組から定義されてものが多いが、３つの構造の１つの遷移確率カーネルが、状態遷移と報酬関数を与えるから、報酬関数を新たに３つの組とは別に追加しなくても well-defined となる。<br>

- 【参考サイト】<br>
    - [マルコフ決定過程 - Wikipedia](https://ja.wikipedia.org/wiki/%E3%83%9E%E3%83%AB%E3%82%B3%E3%83%95%E6%B1%BA%E5%AE%9A%E9%81%8E%E7%A8%8B)<br>
    - [第３章： 強化学習の基礎理論](http://sysplan.nams.kyushu-u.ac.jp/gen/edu/RL_intro.html#Foundations)<br>


<a id="マルコフ決定過程による強化学習のモデル化"></a>

### ◎ マルコフ決定過程による強化学習のモデル化
次に、このように定義したマルコフ決定過程において、システム（環境）とエージェントは、以下のような相互作用を行うものとして、強化学習をモデル化する。<br>

- （状態と報酬の遷移）<br>
    時間 t における状態 ![image](https://user-images.githubusercontent.com/25688193/49642152-94c67b80-fa55-11e8-9df4-e1277abc3fdf.png) にあるシステムにおいて、エージェントの行動 ![image](https://user-images.githubusercontent.com/25688193/49642191-b0ca1d00-fa55-11e8-9110-0b7f8f4f82b7.png) が実行されると、システムはそれに応じた状態遷移を行うが、このとき、以下のような関係が成り立つものとする。<br>
    ![image](https://user-images.githubusercontent.com/25688193/49642225-cb03fb00-fa55-11e8-925a-3bc9bb8becbe.png)<br>
	即ち、次の状態と報酬の組 ![image](https://user-images.githubusercontent.com/25688193/49642251-da834400-fa55-11e8-8f56-1933d89913cb.png) は、状態と報酬をセットにした遷移確率である遷移確率カーネル ![image](https://user-images.githubusercontent.com/25688193/49642273-ea9b2380-fa55-11e8-86f9-0ce942fdae87.png) の確率分布に従う。<br>
    言い換えると、遷移確率カーネル ![image](https://user-images.githubusercontent.com/25688193/49642273-ea9b2380-fa55-11e8-86f9-0ce942fdae87.png) の確率に従って、次の状態 ![image](https://user-images.githubusercontent.com/25688193/49642302-ff77b700-fa55-11e8-9570-8096cb583d34.png) と報酬 ![image](https://user-images.githubusercontent.com/25688193/49642320-0dc5d300-fa56-11e8-8399-73c5b4c44ed4.png) が定まる。<br>

- （定常性）<br>
    遷移後の次の状態 ![image](https://user-images.githubusercontent.com/25688193/49642302-ff77b700-fa55-11e8-9570-8096cb583d34.png) は、現在の時間 t の状態 ![image](https://user-images.githubusercontent.com/25688193/49642152-94c67b80-fa55-11e8-9df4-e1277abc3fdf.png) と行動 ![image](https://user-images.githubusercontent.com/25688193/49642191-b0ca1d00-fa55-11e8-9110-0b7f8f4f82b7.png) のみに依存し、それ以前の時間 t-1,t-2,... での状態や行動によらない。<br>
    即ち、<br>
    ![image](https://user-images.githubusercontent.com/25688193/49642453-6dbc7980-fa56-11e8-8c29-c852c4c30266.png)<br>
    の関係が成り立つものとする。<br>

- （行動則、政策）<br>
    エージェントは、任意の時間 t での状態と報酬の観測の履歴 ![image](https://user-images.githubusercontent.com/25688193/49642500-9775a080-fa56-11e8-83fc-1978a4876ca6.png) の情報を元に行動することが出来る。<br>
    このような履歴情報を元にして、エージェントが行動を選択するルールを行動則（政策）π といい、この行動則 π は、確率で表現される（＝確率的に決定される）ものとなる。<br>

- （行動則により定まる確率過程）<br>
    そして、エージェントの行動則 π と初期状態 ![image](https://user-images.githubusercontent.com/25688193/49641553-d5bd9080-fa53-11e8-9c45-e3771976401b.png) から、確率的に決定される状態・行動・報酬の組の過程、即ち、確率過程 ![image](https://user-images.githubusercontent.com/25688193/49697224-10602e00-fbf8-11e8-8d5c-9eb374f3866f.png) が定まる。<br>


エージェントの目的は、これらの相互作用の過程を繰り返しながら、自身の行動則 π により目的関数である割引総和（＝収益）<br>
![image](https://user-images.githubusercontent.com/25688193/49642631-ffc48200-fa56-11e8-9021-8bf4253d6b89.png)<br>
の期待値を最大化することである。即ち、<br>
![image](https://user-images.githubusercontent.com/25688193/49642659-0b17ad80-fa57-11e8-8b2b-0c50d25a037d.png)<br>

以上のようなマルコフ決定過程による強化学習のモデル化を、行動則による確率的な行動選択とともに、状態遷移図で図示すると以下の図のようになる。<br>

![image](https://user-images.githubusercontent.com/25688193/49737885-4a513300-fcd1-11e8-961b-7e79abd0ebe4.png)<br>

ここで、更に、状態遷移確率による状態選択を含める描くと、以下の図のようになる。<br>

![image](https://user-images.githubusercontent.com/25688193/49738112-e54a0d00-fcd1-11e8-9998-736f8b5dc43e.png)<br>


<a id="価値関数"></a>

### ◎ 価値関数
> 記載中...

- 【参照サイト】<br>
    - [今さら聞けない強化学習（1）：状態価値関数とBellman方程式 - Qiita](https://qiita.com/triwave33/items/5e13e03d4d76b71bc802)<br>
    - [今さら聞けない強化学習（3）：行動価値関数とBellman方程式 - Qiita](https://qiita.com/triwave33/items/8966890701169f8cad47)<br>
    - [人工知能概論 第6回 多段決定(2) 強化学習.](https://slidesplayer.net/slide/11477412/)<br>


<a id="状態価値関数"></a>

#### ☆ 状態価値関数
先に述べたように、強化学習をマルコフ決定過程でモデル化するにあたって、エージェントの目的は、自身の行動則 π により目的関数である割引総和（＝収益）<br>
![image](https://user-images.githubusercontent.com/25688193/49642631-ffc48200-fa56-11e8-9021-8bf4253d6b89.png)<br>
の期待値を最大化することであったが、これを定常方策（＝時間に対して不変な行動則）での収益に限定された状態価値関数の概念を用いて、モデル化することを考える。<br>

![image](https://user-images.githubusercontent.com/25688193/49914512-de3f1c80-fed4-11e8-9dfc-706e81dba655.png)<br>

![image](https://user-images.githubusercontent.com/25688193/49742237-71acfd80-fcdb-11e8-934b-c5749312da03.png)<br>

ここで、この状態価値関数 ![image](https://user-images.githubusercontent.com/25688193/49739008-2c390200-fcd4-11e8-8c93-0609a67c8915.png) を先の状態遷移図に合わせて記載すると、上図のようになる。<br>
この図より、状態価値関数の定義にある将来に対する割引総和（＝収益、利得）の期待値 ![image](https://user-images.githubusercontent.com/25688193/49739267-c9943600-fcd4-11e8-9f8a-88ee2de9e046.png) は、緑枠内での報酬 r の平均をとったものになっていることが分かる。<br>


更に、この将来の割引総和に関しての期待値は、上図から分かるように、赤枠部分 ![image](https://user-images.githubusercontent.com/25688193/49739452-44f5e780-fcd5-11e8-935e-dfacc1315420.png) と青枠部分 ![image](https://user-images.githubusercontent.com/25688193/49739483-550dc700-fcd5-11e8-8d70-754fbf5e38d8.png) で各々平均値をとったものに対応しているので、以下の関係が成り立つことが分かる。<br>
![image](https://user-images.githubusercontent.com/25688193/49739922-63101780-fcd6-11e8-88bf-7e291f8e696c.png)<br>
尚、この関係式を上図から直感的にではなく、式変形で示すと以下のようになる。<br>
![image](https://user-images.githubusercontent.com/25688193/49739957-74f1ba80-fcd6-11e8-894a-bd5efa7164ca.png)<br>

ここで、この関係式は、状態価値関数に関しての方程式になっており、（状態価値関数に対しての）ベルマンの方程式という。<br>
そして、このベルマンの方程式は不動点方程式であるので、この方程式を解けば、原理的には不動点としての状態価値関数の最適解が一意に存在することになる（詳細は後述）<br>


<a id="行動価値関数"></a>

#### ☆ 行動価値関数
マルコフ決定過程において、価値関数の一種として、定常方策 ![image](https://user-images.githubusercontent.com/25688193/49683687-38726300-fb0c-11e8-8de2-b79447dcff37.png) の元での、状態 ![image](https://user-images.githubusercontent.com/25688193/49683696-4a540600-fb0c-11e8-984c-e6f2eccdeb94.png) にいて行動 ![image](https://user-images.githubusercontent.com/25688193/49683701-62c42080-fb0c-11e8-8690-62119c18f03d.png) を選択する価値を表わす、行動価値関数 ![image](https://user-images.githubusercontent.com/25688193/49683704-72dc0000-fb0c-11e8-841c-0c42230bcc5b.png) なるものを考えることも出来る。<br>

![image](https://user-images.githubusercontent.com/25688193/49914603-4d1c7580-fed5-11e8-810b-5063974a4d23.png)<br>
![image](https://user-images.githubusercontent.com/25688193/49915116-56a6dd00-fed7-11e8-838b-4866ed23e442.png)<br>

ここで、この状態行動関数 ![image](https://user-images.githubusercontent.com/25688193/49740845-74f2ba00-fcd8-11e8-90db-f273227cdbd0.png) を先の状態遷移図に合わせて記載すると、上図のようになる。<br>
この図より、状態行動関数の定義にある割引総和（＝収益、利得）の期待値 ![image](https://user-images.githubusercontent.com/25688193/49915218-bb623780-fed7-11e8-8daf-fe4499115302.png) は、オレンジ色内での報酬 r の平均をとったものになっていることが分かる。<br>

一般に、状態行動関数 ![image](https://user-images.githubusercontent.com/25688193/49683704-72dc0000-fb0c-11e8-841c-0c42230bcc5b.png) が与えられたときに、この状態行動関数の価値を最大化する行動、即ち、![image](https://user-images.githubusercontent.com/25688193/49915250-e6e52200-fed7-11e8-9675-de40d57af045.png) とするような行動選択は、「状態 s において、行動価値関数 Q に関して greedy（貪欲）な行動である」という。<br>
更に、”任意の” 状態において、Qに関してのグリーディな行動のみを選択する行動方策は、「行動価値関数 Q に関してグリーディな方策である」という。<br>

従って、達成可能な行動価値の最大値である最適行動価値関数 ![image](https://user-images.githubusercontent.com/25688193/49915291-185ded80-fed8-11e8-8eff-72bd73109142.png) においては、![image](https://user-images.githubusercontent.com/25688193/49915291-185ded80-fed8-11e8-8eff-72bd73109142.png) に関してのグリーディな方策は最適であることが分かる。<br>


<a id="状態価値関数と行動価値関数の関係"></a>

#### ☆ 状態価値関数と行動価値関数の関係
価値関数には、状態価値関数と行動価値関数の２つが存在するが、これら２つの関数は、以下の式によって、互いに結びつく。<br>

![image](https://user-images.githubusercontent.com/25688193/49915540-87881180-fed9-11e8-938c-68be355dde58.png)<br>

![image](https://user-images.githubusercontent.com/25688193/49915568-a5557680-fed9-11e8-9202-92a1cf3f8286.png)<br>
→ 将来の期待利得である状態価値関数 ![image](https://user-images.githubusercontent.com/25688193/49739008-2c390200-fcd4-11e8-8c93-0609a67c8915.png) は、（現在の期待利得である）行動価値関数 ![image](https://user-images.githubusercontent.com/25688193/49683704-72dc0000-fb0c-11e8-841c-0c42230bcc5b.png) に対して、行動方策に関しての分岐（赤枠部分）で和をとった形で表現できる。<br>

![image](https://user-images.githubusercontent.com/25688193/49915586-b605ec80-fed9-11e8-95fc-960c097e7eea.png)<br>
→ 現在の行動価値関数 ![image](https://user-images.githubusercontent.com/25688193/49683704-72dc0000-fb0c-11e8-841c-0c42230bcc5b.png) の値は、現在の状態 s で行動 a をとって状態 s′ に遷移するときの報酬 r(s,a,s′) に対して、確率分布 P(s,a,s′) で表現される遷移後の状態の分岐（青枠部分）に関しての和をとって平均化したもの（＝R）に、状態遷移後の状態価値関数 ![image](https://user-images.githubusercontent.com/25688193/49915737-6673f080-feda-11e8-97e7-a3e6a573cb44.png) の値を加えたものとなる。<br>
尚、状態遷移後の s′ 以降の価値は、状態価値関数 ![image](https://user-images.githubusercontent.com/25688193/49915737-6673f080-feda-11e8-97e7-a3e6a573cb44.png) に含まれるので、これ以降の分岐の和をとる必要はない。<br>


以上の関係式をまとめると、以下のようになる。<br>

![image](https://user-images.githubusercontent.com/25688193/49915748-7e4b7480-feda-11e8-870b-10c0dfb074c3.png)<br>

- （証明略）解釈図より成り立つことが分かる。<br>


<a id="ベルマン方程式"></a>

### ◎ ベルマン方程式
先に見たように、行動価値関数 ![image](https://user-images.githubusercontent.com/25688193/49915291-185ded80-fed8-11e8-8eff-72bd73109142.png) に関してグリーディな方策（＝状態 s において ![image](https://user-images.githubusercontent.com/25688193/49683704-72dc0000-fb0c-11e8-841c-0c42230bcc5b.png) を最大化する行動選択）は最適であるので、最適方策 ![image](https://user-images.githubusercontent.com/25688193/49916370-e18ad600-fedd-11e8-997c-0a1ca74029e4.png) を求めるには、最適行動価値関数 ![image](https://user-images.githubusercontent.com/25688193/49915291-185ded80-fed8-11e8-8eff-72bd73109142.png) の情報、或いは、最適状態価値関数 ![image](https://user-images.githubusercontent.com/25688193/49916303-b0120a80-fedd-11e8-8590-12a538918666.png)・即時報酬 r・遷移確率 P の情報があればよいことになる。<br>

従って、次の問題は、これら最適価値関数 ![image](https://user-images.githubusercontent.com/25688193/49915291-185ded80-fed8-11e8-8eff-72bd73109142.png),![image](https://user-images.githubusercontent.com/25688193/49916303-b0120a80-fedd-11e8-8590-12a538918666.png) を如何にして求めるかということになる。<br>
最適価値関数 ![image](https://user-images.githubusercontent.com/25688193/49915291-185ded80-fed8-11e8-8eff-72bd73109142.png),![image](https://user-images.githubusercontent.com/25688193/49916303-b0120a80-fedd-11e8-8590-12a538918666.png) は、以下のベッセルの方程式と呼ばれる、最適解に関しての不動点方程式から求めることが原理的には可能となる。<br>
※ ”原理的には可能” と表現したのは、ベッセルの方程式が最適価値関数に関しての不動点方程式となっており、収束先の不動点としての最適解を持つことが保証されるが、実際上この方程式を直接解くことは困難であり、代わりに動的計画法（価値反復、方策反復など）の手法で近似解を得るようにすることが多いため。<br>

まずは、最適価値関数のうち、最適状態価値関数 ![image](https://user-images.githubusercontent.com/25688193/49916303-b0120a80-fedd-11e8-8590-12a538918666.png) に対してのベッセルの方程式を考える。<br>

![image](https://user-images.githubusercontent.com/25688193/49916450-3c243200-fede-11e8-80b9-8bd996b0f34f.png)<br>

- （証明略）先の「状態価値関数」の項目を参照<br>

<br>

![image](https://user-images.githubusercontent.com/25688193/49916974-91f9d980-fee0-11e8-9c00-89c0788105a0.png)<br>

- （証明略）先の「状態価値関数」の項目を参照<br>

<br>

同様にして、最適行動価値関数 ![image](https://user-images.githubusercontent.com/25688193/49915291-185ded80-fed8-11e8-8eff-72bd73109142.png) に対してもベッセルの方程式が成り立ち、この不動点方程式の収束解（＝不動点）としての最適行動価値関数の値の存在性が保証される。<br>

![image](https://user-images.githubusercontent.com/25688193/49917004-ad64e480-fee0-11e8-945a-b2e048961947.png)<br>

- （証明略）先の「状態価値関数と行動価値関数の関係」の項目を参照<br>

<br>

- 【参考サイト】<br>
    - [Q-learningの収束性](https://qiita.com/ashigirl966/items/573d533180df49021f28)<br>


---

<a id="参考文献"></a>

## ■ 参考文献

- 速習 強化学習 ―基礎理論とアルゴリズム―
    - [amazonで詳細を見る](https://www.amazon.co.jp/%E9%80%9F%E7%BF%92-%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-%E2%80%95%E5%9F%BA%E7%A4%8E%E7%90%86%E8%AB%96%E3%81%A8%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0%ク%80%95-Csaba-Szepesvari/dp/4320124227)

- つくりながら学ぶ! 深層強化学習 ~PyTorchによる実践プログラミング<br>
    - [amazonで詳細を見る](https://www.amazon.co.jp/dp/4839965625/ref=asc_df_48399656252542427/?tag=jpgo-22&creative=9303&creativeASIN=4839965625&linkCode=df0&hvadid=295723231663&hvpos=1o2&hvnetw=g&hvrand=4934601497265078375&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=1009648&hvtargid=pla-527403650097&th=1&psc=1)<br>


<a id="使用コード"></a>

### ◎ 使用コード
> 実装中...
