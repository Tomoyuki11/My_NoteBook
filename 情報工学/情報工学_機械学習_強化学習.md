# 強化学習 [Reinforcement Learning]（執筆中）
機械学習の一種である強化学習について勉強したことをまとめたノート（忘備録）です。随時追記中。<br>

- 【参考サイト】
    - [今さら聞けない強化学習（1）：状態価値関数とBellman方程式](https://qiita.com/triwave33/items/5e13e03d4d76b71bc802)
    - [強化学習について学んでみた。（まとめ） - いものやま。](http://yamaimo.hatenablog.jp/entry/2016/01/11/200000)
    - [第３章： 強化学習の基礎理論](http://sysplan.nams.kyushu-u.ac.jp/gen/edu/RL_intro.html#Foundations)<br>
    - [これから強化学習を勉強する人のための「強化学習アルゴリズム・マップ」と、実装例まとめ](https://qiita.com/sugulu/items/3c7d6cbe600d455e853b)
    - [ゼロからDeepまで学ぶ強化学習](https://qiita.com/icoxfog417/items/242439ecd1a477ece312)
    - [dissecting reinforcement learning](https://github.com/mpatacchiola/dissecting-reinforcement-learning)


## 目次 [Contents]

1. 概要
    1. 強化学習の各種アルゴリズムの系統図
    1. 強化学習で特有の用語説明
1. [強化学習のモデル化](#強化学習のモデル化)
    1. [エージェントと環境の相互作用](#エージェントと環境の相互作用)
    1. [エピソード的タスクと連続タスク](#エピソード的タスクと連続タスク)
    1. [環境のマルコフ性](#環境のマルコフ性)
    1. [マルコフ決定過程（MDP）](#マルコフ決定過程（MDP）)
        1. マルコフ決定過程の例
    1. [価値関数](#価値関数)
    1. [状態価値関数](#状態価値関数)
    1. [行動価値関数](#行動価値関数)
    1. [状態価値関数と行動価値関数の関係](#状態価値関数と行動価値関数の関係)
    1. [ベルマン方程式](#ベルマン方程式)
        1. [ベルマン最適方程式とグリーディーな選択](#ベルマン最適方程式とグリーディーな選択)
    1. ベルマンの方程式の厳密な解法
1. [代表的な古典的強化学習手法の比較](#代表的な古典的強化学習手法の比較)
1. [強化学習における動的計画法（DP法）](#強化学習における動的計画法（DP法）)
    1. [反復法による近似解](#反復法による近似解)
    1. [方策評価](#方策評価)
        1. [反復方策評価](#反復方策評価)
    1. [方策改善](#方策改善)
        1. [方策改善定理](#方策改善定理)
        1. [方策改善定理とグリーディー方策](#方策改善定理とグリーディー方策)
        1. [方策改善のアルゴリズム](#方策改善のアルゴリズム)
    1. [方策反復法 [policy iteration]](#方策反復法)
    1. [価値反復法 [value iteration]](#価値反復法)
    1. [非同期動的計画法の概要](#非同期動的計画法の概要)
    1. [一般化方策反復（GPI）](#一般化方策反復（GPI）)
    1. 動的計画法の効率
1. [モンテカルロ法（MC法）](#モンテカルロ法)
    1. [初回訪問モンテカルロ法](#初回訪問モンテカルロ法)
    1. [逐一訪問モンテカルロ法](#逐一訪問モンテカルロ法)
    1. [モンテカルロ-ES法](#モンテカルロ-ES法)
    1. [方策オン型モンテカルロ法](#方策オン型モンテカルロ法)
    1. [方策オフ型モンテカルロ法](#方策オフ型モンテカルロ法)
    1. モンテカルロ法による価値推定の適用例
1. [TD学習（時間的差分学習）](#TD学習（時間的差分学習）)
    1. [TD(0)法](TD(0)法)
    1. [Sarsa（方策オン型TD制御）](#Sarsa)
        1. Sarsa の適用例
    1. [Q学習 [Q-learning]（方策オフ型TD制御）](#Q学習)
        1. Q 学習の適用例
    1. [アクター・クリティック手法](#アクター・クリティック手法)
    1. [nステップTD法](#nステップTD法)
    1. [TD(λ)法](#TD(λ)法)
        1. [TD(λ)の前方観測的な見方](#TD(λ)の前方観測的な見方)
        1. [TD(λ)の後方観測的な見方と適格度トレース](#TD(λ)の後方観測的な見方と適格度トレース)
        1. 前方観測的見方と後方観測的見方の等価性
        1. [Sarsa(λ)](#Sarsa(λ))
        1. [Q(λ)](#Q(λ))
    1. [アクタークリティック手法への適格度トレースの適用](#アクタークリティック手法への適格度トレースの適用)
1. [関数による価値関数の近似（関数近似手法）](#関数による価値関数の近似（関数近似手法）)
    1. [損失関数としての最小２乗誤差（MSE）](#損失関数としての最小２乗誤差（MSE）)
    1. [パラメータの更新式としての最急降下法](#パラメータの更新式としての最急降下法)
    1. [線形関数での価値関数の近似（線形手法）](#線形関数での価値関数の近似（線形手法）)
    1. [最急降下型Sarsa（関数近似手法のSarsaへの適用）](#最急降下型Sarsa（線形手法のSarsaへの適用）)
    1. 最急降下型Q学習（関数近似手法のQ学習への適用）
    1. 方策勾配法
        1. 方策勾配法の適用例
1. [モデルとプランニング（各種古典的強化学習の統一的見解）](#モデルとプランニング)
    1. [Dyna,Dyna-Q](#Dyna,Dyna-Q)
1. [深層学習の構造を取り込んだ強化学習手法（ニューラルネットワークによる関数近似）](#深層学習の構造を取り込んだ強化学習手法)
    1. [多層パーセプトロン（MLP）による価値関数の近似](#多層パーセプトロン（MLP）による価値関数の近似)
    1. [Neural Fitted Q Iteration（MLPによる関数近似で学習を安定化させるための工夫）](NeuralFittedQIteration)
    1. [Deep Q Network（DQN）](#DeepQNetwork)
    1. Double-DQN（DDQN）
    1. Dueling Network
    1. prioritized experience replay（優先順位付き経験再生）
    1. A2C [Asynchronous Advantage Actor-Critic]
    1. GORILA [General Reinforcement Learning Architecture]
    1. UNREAL [UNsupervised REinforcement and Auxiliary Learning]
    1. PPO
1. 補足事項
    1. [【外部リンク】機械学習](http://yagami12.hatenablog.com/entry/2017/09/17/111400)
    1. [【外部リンク】ニューラルネットワーク / ディープラーニング](http://yagami12.hatenablog.com/entry/2017/09/17/111935)
        1. [【補足（外部リンク）】パーセプトロン](http://yagami12.hatenablog.com/entry/2017/09/17/111935#ID_2)
        1. [【補足（外部リンク）】畳み込みニューラルネットワーク(CNN)](http://yagami12.hatenablog.com/entry/2017/09/17/111935#ID_3)
    1. [【外部リンク】最適化問題](http://yagami12.hatenablog.com/entry/2017/09/17/101739)
    1. [【外部リンク】グラフ理論](http://yagami12.hatenablog.com/entry/2017/09/17/110406)
        1. [【補足（外部リンク）】確率過程とマルコフ連鎖](http://yagami12.hatenablog.com/entry/2017/09/17/110406#ID_3)
1. [参考文献](#参考文献)
    1. [使用コード](#使用コード)

---

<!--
<a id="概要"></a>

## ■ 概要
> 記載中...

- 強化学習の学習対象：<br>
    強化学習は、教師あり学習や教師なし学習と同じく、機械学習に分類される手法であるが、教師あり学習や教師なし学習のように、与えられたデータを元に学習を行うのではなく、与えられた環境を元に学習を行う点が、これらの機械学習手法とは異なる。<br>
    より詳細には、強化学習では、行動の評価方法（＝価値関数）と行動の選び方（＝行動方策）の２つを学習する。<br>
-->

<!--
<a id="強化学習の各種アルゴリズムの系統図"></a>

### ◎ 強化学習の各種アルゴリズムの系統図
> 記載中...

<a id="強化学習の各種アルゴリズムの関係"></a>

### ◎ 強化学習で特有の用語説明
> 記載中...

-->

<a id="強化学習のモデル化"></a>

## ■ 強化学習のモデル化

<a id="エージェントと環境の相互作用"></a>

### ◎ エージェントと環境の相互作用
![image](https://user-images.githubusercontent.com/25688193/50441622-e9676300-093e-11e9-8179-a4a7deefa11c.png)<br>

上図は、強化学習で取り扱うシステムを図示したものである。<br>
このシステムは、以下のように、エージェントと環境との相互作用でモデル化される。<br>

- エージェントとは、意思決定と学習（※将来に渡る収益の期待値を最大化するような学習）を行う行動主体である。<br>
- 環境とは、エージェントが相互作用を行う対象であり、エージェントの行動 a を反映した上で、エージェントに対して、状態 s と、その状態での報酬 r を与える。<br>
- エージェントの行動 a は、確率分布で表現される行動方策 [policy] ![image](https://user-images.githubusercontent.com/25688193/50441875-081a2980-0940-11e9-9edb-7faff8a50fa5.png) に基づいて選択される。<br>
	ここで、![image](https://user-images.githubusercontent.com/25688193/50441875-081a2980-0940-11e9-9edb-7faff8a50fa5.png) は、![image](https://user-images.githubusercontent.com/25688193/50441923-3d267c00-0940-11e9-997f-f25cd5dd12ca.png) ならば ![image](https://user-images.githubusercontent.com/25688193/50441977-5a5b4a80-0940-11e9-8306-2669589b23e1.png) となる確率を表す。<br>

- エージェントと環境は、継続的に相互作用を行う。<br>
	より詳細には、離散時間 t=0,1,2,... の各々の時間 t おいて、相互作用を行う。<br>
- エージェントがその意思決定目的とする収益は、時間経過 t=0,1,2,.. での時間経過によって、その値が減衰するものとする。<br>
	つまり、時間 t 時点から見て、将来 t+1,t+2,... にわたっての報酬 r の差し引きを考慮した割引収益<br>
	![image](https://user-images.githubusercontent.com/25688193/50442003-752dbf00-0940-11e9-9449-9c2d8391e466.png)<br>
	で考える。<br>
	※ 割引収益で考えるのは、同じ収益値ならば、将来受け取るよりも現時点で受け取るほうがよい値であることの妥当性もあるが、別の理由として、終端状態の存在しない連続タスクにおいて、割引なしの場合の将来にわたっての収益の和が、t→∞ の時間経過により、無限大に発散してしまい、エージェントの目的である将来にわたっての収益の最大化が、数学的に取り扱いずらい問題になってしまうことを防ぐという理由もある。<br>
- そして、エージェントは、時間経過 t=0,1,2,.. での相互作用によって、この将来にわたっての割引収益 ![image](https://user-images.githubusercontent.com/25688193/50442058-a1494000-0940-11e9-9af7-aaa8a2c8b836.png) の期待値 E[R] を最大化するように、自身の行動方策 ![image](https://user-images.githubusercontent.com/25688193/50441875-081a2980-0940-11e9-9edb-7faff8a50fa5.png) を学習しながら、行動する。<br>
	※ 一般的に、強化学習の枠組みにおいて、時間に依存する行動方策 ![image](https://user-images.githubusercontent.com/25688193/50442104-cc339400-0940-11e9-9471-d4864775f324.png) ではなく、時間 t によらない定常方策 ![image](https://user-images.githubusercontent.com/25688193/50442137-e40b1800-0940-11e9-8133-9b607406bb93.png) が用いられる。<br>
	（価値関数の満たすべき方程式を与えるベルマンの方程式は、定常方策 ![image](https://user-images.githubusercontent.com/25688193/50442137-e40b1800-0940-11e9-8133-9b607406bb93.png) を前提とした方程式となっている。）<br>

- ここで、エージェントと環境の相互作用の順序は、以下のようになる。<br>
	① エージェントは、環境から状態![image](https://user-images.githubusercontent.com/25688193/50442183-161c7a00-0941-11e9-95e2-c20d8df60afd.png)を受け取る。（＝エージェントの状態が![image](https://user-images.githubusercontent.com/25688193/50442183-161c7a00-0941-11e9-95e2-c20d8df60afd.png)になる。）<br>
	② エージェントは、現在の状態![image](https://user-images.githubusercontent.com/25688193/50442183-161c7a00-0941-11e9-95e2-c20d8df60afd.png)に基づき、行動方策 ![image](https://user-images.githubusercontent.com/25688193/50441875-081a2980-0940-11e9-9edb-7faff8a50fa5.png) に従って、行動![image](https://user-images.githubusercontent.com/25688193/50442241-58de5200-0941-11e9-8757-a4c9bddb8c28.png)を選択する。<br>
	③ 時間が１ステップ t→t+1 経過後に、エージェントは、エージェントの行動![image](https://user-images.githubusercontent.com/25688193/50442241-58de5200-0941-11e9-8757-a4c9bddb8c28.png)の結果として、報酬 ![image](https://user-images.githubusercontent.com/25688193/50442311-a6f35580-0941-11e9-90f9-2afbb39360c9.png) を受取る。<br>
	※ 時間 t で環境が受け取った行動![image](https://user-images.githubusercontent.com/25688193/50442241-58de5200-0941-11e9-8757-a4c9bddb8c28.png)による即時報酬という意味で、![image](https://user-images.githubusercontent.com/25688193/50442280-84f9d300-0941-11e9-8afb-968821fa5805.png)ではなく ![image](https://user-images.githubusercontent.com/25688193/50442311-a6f35580-0941-11e9-90f9-2afbb39360c9.png) を用いる。<br>
    ※ つまり、状態は行動を選択するための判断材料となり、報酬はその行動を評価するための判断材料となる。<br>

- 尚、強化学習の文脈において、エージェントと環境との相互作用の一連の時系列単位を、エピソード [Episode] という。<br>
	このエピソードは、エージェントの初期状態から始まって、環境との相互作用により終端状態まで移行するまでのの過程を、１回のエピソードとして扱う。<br>
	例えば、迷路探索問題においては、エージェントが、迷路のスタート地点からゴールまで辿りつくまでの一連の過程が、１回のエピソードになる。<br>
	強化学習による学習（＝行動方策の学習や価値関数の学習など）では、１回のエピソード内の各時間ステップ t→t+1 度に学習を行う手法（例えば、価値反復法などの動的計画法やTD法）もあれば、１回のエピソードが完了する毎に学習を行う手法（例えば、モンテカルロ法）も存在するので、エピソードと時間ステップを明確に区別しておく必要がある。<br>

> 【Memo】　エージェントと環境の境界線はどのように設定すべきか？<br>
> このようにモデル化される強化学習のシステムにおいて、実際上のタスクでは、どのようにエージェントと環境との線引を行えばよいのか？という問題は残る。<br>
> これは例えば、ロボット制御タスクにおいては、ロボットを構成するモーターやセンサーなどの物理的デバイスは、その内部の電子回路など自体は、エージェントであるロボット自身からは制御できにので、エージェントの一部というよりも、むしろ環境の一部とみなしたほうが適切であるのでは？と考えられる。<br>
> 同様の例として、人体制御タスクにおいては、人体を構成する筋肉や骨格や感覚器などは、エージェントの一部というよりも、むしろ環境の一部とみなしたほうが適切であるのでは？と考えられる。<br>
> この問題の一般的な基準を与える回答としては、エージェントがに任意に変更出来ないものは、エージェントの外部にあると考え、それらを環境とみなすという解決策が考えられる。<br>

このエージェントと環境の相互作用を、時間や相互作用の順序を含まて考える場合においては、以下のようなバックアップ線図で表現したほうがわかりやすい。<br>
例えば、以下の図は、マス目上に仕切られた迷路探索問題（移動方向が上下左右の４方向）における、バックアップ線図である。<br>
但し、この問題では、状態遷移関数 ![image](https://user-images.githubusercontent.com/25688193/50538553-af9f9200-0bb4-11e9-9442-301138dd7d21.png) が確率１で決定論的に定まるために、状態遷移関数による分岐が存在しないことに注意（青線部分）。<br>

![image](https://user-images.githubusercontent.com/25688193/50538389-8c73e300-0bb2-11e9-8cdc-bd78db544897.png)<br>

より一般的には、以下のバックアップ線図のように、状態遷移関数 ![image](https://user-images.githubusercontent.com/25688193/50538553-af9f9200-0bb4-11e9-9442-301138dd7d21.png) による分岐が存在する。<br>

![image](https://user-images.githubusercontent.com/25688193/50540536-ebe3ea00-0bd6-11e9-86c8-df661d23c4f5.png)<br>

尚、バックアップ線図を用いての詳細な議論は、後述の価値関数～ベルマンの方程式で議論する。<br>
※ バックアップ線図という名前は、価値関数の満たすべき方程式を記述するベルマンの方程式が、前回と今回の価値関数の値の再帰的関係（＝バックアップ）で記述されるためである。<br>


<a id="エピソード的タスクと連続タスク"></a>

### ◎ エピソード的タスクと連続タスク
> 記載中...


<a id="環境のマルコフ性"></a>

### ◎ 環境のマルコフ性
強化学習で生じるエージェントと環境との相互作用の過程（＝確率過程となる）において、時間 t で状態![image](https://user-images.githubusercontent.com/25688193/50442183-161c7a00-0941-11e9-95e2-c20d8df60afd.png)にあるエージェントによって選択された行動![image](https://user-images.githubusercontent.com/25688193/50442241-58de5200-0941-11e9-8757-a4c9bddb8c28.png)に対して、次の時間 t+1 において環境がどのように応答するのかという問題を考える。<br>

この問題の最も一般的なモデル化では、以前に生じた全ての事象 ![image](https://user-images.githubusercontent.com/25688193/50513328-7eeb2a00-0ada-11e9-80f9-e242fb707b51.png) に依存した条件付き同時確率分布でモデル化する。<br>
即ち、<br>
![image](https://user-images.githubusercontent.com/25688193/50513339-90cccd00-0ada-11e9-9796-779541f37c2d.png)<br>

しかしながら、最も一般的なモデルでは、過去の全ての事象に依存するために、問題設定や計算が複雑になるという問題が存在する。<br>
従って、強化学習のモデル化においては、一般的に、環境に対してマルコフ性の性質を仮定する。<br>

ここで、このマルコフ性とは、「ある状態![image](https://user-images.githubusercontent.com/25688193/50442183-161c7a00-0941-11e9-95e2-c20d8df60afd.png)から、別の状態 ![image](https://user-images.githubusercontent.com/25688193/50513369-bce84e00-0ada-11e9-9dd0-4307c3e2ceba.png) に移行する確率が、それ以前の経路 t によらず、現在の状態のみ![image](https://user-images.githubusercontent.com/25688193/50442183-161c7a00-0941-11e9-95e2-c20d8df60afd.png)で決まる。」という性質である。<br>
今の場合、環境に対してマルコフ性の性質を仮定するので、（環境がエージェントに与える）状態 s と報酬 r が対象となる。式で書くと、<br>
![image](https://user-images.githubusercontent.com/25688193/50513421-09cc2480-0adb-11e9-8bd1-d1931879f7fe.png)<br>
である。<br>

このように環境がマルコフ性の性質を満たすとき、現在の状態![image](https://user-images.githubusercontent.com/25688193/50442183-161c7a00-0941-11e9-95e2-c20d8df60afd.png)と行動![image](https://user-images.githubusercontent.com/25688193/50442241-58de5200-0941-11e9-8757-a4c9bddb8c28.png)さえ与えられてば、１ステップ t→t+1 間での環境による遷移確率 ![image](https://user-images.githubusercontent.com/25688193/50513466-57e12800-0adb-11e9-944a-0e37b676d907.png) から、次のステップ t+1 での状態 ![image](https://user-images.githubusercontent.com/25688193/50513369-bce84e00-0ada-11e9-9dd0-4307c3e2ceba.png) と行動 ![image](https://user-images.githubusercontent.com/25688193/50513450-32ecb500-0adb-11e9-9099-6a772a891938.png) を予測することが出来るので、これらの反復処理を逐次繰り返すことにより、過去の全ての履歴が与えられた場合と同様にして、将来における状態と行動を予測できる構造が織り込まれていることになる。（それ故に、マルコフ性の条件が重要である。）<br>
式で書くと、<br>
![image](https://user-images.githubusercontent.com/25688193/50513433-1fd9e500-0adb-11e9-8385-603a511642c9.png)<br>
である。<br>


<a id="マルコフ決定過程（MDP）"></a>

### ◎ マルコフ決定過程（MDP）
先に見たように、強化学習は、エージェント環境が各時間ステップ t=0,1,2,... で継続的に相互作用するシステムにおいて、エージェントが目標を示す数値である割引収益を最大化するように行動方策を学習制御するような過程 ![image](https://user-images.githubusercontent.com/25688193/50513530-b9a19200-0adb-11e9-8453-602482bab801.png) であって、更に、環境に対してマルコフ性の性質を仮定してモデル化された。<br>

このようなモデルは、マルコフ決定過程 [MDP:Markov decision process] の枠組みで厳密にモデル化出来る。
このマルコフ決定過程は、マルコフ性とエルゴード性を持つ確率過程であって、選択可能な行動、及び、報酬を追加したものである。<br>
言い換えると、マルコフ決定過程は、マルコフ連鎖に対して、選択可能な行動、及び、報酬を追加したものである。<br>

- エルゴード性（定常性）：<br>
    任意の初期状態からスタートした確率過程が、無限時間経過後には、最初の状態とは無関係になる性質。<br>
    マルコフ連鎖は、このエルゴード性の性質も合わせ持つ。<br>

<br>

![image](https://user-images.githubusercontent.com/25688193/50538944-a7972080-0bbb-11e9-86ce-31699b9f41c4.png)<br>

<!--
> 【Memo】<br>
> 「速習 強化学習」の本に書いてあるマルコフ決定過程の定義、状態集合、行動集合、遷移確率カーネルの３つの構造の組から定義されてるのに対し、他の文献では、状態集合、行動集合、遷移確率、報酬関数４つの組から定義されてものが多いが、３つの構造の１つの遷移確率カーネルが、状態遷移と報酬関数を与えるから、報酬関数を新たに３つの組とは別に追加しなくても well-defined となる。<br>
-->

このマルコフ決定過程による過程は、以下の遷移グラフで図示するとわかりやすい。<br>
但し、この遷移グラフでは、行動方策による行動の選択の分岐は表現できていないことと、時間ステップ t に対する ![image](https://user-images.githubusercontent.com/25688193/50513611-23ba3700-0adc-11e9-89af-9b8d7e64254d.png) のダイナミクス ![image](https://user-images.githubusercontent.com/25688193/50513625-33398000-0adc-11e9-9038-3148ec0c7335.png) は表現していないこと注意。（添字の位置に注意）<br>

![image](https://user-images.githubusercontent.com/25688193/50513600-100ed080-0adc-11e9-912d-c3b3a41abe56.png)<br>

時間ステップによるダイナミクス、行動方策による行動の選択の分岐と、遷移確率による状態の遷移の分岐を含めて表現するには、以下のバックアップ線図で図示するとわかりやすい。<br>

![image](https://user-images.githubusercontent.com/25688193/50540536-ebe3ea00-0bd6-11e9-86c8-df661d23c4f5.png)<br>

- 【参考サイト】<br>
    - [マルコフ決定過程 - Wikipedia](https://ja.wikipedia.org/wiki/%E3%83%9E%E3%83%AB%E3%82%B3%E3%83%95%E6%B1%BA%E5%AE%9A%E9%81%8E%E7%A8%8B)<br>



<a id="価値関数"></a>

### ◎ 価値関数
> 記載中...

<!--
- 定常方策<br>
	マルコフ決定過程のマルコフ性より、行動方策 ![image](https://user-images.githubusercontent.com/25688193/50535412-08f3cb00-0b8d-11e9-99a9-48cb8d5b501c.png) は、全ての過程の履歴に依存した ![image](https://user-images.githubusercontent.com/25688193/50535431-435d6800-0b8d-11e9-8896-b0d52bbaee87.png) ではなく、時間ステップ間 t→t+1 にのみ依存する。このような行動方策を定常方策 ![image](https://user-images.githubusercontent.com/25688193/50535449-81f32280-0b8d-11e9-9b09-529f91684bc7.png) という。<br>
    以降の議論では、行動方策というと、定常方策であることを前提とする。<br>
-->

- 状態価値関数：![image](https://user-images.githubusercontent.com/25688193/50574476-a6b8f580-0e2c-11e9-9452-59c5584dd01d.png)<br>
    状態 s で現在の行動方策 ![image](https://user-images.githubusercontent.com/25688193/50574479-bdf7e300-0e2c-11e9-8270-089af93d1e67.png) に従いつづけた（＝定常方策）ときに得られる価値。<br>

- 行動価値関数：![image](https://user-images.githubusercontent.com/25688193/50574484-df58cf00-0e2c-11e9-8bc5-5ad14a34838b.png)<br>
    状態 s で行動 a を選択し、その後は既存の行動方策 ![image](https://user-images.githubusercontent.com/25688193/50574479-bdf7e300-0e2c-11e9-8270-089af93d1e67.png) に従いつづけた（＝定常方策）ときに得られる価値。<br>

<br>

- 【参照サイト】<br>
    - [今さら聞けない強化学習（1）：状態価値関数とBellman方程式 - Qiita](https://qiita.com/triwave33/items/5e13e03d4d76b71bc802)<br>
    - [今さら聞けない強化学習（3）：行動価値関数とBellman方程式 - Qiita](https://qiita.com/triwave33/items/8966890701169f8cad47)<br>
    - [人工知能概論 第6回 多段決定(2) 強化学習.](https://slidesplayer.net/slide/11477412/)<br>


<a id="状態価値関数"></a>

### ◎ 状態価値関数
先に述べたように、強化学習をマルコフ決定過程でモデル化するにあたって、エージェントの目的は、自身の行動則 π により目的関数である割引収益<br>
![image](https://user-images.githubusercontent.com/25688193/50535390-cfbb5b00-0b8c-11e9-9f32-ee8b8be25f14.png)<br>
の期待値を最大化することであったが、これを時間に依存しない行動方策である定常方策での収益に限定された状態価値関数の概念を用いて、モデル化することを考える。<br>

![image](https://user-images.githubusercontent.com/25688193/51423254-a24c5500-1c00-11e9-9f05-9547e27376f4.png)<br>

以下の図は、バックアップ線図に状態価値関数 ![image](https://user-images.githubusercontent.com/25688193/50538680-330db300-0bb6-11e9-8f4f-3aa37c5878e2.png) を記した図である。<br>

![image](https://user-images.githubusercontent.com/25688193/50540754-03bd6d00-0bdb-11e9-804a-7a92b0438156.png)<br>

このバックアップ図より、状態価値関数の定義にある将来に対する割引収益の期待値 ![image](https://user-images.githubusercontent.com/25688193/49739267-c9943600-fcd4-11e8-9f8a-88ee2de9e046.png) は、青枠内での（将来における）報酬 r の平均をとったものになっていることが分かる。<br>

更に、この将来の割引総和に関しての期待値は、上図から分かるように、行動方策での分岐（赤線）の和の部分 ![image](https://user-images.githubusercontent.com/25688193/49739452-44f5e780-fcd5-11e8-935e-dfacc1315420.png) と遷移関数での分岐（青線）の和の部分 ![image](https://user-images.githubusercontent.com/25688193/50540677-9b21c080-0bd9-11e9-9b33-f532231b154b.png) で各々平均値をとったものに対応しているので、以下の関係が成り立つことが分かる。<br>
![image](https://user-images.githubusercontent.com/25688193/50540682-ac6acd00-0bd9-11e9-8260-30441b653835.png)<br>
尚、この関係式を上図のバックアップ線図から直感的にではなく、式変形で示すと以下のようになる。<br>
![image](https://user-images.githubusercontent.com/25688193/50540685-c60c1480-0bd9-11e9-9677-1ce92a9f5c13.png)<br>

ここで、この関係式は、状態価値関数に関しての方程式になっており、（状態価値関数に対しての）ベルマンの方程式という。<br>
そして、このベルマンの方程式は不動点方程式であるので、この方程式を解けば、原理的には不動点としての状態価値関数の最適解が一意に存在することになる（詳細は後述）<br>


<a id="行動価値関数"></a>

### ◎ 行動価値関数
マルコフ決定過程において、価値関数の一種として、定常方策 ![image](https://user-images.githubusercontent.com/25688193/49683687-38726300-fb0c-11e8-8de2-b79447dcff37.png) の元での、状態 ![image](https://user-images.githubusercontent.com/25688193/49683696-4a540600-fb0c-11e8-984c-e6f2eccdeb94.png) にいて行動 ![image](https://user-images.githubusercontent.com/25688193/49683701-62c42080-fb0c-11e8-8690-62119c18f03d.png) を選択する価値を表わす、行動価値関数 ![image](https://user-images.githubusercontent.com/25688193/49683704-72dc0000-fb0c-11e8-841c-0c42230bcc5b.png) なるものを考えることも出来る。<br>

![image](https://user-images.githubusercontent.com/25688193/51423267-d162c680-1c00-11e9-8833-2fb4c2e729e9.png)<br>

以下の図は、バックアップ線図に状態価値関数 ![image](https://user-images.githubusercontent.com/25688193/50538680-330db300-0bb6-11e9-8f4f-3aa37c5878e2.png) を記した図である。<br>

![image](https://user-images.githubusercontent.com/25688193/50540791-d1f8d600-0bdb-11e9-94aa-039af2de8b79.png)<br>

このバックアップ線図より、状態行動関数の定義にある割引収益の期待値 ![image](https://user-images.githubusercontent.com/25688193/49915218-bb623780-fed7-11e8-8daf-fe4499115302.png) は、青枠内での報酬 r の平均をとったものになっていることが分かる。<br>


<a id="状態価値関数と行動価値関数の関係"></a>

### ◎ 状態価値関数と行動価値関数の関係
価値関数には、状態価値関数と行動価値関数の２つが存在するが、これら２つの関数は、互いに結びつく。<br>
このことをバックアップ線図を元に見ていく。<br>

以下の図は、バックアップ線図に状態価値関数 ![image](https://user-images.githubusercontent.com/25688193/50538680-330db300-0bb6-11e9-8f4f-3aa37c5878e2.png) と行動価値関数 ![image](https://user-images.githubusercontent.com/25688193/49683704-72dc0000-fb0c-11e8-841c-0c42230bcc5b.png) とを記した図である。<br>

![image](https://user-images.githubusercontent.com/25688193/50540879-7b8c9700-0bdd-11e9-833c-88b86ab42533.png)<br>

このバックアップ線図から、以下のような関係が成り立つことが分かる。<br>

![image](https://user-images.githubusercontent.com/25688193/50540839-6d8a4680-0bdc-11e9-97dc-735c210affd5.png)<br>
→ 将来の期待利得である状態価値関数 ![image](https://user-images.githubusercontent.com/25688193/49739008-2c390200-fcd4-11e8-8c93-0609a67c8915.png) は、（現在の期待利得である）行動価値関数 ![image](https://user-images.githubusercontent.com/25688193/49683704-72dc0000-fb0c-11e8-841c-0c42230bcc5b.png) に対して、行動方策に関しての分岐（赤枠部分）で和をとった形で表現できる。<br>

![image](https://user-images.githubusercontent.com/25688193/50540841-84c93400-0bdc-11e9-8675-580dc0a713e2.png)<br>

以上の関係式をまとめると、以下のようになる。<br>

![image](https://user-images.githubusercontent.com/25688193/51423275-f22b1c00-1c00-11e9-8f37-18b441d8b29e.png)<br>

- （証明略）バックアップ線図より成り立つことが分かる。<br>


<a id="ベルマン方程式"></a>

### ◎ ベルマン方程式
次の問題は、これら最適価値関数 ![image](https://user-images.githubusercontent.com/25688193/49915291-185ded80-fed8-11e8-8eff-72bd73109142.png),![image](https://user-images.githubusercontent.com/25688193/49916303-b0120a80-fedd-11e8-8590-12a538918666.png) を如何にして求めるかということである。<br>
最適価値関数 ![image](https://user-images.githubusercontent.com/25688193/49915291-185ded80-fed8-11e8-8eff-72bd73109142.png),![image](https://user-images.githubusercontent.com/25688193/49916303-b0120a80-fedd-11e8-8590-12a538918666.png) は、以下のベルマン方程式と呼ばれる、最適解に関しての不動点方程式から求めることが原理的には可能となる。<br>
※ ”原理的には可能” と表現したのは、ベルマン方程式が最適価値関数に関しての不動点方程式となっており、収束先の不動点としての最適解を持つことが保証されるが、実際上この方程式を直接解くことは困難であり、代わりに動的計画法（価値反復、方策反復など）の手法で近似解を得るようにすることが多いため。<br>

まずは、最適価値関数のうち、最適状態価値関数 ![image](https://user-images.githubusercontent.com/25688193/49916303-b0120a80-fedd-11e8-8590-12a538918666.png) に対してのベルマン方程式を考える。<br>

![image](https://user-images.githubusercontent.com/25688193/51423283-20a8f700-1c01-11e9-9a93-248404293ba2.png)<br>

- （証明略）先の「状態価値関数」の項目を参照<br>

<br>

![image](https://user-images.githubusercontent.com/25688193/51423295-4e8e3b80-1c01-11e9-91f2-302d7fc1e6b4.png)<br>

- （証明略）先の「状態価値関数」の項目を参照<br>

<br>

同様にして、最適行動価値関数 ![image](https://user-images.githubusercontent.com/25688193/49915291-185ded80-fed8-11e8-8eff-72bd73109142.png) に対してもベルマン方程式が成り立ち、この不動点方程式の収束解（＝不動点）としての最適行動価値関数の値の存在性が保証される。<br>

![image](https://user-images.githubusercontent.com/25688193/51423302-65cd2900-1c01-11e9-8a62-e1dea4ffeb7d.png)<br>

- （証明略）先の「状態価値関数と行動価値関数の関係」の項目を参照<br>

<br>

- 【参考サイト】<br>
    - [Q-learningの収束性](https://qiita.com/ashigirl966/items/573d533180df49021f28)<br>


<a id="ベルマン最適方程式とグリーディーな選択"></a>

#### ☆ ベルマン最適方程式とグリーディーな選択
一般的に、コンピューターサイエンスの分野におけるグリーディーの意味とは、「長期的・大域的には、より良い代替案・最適解を見つかる可能性を考慮せずに、短期的・局所的な情報の中でのみ、代替案・最適解を見つけるような検索方針・意思決定手続き」のこのとを指す。<br>

今考えている、最適価値関数 ![image](https://user-images.githubusercontent.com/25688193/50548596-f06dd880-0c92-11e9-8f2f-8831c8f089b3.png) は、ベルマン最適方程式<br>
![image](https://user-images.githubusercontent.com/25688193/50548622-0aa7b680-0c93-11e9-853e-3ba507cae13a.png)<br>
の解として与られるが、この２つの方程式は、いずれも、１ステップ間 ![image](https://user-images.githubusercontent.com/25688193/50548641-70943e00-0c93-11e9-9f7f-c296533f8431.png) での短期的なグリーディーな選択 ![image](https://user-images.githubusercontent.com/25688193/50548645-7ee25a00-0c93-11e9-82ea-ce42e67f3d07.png) を取りさえすれば、１エピーソード間の長期的な意味でも最適解に到達できることを意味している。<br>
言い換れば、１ステップ間でのその時点での短期的で局所的なグリーディーな選択さえ続けていれば、エピーソード間での大域的最適解が得られることを意味している。<br>

これは、価値関数は、”将来”に対して報酬の期待値として定義しているために、将来の全ての可能な挙動がもららす報酬が、既に考慮されていることに起因する。<br>

<!--
> 【Memo】ベルマン最適方程式と大域的最適解、価値関数の定義について<br>
> ベルマン最適方程式、ベルマン最適作用素が縮小写像になってて、不動点としての価値関数の最適解を持つという見方でだけで見ると、まあそうだよなあ～くらいの感想になるけど、<br>
> このベルマン最適方程式の解としての最適価値関数が、大域的最適解であって、これは１ステップ間の局所的でグリーディーな行動選択を繰り返しさえすれば得れるということ。<br>
> そして、この性質（局所的→大域的）は、状態価値関数が、将来の全ての過程による報酬を考慮した定義になっていることに起因している（逆に言えば、そうなるように状態価値関数を定義している）という見方で見ると、理論構築、モデル化のうまさに感心する。<br>
-->

<!--
<a id="ベルマンの方程式の厳密な解法"></a>

### ◎ ベルマンの方程式の厳密な解法
> 要書き換え...

まずは、価値反復法や方策反復法などの動的計画法による近似手法を用いず、ベルマンの方程式の式に従って、１エピーソード間を再帰的に総当たりで価値関数を求めていく方法の問題点を見てみる。<br>

![image](https://user-images.githubusercontent.com/25688193/50538389-8c73e300-0bb2-11e9-8cdc-bd78db544897.png)<br>

> 記載中...
-->


<a id="代表的な古典的強化学習手法の比較"></a>

## ■ 代表的な古典的強化学習手法の比較
ここでは、代表的な古典的強化学習アルゴリズムである、<br>

- 動的計画法（価値反復法、方策反復法など）
- モンテカルロ法
- TD学習（Sarsa、Q学習など）
- 関数近似手法

などの各種古典的強化学習手法を、統一的観点から、互いの特徴を比較する。<br>

以下の表は、各種古典的強化学習手法（横軸）を、いくつかの代表的な特徴軸（縦軸）で比較した表である。<br>

![image](https://user-images.githubusercontent.com/25688193/51425692-e18b9d80-1c22-11e9-92e9-1c6085ff3b0d.png)<br>

![image](https://user-images.githubusercontent.com/25688193/51425741-6f678880-1c23-11e9-95a6-4295ceb56a76.png)<br>

![image](https://user-images.githubusercontent.com/25688193/51425757-9756ec00-1c23-11e9-901f-63a577482c44.png)<br>

![image](https://user-images.githubusercontent.com/25688193/51425916-30870200-1c26-11e9-998f-1a623bf75631.png)<br>


<a id="強化学習における動的計画法（DP法）"></a>

## ■ 強化学習における動的計画法（DP法）
先のベルマンの方程式<br>
![image](https://user-images.githubusercontent.com/25688193/50536344-b5867a80-0b96-11e9-8c24-a4f95f038217.png)<br>
が示す重要な性質は、「ある状態や行動の価値 ![image](https://user-images.githubusercontent.com/25688193/49739008-2c390200-fcd4-11e8-8c93-0609a67c8915.png), ![image](https://user-images.githubusercontent.com/25688193/49683704-72dc0000-fb0c-11e8-841c-0c42230bcc5b.png) は、その後の状態や行動の価値 ![image](https://user-images.githubusercontent.com/25688193/49915737-6673f080-feda-11e8-97e7-a3e6a573cb44.png), ![image](https://user-images.githubusercontent.com/25688193/50039721-0caa2c80-007a-11e9-8f8b-b1700677dc38.png) を用いて表すことが出来る。」という、ブートストラップ性の性質である。<br>
（※機械学習で用られるデータ分割手法の１つであるブートストラップ法と異なるものであることに注意）<br>

このような価値関数のブートストラップ性を利用して、価値関数を反復的に枝分かれをたどりながら逐次計算していく手法を総称して、動的計画法という。<br>

そして、このブーストラップ性の性質をバックアップ線図で書いたものが、以下のような動的計画法のバックアップ線図になる。<br>
![image](https://user-images.githubusercontent.com/25688193/51425378-a38c7a80-1c1e-11e9-9648-cc415f61d18b.png)<br>

※ この動的計画法で計算可能になるためには、ベルマンの方程式に出てくる状態遷移関数 ![image](https://user-images.githubusercontent.com/25688193/50536348-c9ca7780-0b96-11e9-9f54-67ee92954561.png) が既知であることが条件になる。言い換えると、動的計画法による強化学習手法は、環境のモデルを共にした手法で、モデルベースの強化学習手法である。<br>

※ これに対して、環境のダイナミクスが既知でなくとも有効な価値関数の計算手法として、モンテカルロ法やTD法などの、モデルフリーの強化学習手法がある。（詳細は、後述）<br>

ここで、このベルマンの方程式は、価値関数に対しての不動点方程式にもなっており、この不動点方程式が導く不動点としての最適価値関数の存在の一意性は、価値反復法 [value iteration] や方策反復法 [policy iteration] などの動的計画法のアルゴリズムで、最適解が得られることの根拠となる。<br>


<a id="反復法による近似解"></a>

### ◎ 反復法による近似解
方策評価をする（＝ベルマン方程式を解く）にあたっては、厳密な解法では、ベルマン方程式を、状態集合 S 個の未知変数 s∈S を持つ、S 個の連立一次方程式とみなし、これを解くことになるが、この厳密な解法では、一般的に計算量が膨大となり、現実的でない。<br>
そこで、反復法による近似解を用いる。<br>

反復法は、ベルマンの方程式を満たす価値関数、及びそのときの最適行動方策を、ベルマンの方程式の式にそのままに従って再帰的に総当たり計算するのではなく、今の状態と次の状態の２層間のみの繰り返し計算により近似的に求める動的計画法の一種である。<br>
より詳細には、<br>
- 今の状態 s と次の状態 s' の２層間のみでの計算を、k=0,1,2,... で更新しながら反復する。<br>
- 最適解に近づくまで、この更新処理 k=0,1,2,... を続ける。<br>
- 得られた最適解の近似から、最適行動方針を算出する。<br>
というのが基本的なアイデアである。<br>

![image](https://user-images.githubusercontent.com/25688193/50572784-2766fa00-0e0b-11e9-8c8b-0fcbb1ce55ab.png)<br>

反復法には、それぞれ価値反復法と方策反復法が存在するが、両者の違いは、行動選択における以下のような方向性に違いとなる。<br>

- Policyベース：<br>
    行動方策 π により、行動選択が行われる。<br>
    方策反復法は、このPolicy ベースな反復法になっている。<br>

- Value ベース：<br>
    価値関数を最大化するような行動選択を行う。<br>
    （＝![image](https://user-images.githubusercontent.com/25688193/49993657-98648000-ffca-11e8-8048-e6e8f69202f2.png)））<br>
    価値反復法は、このValueベースな反復法となっている。<br>

<br>

- 【参考サイト】<br>
    - [今さら聞けない強化学習（5）：状態価値関数近似と方策評価](https://qiita.com/triwave33/items/bed0fd7a2b56ee8e7c29#_reference-085df97779eb2480be0f)<br>


<a id="方策評価"></a>

### ◎ 方策評価
状態価値関数 ![image](https://user-images.githubusercontent.com/25688193/50572772-fd153c80-0e0a-11e9-8470-2744fff5f836.png) は、状態 s において、定常方策 ![image](https://user-images.githubusercontent.com/25688193/50574565-1e3b5480-0e2e-11e9-9d9d-fcdc6204b91e.png) に従いつづけた際の価値を表している。<br>
従って、任意の定常方策 ![image](https://user-images.githubusercontent.com/25688193/50572762-d9ea8d00-0e0a-11e9-8faa-32f9a7cf8155.png) に対する、状態価値関数 ![image](https://user-images.githubusercontent.com/25688193/50572772-fd153c80-0e0a-11e9-8470-2744fff5f836.png) を求めることは、ある定常方策の評価指数となりうる価値を求めて、方策を評価していることになるので、（動的計画法の文脈では）”方策評価” という。<br>

状態価値関数についてのベルマン方程式<br>
![image](https://user-images.githubusercontent.com/25688193/50572779-0e5e4900-0e0b-11e9-9c4d-0910b01dc82a.png)<br>
は、ある行動方策 π のもとでの、状態価値関数を解としているので、ベルマン方程式を解くことは、方策評価となる。<br>


<a id="反復方策評価"></a>

#### ☆ 反復方策評価
![image](https://user-images.githubusercontent.com/25688193/50572784-2766fa00-0e0b-11e9-8c8b-0fcbb1ce55ab.png)<br>

上図のように、状態価値関数に対してのベルマン方程式を、更新規則とした更新式<br>
![image](https://user-images.githubusercontent.com/25688193/50574655-c6055200-0e2f-11e9-8d60-8c55bfdd3fae.png)<br>
から生成される近似列 ![image](https://user-images.githubusercontent.com/25688193/50574668-095fc080-0e30-11e9-90d8-5a936f8d73d5.png) を考える。<br>
すると、この近似列 ![image](https://user-images.githubusercontent.com/25688193/50574668-095fc080-0e30-11e9-90d8-5a936f8d73d5.png) は、k → ∞の極限で、![image](https://user-images.githubusercontent.com/25688193/50572807-9d6b6100-0e0b-11e9-91e6-562566b8a54f.png) に収束することが知られている（証明略）。<br>
このアルゴリズムは、ベルマン方程式を繰り返し解くこと、即ち、方策評価を繰り返し行い、価値関数を近似しているので、反復方策評価という。<br>

ここで、この反復方策評価のアルゴリズムには、以下のような特徴が存在する。<br>

- スイープ操作と完全バックアップ<br>
    一般的に、現在評価している行動方策 π の元で、実現可能な全ての１ステップ遷移（＝時間ステップ t→t+1 での遷移）に対して、バックアップ処理（＝時間ステップでの遷移後から遷移前の逆方向への更新処理）を行うことを完全バックアップという。<br>
    今の反復方策評価では、全ての可能な状態 ![image](https://user-images.githubusercontent.com/25688193/50572815-c4299780-0e0b-11e9-9c65-70a52285c37e.png) を sweep 操作しながら、１ステップ遷移 ![image](https://user-images.githubusercontent.com/25688193/50572822-d4da0d80-0e0b-11e9-919f-55dd7b5ca276.png) 毎に、１度ずつバックアップ処理（ ![image](https://user-images.githubusercontent.com/25688193/50572825-ea4f3780-0e0b-11e9-992e-6da6e107cb86.png) ）を行っているので、完全バックアップ処理を行っていることになる。（下図の迷路探索問題の例を参照）<br>
    ![image](https://user-images.githubusercontent.com/25688193/50572933-6ac26800-0e0d-11e9-9ab9-1a713c1e62df.png)<br>
    cf : サンプルバックアップ<br>
    <br>
- その場更新型のコード実装<br>
    反復方策評価で取り扱うインデックス k に対する更新式（＝逐次近似式）<br>
    ![image](https://user-images.githubusercontent.com/25688193/50572860-8bd68900-0e0c-11e9-8cb3-ed901bb5f32e.png)<br>
    をコード実装する際に、<br>
    単純に考えると、新しい価値関数の配列 ![image](https://user-images.githubusercontent.com/25688193/50572922-ff789600-0e0c-11e9-8a64-90bf60627366.png) と元の配列 ![image](https://user-images.githubusercontent.com/25688193/50572924-2931bd00-0e0d-11e9-8af2-8c7bdfbff633.png) の２つの配列を確保し、それらを逐次更新していく実装が考えられるが、<br>
    別の実装方法として、価値関数の配列を１つだけ確保し、その時点 k での新しい価値関数 ![image](https://user-images.githubusercontent.com/25688193/50572861-9abd3b80-0e0c-11e9-875f-217476556858.png) を、古い価値関数 ![image](https://user-images.githubusercontent.com/25688193/50572914-d48e4200-0e0c-11e9-95e1-d10dec96d54f.png) に直接上書きする実装方法も考えられる。<br>
    後者の方法では、新しいデータをすぐに利用するので、２つの配列を用いる方法よりも、通常は速く収束するというメリットが存在する。<br>

以上の事項をアルゴリズムとしてまとめると、以下のようになる。<br>

![image](https://user-images.githubusercontent.com/25688193/51423316-a2992000-1c01-11e9-8a1d-a58d4759252c.png)<br>


<a id="方策改善"></a>

### ◎ 方策改善
従来の行動方策から、別の新しい行動方策に変更することで、得られる価値がより良くなるのかを？という方策改善の問題を考える。<br>
この方策改善は、<br>
① 状態 s において、従来の行動方策 π に従いつづけた際の価値である状態価値関数 ![image](https://user-images.githubusercontent.com/25688193/50580760-1a431d00-0e96-11e9-98ea-7f6cbdfe935d.png) と、<br>
② 状態 s から、新しい行動方策 π' に従って、一度だけ行動 a を選択して、その後は従来行動方策 π に従いつづけた際の価値である行動価値関数 ![image](https://user-images.githubusercontent.com/25688193/50589246-3a400400-0ec9-11e9-921c-bee22c53e1db.png)<br>
という２つの値との大小比較<br>
![image](https://user-images.githubusercontent.com/25688193/50589262-4a57e380-0ec9-11e9-9bb6-b41cfa16c026.png)<br>
から、計量的に取り扱うことが出来る。<br>

- 【参考サイト】<br>
    - [今さら聞けない強化学習（6）：反復法による最適方策](https://qiita.com/triwave33/items/59768d14da38f50fb76c)<br>


<a id="方策改善定理"></a>

#### ☆ 方策改善定理
このことを一般的に述べたものが、以下の方策改善定理と呼ばれるものになる。<br>

![image](https://user-images.githubusercontent.com/25688193/51423339-0885a780-1c02-11e9-8a7c-5ad006d76ffe.png)<br>

- （証明）<br>
    状態価値関数と行動価値関数の関係より、<br>
    ![image](https://user-images.githubusercontent.com/25688193/50589355-a7ec3000-0ec9-11e9-8d93-51bcccd50235.png)<br>
    の関係が成り立つので、以下のような時間ステップ t に関しての漸化式が成り立つ。<br>
    ![image](https://user-images.githubusercontent.com/25688193/50589372-b89ca600-0ec9-11e9-9d7c-594ec9bf19b0.png)<br>
    従って、![image](https://user-images.githubusercontent.com/25688193/50589407-d4a04780-0ec9-11e9-8a07-1a9e6af85b5c.png) の関係が成り立つので、定理が成り立つことが分かる。<br>


<a id="方策改善定理とグリーディー方策"></a>

#### ☆ 方策改善定理とグリーディー方策
ここで、以下のように定義される新しい行動方策としてのグリーディー方策<br>
![image](https://user-images.githubusercontent.com/25688193/50593589-41244200-0edc-11e9-8637-4e229e91ba1b.png)<br>
は、方策改善定理にある新たな方策 π′ の条件 ![image](https://user-images.githubusercontent.com/25688193/50589481-2517a500-0eca-11e9-8bdf-5f2c9f5f1621.png) を満たし、方策改善になっている。（証明略）<br>
つまり、１ステップ間 t→t+1 で、グリーディー方策で行動を選択することで、方策が改善されていくことが保証される。<br>


<a id="方策改善のアルゴリズム"></a>

#### ☆ 方策改善のアルゴリズム
このグリーディー方策による方策改善をアルゴリズムとしてまとめると、以下のようになる。<br>

![image](https://user-images.githubusercontent.com/25688193/51423343-1fc49500-1c02-11e9-9c28-cf0b9b8068d0.png)<br>


<a id="方策反復法"></a>

### ◎ 方策反復法 [policy iteration]
先にみた方策評価と方策改善のプロセスを繰り返すことで、方策と価値価値関数を次々と更新し、最終的には、最適行動方策と最適価値関数に収束させるようなアルゴリズムであって、行動選択を行動方策に基づきて行う Policyベースののアルゴリズムを、方策反復法という。<br>

![image](https://user-images.githubusercontent.com/25688193/51423332-dffdad80-1c01-11e9-8d90-c475f103c039.png)<br>

- 【参考サイト】<br>
    - [強化学習について学んでみた。（その12）](http://yamaimo.hatenablog.jp/entry/2015/09/06/200000)<br>


<a id="価値反復法"></a>

### ◎ 価値反復法 [value iteration]
先に見てきた方策反復法では、そのアルゴリズム中に含まれる方策評価のプロセスにおいて、全ての状態に対してのスイープ操作による完全バックアップ処理を行うために、方策改善のプロセスの段階に移行するまで時間がかかるという欠点が存在する。<br>

そこで、価値反復法と呼ばれるアルゴリズムでは、最適解への収束性を失うことなく、１回のスイープ操作度に、方策改善を行うようにして、方策反復法よりも速く方策改善が行えるようにしている。<br>

更に、価値反復法では、状態価値関数の推定値 V(s) の更新式として<br>
![image](https://user-images.githubusercontent.com/25688193/50592761-dd4c4a00-0ed8-11e9-9cf9-34f3513d0274.png)<br>
というベルマン最適方程式に準じた形の更新式を利用することで、
価値関数を最大化するように行動選択を行うという、Valueベースの手法となっている。<br>

この更新式は、先のPolicyベースな方策反復法での、状態価値関数の更新式<br>
![image](https://user-images.githubusercontent.com/25688193/50592786-f05f1a00-0ed8-11e9-98e5-6b3573b7c975.png)<br>
と比較すると、<br>
価値反復法では、行動方策に関しての全ての分岐 ![image](https://user-images.githubusercontent.com/25688193/50592834-21d7e580-0ed9-11e9-835c-3c75ff156b05.png) の情報に従って更新を行うのではなくて、グリーディーな行動選択 ![image](https://user-images.githubusercontent.com/25688193/50593104-46808d00-0eda-11e9-8902-2451d812fa66.png) に従って、更新を行っていることになるので、<br>
分岐の総和をとる方策反復法に比べて、グリーディーで max のみを選択するために、１ステップあたりの計算量は、価値反復法に比べて減少するというメリットが存在する。<br>

以上のことをアルゴリズムとしてまとめると、以下のようになる。<br>

![image](https://user-images.githubusercontent.com/25688193/51423409-f6f0cf80-1c02-11e9-95ad-46d371b69750.png)<br>


- 【参考サイト】<br>
    - [強化学習について学んでみた。（その13）](http://yamaimo.hatenablog.jp/entry/2015/09/07/200000)<br>


<a id="非同期動的計画法の概要"></a>

## ◎ 非同期動的計画法の概要
この非同期動的計画法のアルゴリズムの詳細はここでは述べない。以下はその概要説明<br>

これまで見てきた各種DP手法（＝方策反復法、価値反復法）では、状態集合 S 全体に対して、スイープ操作とバックアップ処理が必要であるために、多くの実際上の強化学習タスクのように、状態の数が莫大になるようなケースにおいて、計算量とメモリが爆発的に増加してしまうという欠点が存在する。<br>

非同期動的計画法のアルゴリズムでは、この問題を解決するために、全状態をスイープ操作するのではなくて、ある状態の状態価値関数の更新に関係のある状態を任意に選別して、それらを元に状態価値関数を更新（＝バックアップ）する。<br>
このように、選択する状態を選別することで、動的計画法による最適化にあまり関係のない状態の価値関数の更新処理を省くことが出来るため、計算量やメモリ消費量を大幅に削減することが出来る。<br>


<a id="一般化方策反復（GPI）"></a>

## ◎ 一般化方策反復（GPI）
一般化方策反復（GPI）とは、これまで見てきた様々な強化学習手法でなぜ最適解が得られるのかを、共通の枠組みで解釈するための考え方・概念のことである。（※アルゴリズムではないことに注意）<br>

これまで見てきた強化学習手法では、<br>
例えば、方策反復法では、方策評価のループが完了→方策改善に移行。<br>
価値反復法では、１回の方策評価→１回の方策改善に移行。<br>
といったように、方策評価のプロセスと方策改善のプロセスにおいて、その粒度の違いは存在するものの、方策評価と方策改善の２つのプロセスの相互作用で、最適解に収束させていくアルゴリズムであった。<br>

一般化方策反復（GPI）では、このような強化学習手法に共通する、方策評価と方策改善の２つのプロセスの相互作用による最適解への収束過程を、一般的概念（共通概念）として扱う。<br>
これにより、ほとんど全ての強化学習手法は、（明確な方策と価値関数、及びそれによる方策評価、方策改善プロセスを持つために、）この一般化方策反復の枠組みで、定性的に解釈することが出来る。<br>

以下の図は、一般化方策反復の枠組みで、方策評価と方策改善のプロセスが互いに相互作用しながら、最適解に収束していく様子を図示した模式図である。<br>

![image](https://user-images.githubusercontent.com/25688193/50631021-36be8280-0f86-11e9-8873-8d3d81d36122.png)<br>

- 安定性：<br>
	一般化方策反復の枠組みでの安定性とは、方策評価と方策改善のプロセスにおいて、これ以上変化が生じず、価値関数と方策が共に最適になっていることを意味している。<br>
	ここで、価値関数が安定になるのは、現在の方策との ”整合性” が取れている場合である。<br>
    又、方策が安定になるのは、行動方策が、現在の価値関数に対してのグリーディ方策になっている場合のみである。<br>


<a id="モンテカルロ法"></a>

## ■ モンテカルロ法による価値推定、方策評価と方策改善
モンテカルロ法とは、数値計算やシミュレーションなどにおいて、ランダムな乱数をサンプリングすることで数値計算（例えば、積分計算など）を行う手法の総称であるが、今考えているマルコフ決定過程における価値関数の推定問題、方策評価と方策改善問題にも応用出来る。<br>

先に見たように、動的計画法と呼ばれる、マルコフ決定過程でモデル化された環境における価値関数のブートストラップ性を利用して、反復的に枝分かを辿りながら価値関数を計算していく手法は、モデルベースの強化学習手法であり、ベルマンの方程式に出てくる状態遷移関数 ![image](https://user-images.githubusercontent.com/25688193/50039765-09637080-007b-11e9-8694-e28e343c2bb7.png) が既知でなくてはならないという問題があった。<br>

これに対して、モンテカルロ法による価値関数の推定法では、ブートストラップ性を利用せず、（マルコフ決定過程でモデル化された環境ではなく）実際の環境が繰り返し生成するエピソードの系列（＝サンプル）を利用して、経験的に価値関数を推定するため、この状態遷移関数が既知でなくてもよいというメリットが存在する。（＝モデルフリーの強化学習手法）<br>
※ 但し、状態遷移関数が既知でなくてもよいのは、状態価値関数ではなく行動価値関数を推定するアルゴリズムに限定される。（詳細は後述）<br>
※ モンテカルロ法では、エピソードの系列全体を利用するので、エピソードの完了を待ってから方策評価・方策改善を行う必要がある。<br>
※ 又、収益の分散が大きい場合、モンテカルロ法による価値関数の推定は、信頼度が低くなってしまうという問題がある。<br>

<br>

下図は、動的計画法とモンテカルロ法による手法の違いを示したバックアップ線図である。<br>

![image](https://user-images.githubusercontent.com/25688193/51425191-26600600-1c1c-11e9-9e4b-902836f21ad9.png)<br>

動的計画法の各種手法は、マルコフ決定過程でモデル化した環境が、ベルマン方程式を満たすこと、更に、ベルマン方程式のブーストラップ性により、動的計画法の各種手法のバックアップ線図は、現在の状態行動対 (s,a)∈S×A →次回の状態行動対 (s′,a′)∈S×A で可能な遷移経路全てで分岐していた。<br>

一方、モンテカルロ法では、（モデル化された環境ではなくて）実際の環境上で、１つのエピソードが繰り返し試行され、経験的に１つの経路が繰り返し試行されるだけなので、モンテカルロ法のバックアップ線図は、上図の太線部分で示された１つのエピソードの開始から終端までの１つのサンプリングされた経路となる。<br>


ここで、状態価値関数は、<br>
![image](https://user-images.githubusercontent.com/25688193/50636865-059c7d00-0f9b-11e9-9c9f-5baa5479c22f.png)<br>
のように期待利得に関する期待値で定義されるものであったことを考えると、モンテカルロ法で価値推定を行う場合には、実際の環境が生成するエピソードの系列に従って状態を訪問した後に観測した収益を平均化すれば良いことが分かる。<br>
そうすることで、より多くの訪問回数を重ねるにつれて、平均値が期待値に収束するようになる。<br>

- 【参考サイト】<br>
    - [今さら聞けない強化学習（7）：モンテカルロ法で価値推定](https://qiita.com/triwave33/items/0c8833e6b899c26b208e#_reference-523997a713762bb0a83c)<br>
    - [強化学習について学んでみた。（その14）](http://yamaimo.hatenablog.jp/entry/2015/09/30/200000)<br>
    - [強化学習について学んでみた。（その16）](http://yamaimo.hatenablog.jp/entry/2015/10/02/200000)<br>


<a id="初回訪問モンテカルロ法"></a>

### ◎ 初回訪問モンテカルロ法による価値推定
初回訪問モンテカルロ法は、あるエピソードにおいて、状態 s への初回訪問の結果で発生した収益のみを平均値するアルゴリズムで、以下のようになる。<br>
※ このアルゴリズムは、状態価値関数の推定を行っているだけで、方策評価と方策改善は行っていないことに注意。<br>

![image](https://user-images.githubusercontent.com/25688193/51425203-4d1e3c80-1c1c-11e9-8504-1ca193f26276.png)<br>

<a id="逐一訪問モンテカルロ法"></a>

### ◎ 逐一訪問モンテカルロ法による価値推定
逐一訪問モンテカルロ法は、エピソード群全体での、状態 s への全ての訪問後に発生した収益に対して、平均化処理を行い、その平均値で状態価値関数の推定を行う。<br>
※ このアルゴリズムは、状態価値関数の推定を行っているだけで、方策評価と方策改善は行っていないことに注意。<br>


<a id="モンテカルロ-ES法"></a>

### ◎ モンテカルロ-ES法
モンテカルロ法による価値推定、方策評価が有効であるためには、以下のような事項を考慮する必要がある。<br>
そして、これらの事項を考慮したモンテカルロ法を、モンテカルロ-ES法 [ES:Exploring Starts] という。<br>

- 状態価値関数ではなく行動価値関数で方策評価：<br>
    状態遷移関数 ![image](https://user-images.githubusercontent.com/25688193/50039765-09637080-007b-11e9-8694-e28e343c2bb7.png) が与えられていて、モデルのダイナミクスが明確なときは、<br>
    ![image](https://user-images.githubusercontent.com/25688193/50676409-8610bb80-1037-11e9-8771-5815123238cb.png)<br>
    の関係式に従って、算出した状態価値関数の推定値 V(s) から、行動方策 π を算出することが出来る。<br>
	逆に、状態遷移関数 ![image](https://user-images.githubusercontent.com/25688193/50039765-09637080-007b-11e9-8694-e28e343c2bb7.png) が不明で、モデルのダイナミクスが不明なときは、この関係式に従って、状態価値関数の推定値 V(s) から、行動方策 π を算出することが出来ないので、状態価値関数ではなくて行動価値関数を推定値 Q(s,a) を算出し、そこから<br>
    ![image](https://user-images.githubusercontent.com/25688193/50676459-c7a16680-1037-11e9-9653-d35cd0b68be5.png)<br>
    の関係式に従って、行動方策を算出する必要がある。<br>
    モンテカルロ法のメリットの１つは、先に述べたようにモデルのダイナミクスが不明であっても、価値推定が行えることにあるので、このメリットを享受するためには、状態価値関数ではなく、行動価値関数で方策評価する必要がある。<br>

- 到達されない状態行動対と開始点検索の仮定：<br>
	モンテカルロ法では、シミュレーションに従って経験的にエピソードの試行を行うが、行動方策が確率１の決定論的な行動方策であった場合、或いは、ほとんど決定論的な行動方策であった場合、エピソードの試行回数を多くしても、モンテカルロ法のシミュレーション上で到達されない状態や行動が出てきてしない、結果として、平均化すべき収益がなくなるので、収益の平均値として算出される行動価値関数の推定値も正しく算出出来なくなるという問題が存在する。<br>
	この問題を解決するための１つの方法として、全ての状態とそれに続く行動セット（＝状態行動対）を、エピソードの開始点に取ることが出来るという、開始点検索の仮定をおく方法が考えられる。<br>
    これにより、無限回のエピソードの試行において、全ての状態行動対が訪問され、正しく行動価値関数の推定値が算出可能となることが保証される。<br>

- 無限個のエピソードを試行する必要があることへの対応：<br>
    モンテカルロ法による価値推定が、正しい値に収束するためには、原理的には、無限回のエピソードを試行する必要がある。<br>
	但し、（これまで見てきた動的計画法の手法と同じように）推定値の近似値を得るという目的であれば、無限回のエピソードを試行せずとも、十分回数の多いエピソードの試行回数で近似推定値を得ることが出来る。<br>
    或いは無限回のエピソードを試行することを避ける別の方法としては、方策評価が完了して方策改善を行う手順ではなくて、方策評価と方策改善を交互に逐次行っておく方法が考えられる。<br>
    （例えば、動的計画法の手法の１つである価値反復法では、１回の方策評価→１回の方策改善を繰り返すアルゴリズムになっている。）<br>
    モンテカルロ-ES法では、この方法を採用しており、１エピーソード単位で方策評価と方策改善の交互に行う。つまりは、各エピーソード完了後に、観測された収益から算出された行動価値関数の推定値で、方策評価を行い、エピーソード中に訪問された全ての状態において、方策改善が行われるという具合である。<br>

これらの事項を考慮したモンテカルロ法を、モンテカルロ-ES法といい、以下のようなアルゴリズムとなる。<br>

![image](https://user-images.githubusercontent.com/25688193/51423419-2f90a900-1c03-11e9-94f2-34c92e50b319.png)<br>


<a id="方策オン型モンテカルロ法"></a>

### ◎ 方策オン型モンテカルロ法
先のモンテカルロ-ES法では、決定論的な行動方策において、モンテカルロシミュレーション上で、実際には到達しない状態行動対が存在するために、利得の平均値の計算が出来ず、結果として、収益の平均値として算出される行動価値関数の推定値も正しく算出出来なくなるという問題を回避するために、開始点検索（＝全ての状態行動対を、エピソードの開始点に設定出来る）の仮定を行っていた。<br>

しかしながら、この開始点検索の仮定は、実際上のタスクにおいて、非現実的なものであので、次に、開始点検索の仮定を除外することを考える。<br>
この方法には、大きく分けて２つの方法（方策オン型モンテカルロ法と方策オフ型モンテカルロ法）があるが、いづれもエージェントに、到達しない状態行動対を選び続けさせるというのが基本的なコンセプトになっている。<br>
※ ここでのオン・オフの意味は、モンテカルロシミュレーションでエピソードの系列（＝状態行動対）を観測する際に、方策評価、方策改善で対象となる行動方策を「使う」/「使わない」ということを意味している。<br>

まずは、方策オン型モンテカルロ法について見ていく。<br>

<br>

方策オン型モンテカルロ法では、決定論的な行動方策を全てにおいて完全に排除して、常に全ての状態 ∀s∈S と行動 ∀a∈A において ![image](https://user-images.githubusercontent.com/25688193/50682651-e9f4ad80-1052-11e9-908a-bb7b9ffff6b8.png) というソフトなもの置き換える。<br>
これにより、常に ![image](https://user-images.githubusercontent.com/25688193/50682687-009b0480-1053-11e9-8b31-4a21adcfdada.png) なので、到達されない状態行動対は存在しなくなり、開始点検索の仮定は必要なくなる。<br>
そして、上記のように方策をソフトなものに置き換えた上で、行動方策を、決定論的になる最適方策に向けて、徐々に移行させていくようにする。<br>

方策をソフトなものに置き換えるやり方は、いくつか変種があるが、ここでは、ε-greedy 手法を用いる方法を見ていく。<br>

ε-グリーディ方策では、以下の式で与えられる行動方策 ![image](https://user-images.githubusercontent.com/25688193/50687984-ca678000-1066-11e9-9091-91c9d0d9c06a.png) である。<br>
![image](https://user-images.githubusercontent.com/25688193/50721383-f1bf5b00-1101-11e9-8cc2-962005cf9c39.png)<br>

先に見た一般化方策（GPI）による共通の知見では、方策改善が行えるためには、行動方策がグリーディ方策に移行しなくてはならないが、これは、移行の過程で、該当方策がずっとグリーディ方策であることは要求しておらず、グリーディ方策に移行することだけが要求されている。<br>

今考えている方策オン型モンテカルロ法の場合では、単純にεーグリーディ方策へと移行するだけである？<br>
行動価値関数 ![image](https://user-images.githubusercontent.com/25688193/50721389-07cd1b80-1102-11e9-999e-244a455abbe9.png) に関するどのような ε-グリーディ方策も、従来のソフト方策に関して、改善された方策になっている。このことは方策改善定理によって保証されている。<br>
即ち、任意の状態 ∀s∈S 、新しい ε-グリーディー方策 π′、従来のソフト方策 π に対して、<br>
![image](https://user-images.githubusercontent.com/25688193/50721375-d6ece680-1101-11e9-8ec2-d2b888db50a8.png)<br>
となり、方策改善定理の条件が成り立つので、新しい εーグリーディー方策 π′ と従来のソフト方策 π に対して、![image](https://user-images.githubusercontent.com/25688193/50721400-30551580-1102-11e9-9cde-8a69a8ebb475.png) となり、![image](https://user-images.githubusercontent.com/25688193/50721406-44991280-1102-11e9-90cc-9b0d9d5ffe49.png) となる。<br>
つまり、行動価値関数 ![image](https://user-images.githubusercontent.com/25688193/50721389-07cd1b80-1102-11e9-999e-244a455abbe9.png) に関するどのような ε-グリーディ方策も、従来のソフト方策に関して、改善された方策になっている。<br>

<br>

以上のことを踏まえて、先のモンテカルロ-ES法から、開始点検索の仮定を取り除き、決定論的な行動方策をソフト方策に置き換ると、以下のような方策オン型モンテカルロ法のアルゴリズムが得られる。<br>

![image](https://user-images.githubusercontent.com/25688193/51423430-5949d000-1c03-11e9-88df-96bf2870dc9e.png)<br>


<a id="方策オフ型モンテカルロ法"></a>

### ◎ 方策オフ型モンテカルロ法
開始点検索の仮定を外しても、価値推定を正しく行うことが出来る別の方法として、方策オフ型モンテカルロ法が存在する。<br>

この方策オフ型モンテカルロ法では、行動方策を、次の２つの方策 π′,π に分ける。<br>
- 挙動方策 π′ [behavior policy]：エピーソードを生成するための方策<br>
- 推定方策 π [estimation policy]：方策評価・方策改善に使用する方策<br>

このように方策を２つの方策に分けた場合は、挙動方策と推定方策とで得られる収益が異なるので、如何にしてエピソードを生成する挙動方策から、推定方策を使って方策評価や方策改善を行えばよいのか？というのが課題となる。<br>

この課題を解決するために、挙動方策と推定方策それぞれで、実際に生成されるエピソードの系列が実現される確率を比較し、それらを収益で評価（＝価値関数で評価）するときの重みとして利用することを考える。<br>

![image](https://user-images.githubusercontent.com/25688193/50761541-02accf80-12ae-11e9-8923-2b2feff21623.png)<br>

そのためにまず、上図のように、挙動方策と推定方策２つの方策 π,π′ に対して、エピソード全体に渡っての、以下のような確率を導入する。<br>

![image](https://user-images.githubusercontent.com/25688193/50761863-ce85de80-12ae-11e9-9e55-1ecf4582929e.png)<br>

今のモンテカルロ法の場合、この２つの確率 ![image](https://user-images.githubusercontent.com/25688193/50761938-fa08c900-12ae-11e9-990b-4112da28c31d.png) は、状態遷移関数 ![image](https://user-images.githubusercontent.com/25688193/50761964-09881200-12af-11e9-9699-9d60674fa353.png) が不明なので、未知の変数であるが、その確率の比 ![image](https://user-images.githubusercontent.com/25688193/50761984-160c6a80-12af-11e9-82a8-71370818403d.png) をとると、<br>
![image](https://user-images.githubusercontent.com/25688193/50762126-7a2f2e80-12af-11e9-8898-184fd855559b.png)<br>
となるので、状態遷移確率によらないので、モデルのダイナミクスが不明であっても、２つの行動方策（推定方策、挙動方策）から計算可能な値となる。<br>

次に、挙動方策 π′ で収益 R が観測されたときに、推定方策 π でも収益 R が観測される確率を考えると、この確率は、確率の比 ![image](https://user-images.githubusercontent.com/25688193/50761984-160c6a80-12af-11e9-82a8-71370818403d.png) で表現されるので、推定方策 π のもとでの収益の合計 ![image](https://user-images.githubusercontent.com/25688193/50763504-46ee9e80-12b3-11e9-9ec7-92a6a48e4c4d.png) と行動価値関数の推定値 Q(s,a) は、以下のようなアルゴリズムで求まることが分かる。<br>
![image](https://user-images.githubusercontent.com/25688193/50763520-55d55100-12b3-11e9-8c71-860362ebeda6.png)<br>

ここまでの事項をアルゴリズムとしてまとめると、以下のような方策オフ型モンテカルロ法のアルゴリズムが得られる。<br>

![image](https://user-images.githubusercontent.com/25688193/51423453-b80f4980-1c03-11e9-8659-9cd59cdb2433.png)<br>


<a id="TD学習（時間的差分学習）"></a>

## ■ TD学習（時間的差分学習）
先のモンテカルロ法は、モデルフリーの強化学習でモデルを必要としないが、実際の環境が生成したエピソードの開始から終端までで繰り返しサンプリングを行うために、エピソードの完了までに待たなくては計算可能とならない問題が存在する。<br>

一方、動的計画法は、ブーストラップ性によりエピソードの完了を待たずとも計算可能となるが、モデルベースの強化学習手法であり、状態遷移確率の形が具体的に与えられていないと、計算が出来ないという問題が存在する。<br>

この両者（MC法、DP法）の利点を織り込んだ最適価値関数の推定手法として、TD学習と呼ばれるモデルフリーの強化学習手法が存在する。<br>
具体的には、このTD学習では、状態遷移確率の具体的な形を必要せず、なおかつ、ブートストラップ性を用いて、エピソードの終端まで待たずに、価値関数の更新をオンライン型で逐次行うことが出来る。<br>
※ 但し、エピソードの完了を待たずに、逐次現時点での見積もりを行っていることになるため、１エピソード全体に渡っての見積もりを行うMC法と比べて、特に初期段階においては正確性に欠けることになる。<br>

<br>

ここで、オンライン型の推定値の計算の一般的な議論の例として、平均値の計算の取扱いについて見てみる。
報酬 ![image](https://user-images.githubusercontent.com/25688193/50372449-d177a280-0611-11e9-8721-b0385816271b.png) の平均値計算は、以下のようにして計算できる。<br>
![image](https://user-images.githubusercontent.com/25688193/50372454-e0f6eb80-0611-11e9-8b73-1f8778b0e417.png)<br>
この平均値計算式の形式では、新しいデータが追加されるたびに、和の操作を再度実行する必要があるために、計算負荷が大きくなりやすいという問題が存在する。即ち、<br>
![image](https://user-images.githubusercontent.com/25688193/50372466-0d126c80-0612-11e9-86a9-e56599079ada.png)<br>
従って、この式を、前回の平均値と最新のデータから、新しい平均値を算出するという漸近式の表現に書き直すと、<br>
![image](https://user-images.githubusercontent.com/25688193/50372472-2c10fe80-0612-11e9-995c-dec39cf37e0f.png)<br>
即ち、平均値の計算を、漸近式の形で表現すると、以下のような形式となる。<br>

- 新しい推定値 ← 古い推定値＋ステップサイズ ✕ [最新の更新データ ー 古い推定値]<br>

この漸近式の表現では、新しいデータに対して、都度平均値計算をし直す必要はなく、オンライン型の手続きが可能となり、計算負荷が軽減されるといったメリットが存在する。<br>

<br>

次に、この漸近式の表現での価値関数の推定方法を、先の逐次訪問モンテカルロ法による価値推定の方法に適用してみることを考える。<br>
先のモンテカルロ法による価値推定の方法では、各状態 s に対する得られた収益のリスト Returns(s) を平均化したもので価値関数の推定を行っていたが、これを漸近式の形で表現すると、<br>

![image](https://user-images.githubusercontent.com/25688193/50373326-b8c2b900-0620-11e9-8263-32ab0a598a00.png)<br>

この収益 ![image](https://user-images.githubusercontent.com/25688193/50373274-aeec8600-061f-11e9-984b-a741f664ff03.png) を目標値とする漸近式の形で表現した、逐次訪問モンテカルロ法による価値推定手法を、アルファ不変MC法という。<br>
尚、このアルファ不変MC法は、漸近式の形で表現されているが、報酬の平均値である収益 ![image](https://user-images.githubusercontent.com/25688193/50373274-aeec8600-061f-11e9-984b-a741f664ff03.png) がエピソードの完了までまたないと計算できないため、漸近式のメリットであるオンライン型の手続きのメリットは享受できない。<br>


<a id="TD(0)法"></a>

### ◎ TD(0)法
TD学習では、このα不変MC法のように、エピソードの完了までまたないと計算できない収益 ![image](https://user-images.githubusercontent.com/25688193/50373274-aeec8600-061f-11e9-984b-a741f664ff03.png) ではなく、次のステップでの価値関数と報酬の推定値 ![image](https://user-images.githubusercontent.com/25688193/50373355-3be40f00-0621-11e9-8fb4-fa66eb1cf1f2.png) を使用する。<br>
即ち、![image](https://user-images.githubusercontent.com/25688193/50810918-63401900-134f-11e9-8c32-4b632e0d326f.png) の１ステップでの関係式を、先の不変MC法の式 ![image](https://user-images.githubusercontent.com/25688193/50373474-f45e8280-0622-11e9-8298-7d58f29bc1bd.png) の収益 ![image](https://user-images.githubusercontent.com/25688193/50373274-aeec8600-061f-11e9-984b-a741f664ff03.png) の項に入力した式<br>

![image](https://user-images.githubusercontent.com/25688193/50810484-38ed5c00-134d-11e9-8b26-c566551e2c2e.png)<br>
により、価値関数の推定値の更新を行う。<br>
このような１ステップ間での直近の価値関数の推定値を用いる最も単純なTD法を、TD(0)法と呼ぶ。<br>

以下の図は、モンテカルロ法とTD(0)のバックアップ線図を比較した図である。<br>

![image](https://user-images.githubusercontent.com/25688193/51425218-717a1900-1c1c-11e9-897a-219abd875522.png)<br>

この図から分かるように、このTD(0)法では、α不変MC法と比較すると、![image](https://user-images.githubusercontent.com/25688193/50373489-32f43d00-0623-11e9-9219-a98a40f764a4.png) を目標値として更新を行うが、この際に、α不変MC法とは異なり、エピソードの完了まで待たなくとも、次のステップ t+1 まで待つだけでよい。<br>
※ この TD(0)法では、上図のバックアップ線図で示したように、１ステップでのバックアップ処理の式になっているので、１ステップTD法であるとも言える（詳細は後述）<br>

![image](https://user-images.githubusercontent.com/25688193/51423462-e4c36100-1c03-11e9-8fd3-12f9f62598ed.png)<br>

> 【Memo】<br>
> - TD法の収束性<br>
>   TD法のアルゴリズムでは、如何なる定常方策 π に対しても、ステップサイズ α が十分小さな値定数であれば、推定値 V(s) が価値関数 ![image](https://user-images.githubusercontent.com/25688193/50376139-6a2b1400-064c-11e9-8438-31d254a79957.png) へ収束されることが保証されている。<br>
>	但し、この収束性の保証は、ほとんどの場合、テーブル形式のTD法（＝テーブルTD(0)法）で適用されるものである。<br>
> - TD法とMC法の収束速度<br>
>   TD法とMC法は、両方とも正しい価値関数の値に収束するが、どちらがより速く収束するのか？という疑問に対する数学的な証明は、現時点では未解決である。<br>
>	しかしながら、例えば、ランダムウォークのような確率的なタスクにおいては、TD法のほうが、アルファ不変MCより速く収束することが知られている。（詳細略）<br>

- 【参考サイト】<br>
    - [今さら聞けない強化学習（9）: TD法の導出](https://qiita.com/triwave33/items/277210c7be4e47c28565)<br>
    - [強化学習について学んでみた。（その18）](http://yamaimo.hatenablog.jp/entry/2015/10/15/200000)



<a id="Sarsa"></a>

### ◎ Sarsa（方策オン型TD制御）
ここでは、TD法を制御問題に適用する方法について見ていく。<br>
TD法を制御問題に適用した手法には、大きく分けて、方策オン型TD制御手法と方策オフ型TD制御手法の２つが存在する。<br>
Sarsa は、このうち方策オン型TD制御手法に分類される手法の１つであり、先のTD法と ε-greedy 手法を組み合わせた手法である。<br>
より具体的には、行動価値関数の推定をTD法によって行い、エピソードの更新（＝行動選択）を ε-greedy な行動方策によって行う手法である。<br>

<br>

まず、行動選択に関する ε-greedy 手法について見ていく。<br>
行動価値関数 Q(s,a) に関してグリーディな行動方策とは、![image](https://user-images.githubusercontent.com/25688193/50381321-74d6bf00-06c7-11e9-9cc1-638686addf68.png) となるような行動 a のことであったが、このグリーディーな行動方策では、常にその時点での最適解を模索するため、局所的最適解に陥る恐れが存在する。<br>
そこで、局所的最適解を脱出して大域的最適解に移動できるように、リスクをとって、一定の確率 ε でランダムな行動をとるよな行動方策を考える。この行動方策を ε-greedy 手法という。<br>

<br>

Sarsa では、行動価値関数の推定をTD法によって行うが、TD法による行動価値関数の更新と推定は、以下のような漸近式で表現されるのであった。<br>
![image](https://user-images.githubusercontent.com/25688193/50381318-583a8700-06c7-11e9-854f-aedaef25bb78.png)<br>

記号の簡単化のため、t の添字なしで書き換えると、以下のようになる。<br>
![image](https://user-images.githubusercontent.com/25688193/50888440-002ab100-1439-11e9-87f3-6caebf530664.png)<br>

Sarsa では、この更新式により、推定する行動価値関数の更新を行う。<br>
※ Sarsa というアルゴリズムの名前は、上式の推定式の右辺の変数 s,a,r′,s′,a′ の頭文字を繋げたものに由来する。<br>
そして、エピソードの更新（＝行動選択）は、ε-greedy 手法に従った行動方策によって行う。<br>

以下の図は、Sarsa のバックアップ線図である。<br>

![image](https://user-images.githubusercontent.com/25688193/51425232-93739b80-1c1c-11e9-8c10-cfdaca1098a1.png)<br>

上図から分かるように、この Sarsa は、ε-greedy な行動方策によってエピソードの更新（＝行動選択）が行われ、それに伴って次の状態 s',a' が定まり、価値関数の更新が行われることになる。<br>
即ち、Sarsa では、実際に進む行動（＝エピソードを更新する行動）と価値関数の推定値の更新に用いる行動が同様となる。<br>
そして、このような制御手法を、方策オン型TD制御という。<br>

この Sarsa をアルゴリズムとしてまとめると、以下のようになる。<br>

![image](https://user-images.githubusercontent.com/25688193/51423505-982c5580-1c04-11e9-974b-641dc7167ea0.png)<br>


<a id ="Sarsaの適用例"></a>

#### ☆ Sarsa の適用例

- （例）Sarsa を利用した単純な迷路探索問題<br>
    > 記載中...

    尚、本適用例の実装コードは、以下のサイトに保管してあります。<br>
    [GitHub/Yagami360/ReinforcementLearning_Exercises/MazeSimple_Sarsa/](https://github.com/Yagami360/ReinforcementLearning_Exercises/tree/master/MazeSimple_Sarsa)


<a id="Q学習"></a>

### ◎ Q学習 [Q-learning]（方策オフ型TD制御）
先に見た Sarsa は、エピソードの更新（＝行動選択）と価値関数の更新の双方が、共に同じ ε-greedy な行動方策に従って行われる方策オン型TD制御アルゴリズムであった。<br>

一方、Q学習は、エピソードの更新（＝行動選択）と価値関数の更新の双方が、必ずしも同じ行動方策になるとは限らない方策オン型TD制御アルゴリズムになっている。<br>

まず、Q学習における、価値関数の推定値の更新式は、以下のようになる。<br>
![image](https://user-images.githubusercontent.com/25688193/50888533-3c5e1180-1439-11e9-88cb-acec148af417.png)<br>
この式から分かるように、Sarsa とは異なり、Q学習では、価値関数の値が max となるようなグリーディーな行動選択を一律に行う。<br>
（※ Sarsa では、ε-greedy な行動方策に従って、一定のランダム性を織り込んだ行動選択であった。）<br>
一方、エピソードの更新（＝行動選択）は、Sarsa と同様にして、ε-greedy 手法に従った行動方策によって行う。<br>

以下の図は、Q学習のバックアップ線図である。<br>

![image](https://user-images.githubusercontent.com/25688193/51425243-ba31d200-1c1c-11e9-84f0-1557091b8dd8.png)<br>

上図から分かるように、（推定値の更新式に含まれる項である）![image](https://user-images.githubusercontent.com/25688193/50384493-afad1700-0708-11e9-9600-941f4df96415.png) によるグリーディーな行動選択と、ε-greedy な行動方策によるエピソードの更新（＝行動選択）とは、必ずしも一致するとは限らない。<br>
即ち、Q学習では、実際に進む行動（＝エピソードを更新する行動）と価値関数の推定値の更新に用いる行動が異なる。<br>
そして、このような制御手法を、方策オフ型TD制御という。<br>

このQ学習をアルゴリズムとしてまとめると、以下のようになる。<br>

![image](https://user-images.githubusercontent.com/25688193/51423530-fe18dd00-1c04-11e9-91a2-b713c500f3ce.png)<br>

> 【Memo】<br>
> - SarsaとQ学習のの収束性の比較<br>
>   行動選択に関しては、SarsaとQ学習は共に ε-greedy によるランダム性をもった行動選択となるが、価値関数の推定値の更新に関しては、Sarsa が ε-greedy ランダム性が更新式に入る一方で、Q学習では、![image](https://user-images.githubusercontent.com/25688193/50384493-afad1700-0708-11e9-9600-941f4df96415.png) のようにランダム性を持たない一律のグリーディーな行動選択となる。<br>
>   従って、ランダム性がない分、Q学習のほうが Sarsa より収束が早い傾向がある。<br>


- 【参考サイト】<br>
    - [今さら聞けない強化学習（10）: SarsaとQ学習の違い](https://qiita.com/triwave33/items/cae48e492769852aa9f1#_reference-745dd3cb44ecaa9c9752)<br>
    - [強化学習について学んでみた。（その19）](http://yamaimo.hatenablog.jp/entry/2015/10/16/200000)


<a id ="Q学習の適用例"></a>

#### ☆ Q学習の適用例

- （例）Q学習を利用した単純な迷路探索問題<br>
    > 記載中...

    尚、本適用例の実装コードは、以下のサイトに保管してあります。<br>
    [GitHub/Yagami360/ReinforcementLearning_Exercises/MazeSimple_Qlearning/](https://github.com/Yagami360/ReinforcementLearning_Exercises/tree/master/MazeSimple_Qlearning)


<a id="アクター・クリティック手法"></a>

### ◎ アクター・クリティック手法
アクター・クリティック手法は、TD法の１種であるが、これまでの強化学習手法とは異なる考え方に基づくアプローチであり、行動方策に基づく行動と方策評価のプロセスを、それぞれアクターとクリティックという２つの機構に分離して強化学習をモデル化している。<br>

![image](https://user-images.githubusercontent.com/25688193/51103039-a7865a00-1824-11e9-9e70-0db2eac26393.png)<br>

- アクター（行動器）[actor]：<br>
    行動方策 ![image](https://user-images.githubusercontent.com/25688193/50441875-081a2980-0940-11e9-9edb-7faff8a50fa5.png) に基づき、行動を選択する機構。<br>
    行動方策は、以下のような softmax 関数で選択される。<br>
    ![image](https://user-images.githubusercontent.com/25688193/51118688-51310f80-1854-11e9-8ab5-fc3999e2c675.png)<br>

- クリティック（評価器）[critic]：<br>
    現在アクターが利用している行動方策 ![image](https://user-images.githubusercontent.com/25688193/50441875-081a2980-0940-11e9-9edb-7faff8a50fa5.png) に対する評価を行う機構。<br>


クリティックによる方策評価は、TD誤差 ![image](https://user-images.githubusercontent.com/25688193/51103169-21b6de80-1825-11e9-9dbb-5cc6c3883f5a.png) を、状態![image](https://user-images.githubusercontent.com/25688193/50442183-161c7a00-0941-11e9-95e2-c20d8df60afd.png)で行われた行動![image](https://user-images.githubusercontent.com/25688193/50442241-58de5200-0941-11e9-8757-a4c9bddb8c28.png)が選択される確率である ![image](https://user-images.githubusercontent.com/25688193/51105587-c38df980-182c-11e9-84b0-d39e8b8ab1d1.png) への評価として利用することで行われる。<br>
即ち、TD誤差の値に応じて、![image](https://user-images.githubusercontent.com/25688193/51105587-c38df980-182c-11e9-84b0-d39e8b8ab1d1.png) を以下のようにを増減させる。<br>

![image](https://user-images.githubusercontent.com/25688193/51105730-254e6380-182d-11e9-9ed1-66a801a29c4b.png)<br>


<a id="nステップTD法"></a>

### ◎ nステップTD法
先のTD(0)では、価値関数の更新式（＝漸化式）![image](https://user-images.githubusercontent.com/25688193/50813583-39d8ba80-135a-11e9-8b28-e7975febabb1.png) の式での収益 R おいて、１ステップでの関係式 ![image](https://user-images.githubusercontent.com/25688193/50811225-c2eaf400-1350-11e9-8a6d-e27bf744e286.png) を代入した式<br>
![image](https://user-images.githubusercontent.com/25688193/50813606-4bba5d80-135a-11e9-8ebd-52ee8bfea10f.png)<br>
つまりは、１ステップバックアップで価値関数の推定を行っていた。<br>
一方、モンテカルロ法では、エピソードの開始から終端までの全ての系列に基づいてバックアップを行っていた。<br>

nステップTD法は、この１ステップバックアップであるTD(0)法と、全ステップバックアップであるモンテカルロ法の中間のバックアップであり、n ステップバックアップで価値関数の推定を行う（下図参照）<br>
※ 尚、先の TD(0) 学習は、このnステップTD法の範疇にあり、n=1 のときの手法となる。<br>

![image](https://user-images.githubusercontent.com/25688193/50812093-aa7cd880-1354-11e9-8827-c8da138ef3e1.png)<br>

式で書くと、TD学習における、価値関数の更新式（＝漸化式）<br>
![image](https://user-images.githubusercontent.com/25688193/50813583-39d8ba80-135a-11e9-8b28-e7975febabb1.png)<br>
の収益 R おいて、TD(0) 法では、１ステップでの関係式 ![image](https://user-images.githubusercontent.com/25688193/50811225-c2eaf400-1350-11e9-8a6d-e27bf744e286.png) を代入していたのに対して、
nステップTD法では、nステップ先での関係式<br>
![image](https://user-images.githubusercontent.com/25688193/50813756-c1bec480-135a-11e9-9775-a6b29d5feaff.png)<br>
に拡張して代入する。即ち、価値関数の推定値の更新式は、<br>
![image](https://user-images.githubusercontent.com/25688193/50813785-dbf8a280-135a-11e9-8646-88d414c25763.png)<br>
となる。<br>


<a id="TD(λ)法"></a>

### ◎ TD(λ)法
> 記載中...


<a id="TD(λ)の前方観測的な見方"></a>

#### ☆ TD(λ)の前方観測的な見方
一般的に、TD法におけるバックアップ<br>
![image](https://user-images.githubusercontent.com/25688193/50830843-60174e00-138c-11e9-9e0e-f3c0922a6190.png)<br>
の収益項（＝目標値）![image](https://user-images.githubusercontent.com/25688193/50831087-24c94f00-138d-11e9-8ddf-cd7c069e543e.png)は、nステップ収益に対してのみだけではなく、nステップ収益の平均値に対しても適用でき、この平均化された収益で、上式に従って価値推定を行うことが出来る。<br>

TD(λ) アルゴリズムは、このような、nステップバックアップを平均化した収益で価値推定を行うアルゴリズムである。<br>
詳細には、各 n ステップ収益 ![image](https://user-images.githubusercontent.com/25688193/50831112-3c083c80-138d-11e9-95dd-d83354dc87c7.png) を ![image](https://user-images.githubusercontent.com/25688193/50831167-61954600-138d-11e9-80b2-bdcd55e62a2e.png) で重み付け平均化した収益<br>
![image](https://user-images.githubusercontent.com/25688193/50832622-1d587480-1392-11e9-88a9-742f039a20f7.png)<br>
を収益として利用して、価値推定を行う。<br>
※ この式において、λ=0 とすると、![image](https://user-images.githubusercontent.com/25688193/50833402-64476980-1394-11e9-81d1-60fe6f168bd0.png) となるので１ステップ収益となり、TD(0) に帰着する。<br>
※ 又、λ=1 とすると、![image](https://user-images.githubusercontent.com/25688193/50833442-7d501a80-1394-11e9-9a5c-797444fa146c.png) となり、アルファ不変MCに帰着する。<br>

この重み付けの様子を、バックアップ線図とともに図示したものが以下の図である。<br>
![image](https://user-images.githubusercontent.com/25688193/50832750-7b855780-1392-11e9-8dbe-dea119e73f94.png)<br>

ここで、この重み付け平均化した利得 ![image](https://user-images.githubusercontent.com/25688193/50874105-0a7f8780-1406-11e9-9290-b6f8e7a54df5.png) の計算は、現在の時間 t から将来の時間 t+1,t+2,... に渡っての重み付け平均化処理であり、時間の経過方向 t→t+1→t+2→... に沿って観測しているので、前方観測的見方となっていることが分かる。<br>


<a id="TD(λ)の後方観測的な見方と適格度トレース"></a>

#### ☆ TD(λ)の後方観測的な見方と適格度トレース
先に見たTD(λ)の前方観測的見方では、<br>
![image](https://user-images.githubusercontent.com/25688193/50875424-a1027780-140b-11e9-8213-63ac5ad072c8.png)<br>
の式に従って、重み付け平均化された収益 ![image](https://user-images.githubusercontent.com/25688193/50874105-0a7f8780-1406-11e9-9290-b6f8e7a54df5.png) を算出したが、これは、現在の時間 t から将来の時間 t+1,t+2,... に渡っての重み付け平均化処理であり、時間の経過方向 t→t+1→t+2→... に沿って観測しているので、前方観測的見方となっている。<br>

しかしながら、前方観測的見方の方法では、時間が n ステップ経過後でないと、実際に利益 R_t^((n) )  が計算できないので、価値推定を行えないという問題が存在する。<br>

この問題を解決するために、前方観測的見方を反転し、時間の経過方向とは逆の方向からみた後方観測的見方からTD(λ)を考え直す。<br>
より詳細には、今考えているTD(λ)において、後方の時間ステップ t+n から見た、TD誤差（＝現在の推定価値との差分）![image](https://user-images.githubusercontent.com/25688193/50882880-9ce55280-1429-11e9-969d-118b3ba459ac.png) が、時間ステップ t での価値推定 ![image](https://user-images.githubusercontent.com/25688193/50879411-20994200-141e-11e9-9127-d45200ac8651.png) にどの程度（の重みで）影響を与えているのか？ということを考える。<br>

このことを示したのが以下の図である。<br>
> 図を要追加...

図から分かるように、TD(λ)において、各時間ステップ t+1,t+2,... でのTD誤差は、nステップ前の価値の推定に、![image](https://user-images.githubusercontent.com/25688193/50879475-67873780-141e-11e9-9d02-cc334a893ac1.png) の重みで影響を与えているということが分かる。<br>

従って、![image](https://user-images.githubusercontent.com/25688193/50879475-67873780-141e-11e9-9d02-cc334a893ac1.png) に比例した重みで影響を与えるような以下のような適格度トーレスなるものを考える。<br>

![image](https://user-images.githubusercontent.com/25688193/50879553-a4532e80-141e-11e9-810e-f751702b9728.png)<br>

- この適格度トーレスは、<br>
    「時間ステップ t において、次の状態 ![image](https://user-images.githubusercontent.com/25688193/50879575-b765fe80-141e-11e9-9392-32410b2404ff.png) を観測した際に、ある状態 s∈S の価値 V(s) に反映させるための、現在状態との価値の差分 ![image](https://user-images.githubusercontent.com/25688193/50879374-0b241800-141e-11e9-8b4d-6c565c68f6eb.png) に対しての重み」<br>
    を表しており、全ての状態 ∀s∈S に対して定義される。<br>
- より端的・包括的に言い換えると、<br>
    「強化事象が発生したときに、各状態が学習上の変化を受けることが、”適格” であることの度合い」<br>
    を表しており、その意味で、適格度トレースという。<br>

この適格度トレースは、価値関数に反映させるための、TD誤差 ![image](https://user-images.githubusercontent.com/25688193/50882880-9ce55280-1429-11e9-969d-118b3ba459ac.png) に対しての重みになっているので、この２つ（＝TD誤差と適格度トレース）を乗算した式から、TD(λ)の価値推定（＝価値関数の更新）を行う。<br>
即ち、<br>
![image](https://user-images.githubusercontent.com/25688193/50883038-25fc8980-142a-11e9-860a-dac9da3f8917.png)<br>

<br>

- 【参考サイト】<br>
    - [強化学習について学んでみた。（その25）](http://yamaimo.hatenablog.jp/entry/2015/12/12/200000)<br>


<!--
<a id="前方観測的見方と後方観測的見方の等価性"></a>

#### ☆ 前方観測的見方と後方観測的見方の等価性
> 記載中...
-->

<a id="Sarsa(λ)"></a>

#### ☆ Sarsa(λ)
Sarsa に対して、TD(λ) と同様にして、適格度トレースを導入したものを Sarsa(λ) という。<br>
これは、単純に、Sarsaにおける価値推定の更新式<br>
![image](https://user-images.githubusercontent.com/25688193/50887888-aa093e00-1437-11e9-8423-c35c3b0af521.png)<br>
を、適格度トレースを使用した以下のような価値推定の更新式に置き換えるものである。<br>
![image](https://user-images.githubusercontent.com/25688193/50887916-bbeae100-1437-11e9-981f-9f2057fa83d1.png)<br>

この Sarsa(λ) をアルゴリズムとしてまとめると、以下のようになる。<br>

![image](https://user-images.githubusercontent.com/25688193/51423533-15f06100-1c05-11e9-8b45-9306d39744c1.png)<br>


<a id="Q(λ)"></a>

#### ☆ Q(λ)
Q学習に対しても、適格度トレースの考えを適用することを考える。<br>
このような手法は、Q(λ) と呼ばれ、”Watkins のQ(λ)” と ”Peng のQ(λ)” の２つの手法が存在する。<br>

ここで、Q学習は、（推定値の更新式に含まれる項である）![image](https://user-images.githubusercontent.com/25688193/50902154-7b05c300-145d-11e9-856d-59d61fbe1354.png) によるグリーディーな行動選択と、ε-greedy な行動方策によるエピソードの更新（＝行動選択）とが、必ずしも一致するとは限らない方策オフ型TD制御であった。<br>
そのため、方策オン型TD制御である Sarsa(λ) で単純に適格度トレースを導入出来たのとは異なり、Q学習に適格度トレースを導入する際には、特別な対応を考える必要がある。<br>

ここで、このQ学習に適格度トレースを導入する際の特別な対応というのは、具体的には以下のような事項となる。<br>

- Q(λ)におけるバックアップ処理のステップ数：<br>
    Q(λ)におけるバックアップ処理では、価値関数の更新でのグリーディ行動と、εーgreedy な行動選択とが一致しない（＝非グリーディ行動が選択）場合は、その時点で終了させる。<br>
    最初の非グリーディ行動を nステップ目の a_(t+n)  として、式で表現すると、<br>
    ![image](https://user-images.githubusercontent.com/25688193/50906295-23b92000-1468-11e9-832e-c1f42d163d35.png)<br>
    一方、常にグリーディ行動が選択され続けた場合は、エピソードの終端までバックアップ処理する。<br>
    エピソードの終端を t=T として、式で表現すると、<br>
    ![image](https://user-images.githubusercontent.com/25688193/50906336-35022c80-1468-11e9-9d3b-4ae5466e1591.png)<br>

> 記載中...


<a id="アクタークリティック手法への適格度トレースの適用"></a>

### ◎ アクタークリティック手法への適格度トレースの適用
アクター・クリティック手法に対しても、TD(λ) の後方観測的見方と同様にして、前方観測的見方→後方観測的見方に置き換えて、適格度トレースを導入することが出来る。<br>

まず、アクター・クリティック手法（＝厳密には、１ステップ・アクター・クリティック手法）では、状態 s で行動 a を選択する確率 ![image](https://user-images.githubusercontent.com/25688193/51105587-c38df980-182c-11e9-84b0-d39e8b8ab1d1.png) を、以下のように増減させていた。<br>
![image](https://user-images.githubusercontent.com/25688193/51118727-6c038400-1854-11e9-9602-92e174ad240c.png)<br>

この更新式は、TD(λ) での推定価値関数の更新式 ![image](https://user-images.githubusercontent.com/25688193/51118794-8b9aac80-1854-11e9-9fab-f49cca8e437b.png) に対応したものになっているので、TD(λ) の後方観測的見方での推定価値関数の更新式<br>
![image](https://user-images.githubusercontent.com/25688193/51118840-a4a35d80-1854-11e9-93e7-78e8bbce8051.png)<br>
と同様にして、以下のように、適格度トレースを用いて書き換えられる。<br>
即ち、<br>
![image](https://user-images.githubusercontent.com/25688193/51118869-b8e75a80-1854-11e9-918a-a2ef4f5b21e7.png)<br>


<a id="関数による価値関数の近似（関数近似手法）"></a>

## ■ 関数による価値関数の近似（関数近似手法）
これまで見てきた各種強化学習アルゴリズムでは、いずれも、離散的な状態空間 ![image](https://user-images.githubusercontent.com/25688193/51009809-fa49e280-1594-11e9-8758-79028000c2a3.png) 、及び、離散的な行動空間 ![image](https://user-images.githubusercontent.com/25688193/51009845-1baace80-1595-11e9-9729-8fd26deba066.png) に対して、<br>
状態価値関数 V(s) の各要素<br>
![image](https://user-images.githubusercontent.com/25688193/51009871-32e9bc00-1595-11e9-83d5-9b413b617683.png)<br>
或いは、行動価値関数 Q(s,a)  の各要素<br>
![image](https://user-images.githubusercontent.com/25688193/51009888-4137d800-1595-11e9-956e-936ed24aa20c.png)<br>
の推定を行っていた。<br>

※ 下図は、状態価値関数 V(s) の離散的な各状態 s∈S に対しての、各要素の推定値の模式図。行動価値関数 Q(s,a) では、離散的な各状態行動対 (s,a)∈S×A に対しての３次元プロットになる。<br>

![image](https://user-images.githubusercontent.com/25688193/51009936-6a586880-1595-11e9-848a-f3a7bf4feeeb.png)<br>

そして、このような処理を実現するために、これらの離散的な状態空間、離散的な行動空間に対しての、価値関数の各要素の推定値<br>
![image](https://user-images.githubusercontent.com/25688193/51009998-9e338e00-1595-11e9-945c-eaedcddb123e.png)<br>
をそのままメモリ上に保管し、価値関数の推定値の更新処理を行っていた。<br>
このような方式を、テーブル法という。<br>

しかしながら、このテーブル法では、多くの実際上の強化学習のタスクにあるように、状態数や行動数が膨大な数存在する場合、或いは、状態や行動が離散的ではなく連続である場合において、膨大なメモリを消費しい、又計算コストも膨大になるという問題が存在する。<br>

<br>

この問題を解決するための別の方法として、下図のように、真の価値関数を、離散的な状態行動対に対しての価値関数で推定するのではなくて、連続な関数で近似することを考える。このような手法を、関数近似手法という。<br>

![image](https://user-images.githubusercontent.com/25688193/51069981-05783d80-167d-11e9-9507-2ed745363cc2.png)<br>

まず、真の価値関数の関数近似を行う関数を、時間ステップ t に依存したパラメータ ![image](https://user-images.githubusercontent.com/25688193/51069984-19bc3a80-167d-11e9-8e22-3e258566740a.png) で表現される関数とする。<br>
即ち、時間ステップ t での価値関数を、以下のように近似することを考える。<br>

![image](https://user-images.githubusercontent.com/25688193/51069991-3ce6ea00-167d-11e9-95d3-5476d8cae690.png)<br>
※ この近似関数は、例えば、パーセプトロンの場合は、パラメータ ![image](https://user-images.githubusercontent.com/25688193/51069984-19bc3a80-167d-11e9-8e22-3e258566740a.png) 、各層のノード間の重みとなる。<br>
※ 別の例として、決定木で計算される関数を、この近似関数とすることも出来る。（この場合のパラメータ ![image](https://user-images.githubusercontent.com/25688193/51069984-19bc3a80-167d-11e9-8e22-3e258566740a.png) は、木の全ての分岐数と葉の数を決めるようなパラメータに相当する。）<br>

ここで、このパラメータ ![image](https://user-images.githubusercontent.com/25688193/51069984-19bc3a80-167d-11e9-8e22-3e258566740a.png) の要素数は、状態数 |S| に比べて、圧倒的に少なく、パラメータ１つを変更すれば、多くの価値関数の推定値が変更されることを想定している。<br>
これにより、テーブル方式でメモリ管理する手法に比べて、大幅にメモリを消費量を削減できるようになる。<br>

- 【参考サイト】<br>
    - [今さら聞けない強化学習(11) 線形関数による価値関数近似](https://qiita.com/triwave33/items/78780ec37babf154137d#_reference-dc3276a2c27b967c9689)<br>
    - [強化学習について学んでみた。（その28）](http://yamaimo.hatenablog.jp/entry/2016/01/07/200000)<br>
    - [強化学習について学んでみた。（その29）](http://yamaimo.hatenablog.jp/entry/2016/01/08/200000)<br>


<a id="損失関数としての最小２乗誤差（MSE）"></a>

### ◎ 損失関数としての最小２乗誤差（MSE）
強化学習における関数近似では、一般的に、損失関数 L として、最小２乗誤差（MSE）を考え、これが最小（※出来れば大域的最適解の意味での最小だが、一般的には、局所的最適解の意味での最小になることが多い）になるように、パラメータ ![image](https://user-images.githubusercontent.com/25688193/51069984-19bc3a80-167d-11e9-8e22-3e258566740a.png) の学習を行う。<br>
即ち、<br>
![image](https://user-images.githubusercontent.com/25688193/51460363-8da5c380-1d9e-11e9-885d-b5f8bb1fae1d.png)<br>

ここで、P(s) は、異なる状態 ![image](https://user-images.githubusercontent.com/25688193/51070027-ddd5a500-167d-11e9-8366-8bbf4f6ab20d.png) の誤差 ![image](https://user-images.githubusercontent.com/25688193/51070034-ef1eb180-167d-11e9-99cc-864888775d7e.png) に対して、重み付けを行う分布関数である。<br>
一般的に、全ての状態 s に対して、良い価値関数の近似を得ることは難しく、そのため、ある幾つかのの状態 s に対しては、良い近似とする代わりに、別の幾つかの状態の近似を犠牲にすることが多い。この状態に対しての重みの分布関数 P(s) を導入することにより、このようなトレードオフを表現することが出来る。<br>


<a id="パラメータの更新式としての最急降下法"></a>

### ◎ パラメータの更新式としての最急降下法
先に見たように、関数近似器による真の価値関数への関数近似を行うためには、損失関数としての最小２乗誤差（MSE）を採用し、これを最小化するようにパラメータを学習すれば良い。<br>
ここでは、このパラメータの学習規則であるパラメータの更新式を、最急降下法によって導く。<br>

最急降下法では、損失関数 L が最も変化する方向（＝勾配方向）に、パラメータを更新する。<br>
即ち、<br>
![image](https://user-images.githubusercontent.com/25688193/51460462-e7a68900-1d9e-11e9-8f8f-161f78990beb.png)<br>

ここで、この損失関数の勾配は<br>
![image](https://user-images.githubusercontent.com/25688193/51462463-ffccd700-1da3-11e9-917d-38846b853b80.png)<br>
と書けるので、先の最急降下法によるパラメーターの更新式は、以下のように書き換えられる。
![image](https://user-images.githubusercontent.com/25688193/51460618-4d931080-1d9f-11e9-891d-81dda6efb110.png)<br>

<br>

ここで重要なのは、このパラメータの更新式<br>
![image](https://user-images.githubusercontent.com/25688193/51070067-563c6600-167e-11e9-9735-e4d6389a49ca.png)<br>
に含まれる真の価値関数 ![image](https://user-images.githubusercontent.com/25688193/51070074-68b69f80-167e-11e9-834d-1d3506b5891a.png) は、不明であるということである。<br>
これが、教師あり学習と強化学習の大きな違いの１つであり、強化学習では正解となる真の価値関数という教師データが与えられておらず、教師なし学習となっている。<br>

しかしながら、このパラメータの更新式に従って、パラメータの学習を進めていくには、何かしらの目標となる値が必要であることには変わりない。<br>
つまり、真の価値関数 ![image](https://user-images.githubusercontent.com/25688193/51070074-68b69f80-167e-11e9-834d-1d3506b5891a.png) ではなく、何かしらの目標値 ![image](https://user-images.githubusercontent.com/25688193/51070082-97347a80-167e-11e9-9be8-c5bb6454a19c.png) に従って、パラメータの学習を進めていく必要がある。<br>
即ち、<br>
![image](https://user-images.githubusercontent.com/25688193/51070087-ad423b00-167e-11e9-8c77-439cb0f439d7.png)<br>

この何かしらの目標値 ![image](https://user-images.githubusercontent.com/25688193/51070082-97347a80-167e-11e9-9be8-c5bb6454a19c.png) として良い目標値を与えるのが、先に見てきたモンテカルロ法やTD法などの各種強化学習手法である。<br>

例えば、モンテカルロ法での、![image](https://user-images.githubusercontent.com/25688193/51071132-99530500-168f-11e9-8fbe-1416c49963d5.png) の更新式で用いた ![image](https://user-images.githubusercontent.com/25688193/51071137-abcd3e80-168f-11e9-9814-55d3b3825dd5.png) を目標値として利用できる。（![image](https://user-images.githubusercontent.com/25688193/51071143-bf78a500-168f-11e9-8ccc-49e6aed301ec.png)）<br>
このとき、パラメータの学習の更新式は、以下のようになる。<br>
![image](https://user-images.githubusercontent.com/25688193/51071174-1d0cf180-1690-11e9-9af0-1291c30cd463.png)<br>
※ この最急降下法を用いたモンテカルロ状態価値推定は、偏りのない真の価値関数の推定値となるので、局所的最適解の発見が保証される。<br>

同様にして、TD(λ)での、各 n ステップ収益 ![image](https://user-images.githubusercontent.com/25688193/51071183-2d24d100-1690-11e9-8cd8-246fc04d3ef8.png) を重み付け平均化した収益<br>
![image](https://user-images.githubusercontent.com/25688193/51071187-40d03780-1690-11e9-96c1-5aa78cd0048c.png)<br>
も目標値として利用できる。（![image](https://user-images.githubusercontent.com/25688193/51071199-59d8e880-1690-11e9-8575-41b20c19f8f5.png)）<br>
このとき、パラメータの学習の更新式は、以下のようになる。<br>
![image](https://user-images.githubusercontent.com/25688193/51071205-69f0c800-1690-11e9-8756-e8f19c3479b1.png)<br>
※ この最急降下法に基づく TD(λ)での状態価値推定は、（モンテカルロ法とは異なり）偏りのない真の価値関数の推定値とならないので、局所的最適解の発見が保証されない。<br>
※ 但し、特別なケースにおいて、この局所的最適解とは異なる面での性能保証が得られる。その意味でも、このTD(λ)のようなブーストラップ手法は、重要なものとなる。（詳細は後述）<br>

ここで、この最急降下法に基づく TD(λ)での状態価値推定は、時間ステップ t→t+1→t+2 方向で更新される式であり、前方観測的な見方の式になっているが、先に見たように、これを後方観測的見方で書き換えることも出来る。即ち、最急降下法に基づく TD(λ) の後方観測的見方は、以下の更新式で与えられる。<br>
![image](https://user-images.githubusercontent.com/25688193/51081241-71ff4500-172e-11e9-87f0-46a45fdfdb74.png)<br>


<a id="線形関数での価値関数の近似（線形手法）"></a>

### ◎ 線形関数での価値関数の近似（線形手法）
線形手法は、関数近似器 ![image](https://user-images.githubusercontent.com/25688193/51073370-35423800-16b3-11e9-91b6-4ae31ad5bafd.png) を、特徴空間 ![image](https://user-images.githubusercontent.com/25688193/51073372-468b4480-16b3-11e9-8a09-b40e8c8255f8.png) 内でのパラメータ ![image](https://user-images.githubusercontent.com/25688193/51073376-5f93f580-16b3-11e9-9a60-5dc8c2bfc4b3.png) に関しての線形関数で表現する手法である。<br>
※ 特徴空間 ![image](https://user-images.githubusercontent.com/25688193/51073372-468b4480-16b3-11e9-8a09-b40e8c8255f8.png) 内でパラメータ ![image](https://user-images.githubusercontent.com/25688193/51073376-5f93f580-16b3-11e9-9a60-5dc8c2bfc4b3.png) に対して線形なのであって、単に状態価値空間 S✕A での線形関数ではないことに注意。<br>

より詳細には、関数近似器を、特徴写像後のベクトル ![image](https://user-images.githubusercontent.com/25688193/51073412-d7fab680-16b3-11e9-8cf9-36cba4d57552.png) とパラメータ ![image](https://user-images.githubusercontent.com/25688193/51073376-5f93f580-16b3-11e9-9a60-5dc8c2bfc4b3.png) との線形結合で表現する。<br>
![image](https://user-images.githubusercontent.com/25688193/51073426-13958080-16b4-11e9-9ca6-6da5928d858d.png)<br>
※ この特徴写像後のベクトル（＝特徴ベクトル）![image](https://user-images.githubusercontent.com/25688193/51073449-52c3d180-16b4-11e9-8ffb-2036ab2cbe8d.png) は、特徴空間 ![image](https://user-images.githubusercontent.com/25688193/51073372-468b4480-16b3-11e9-8a09-b40e8c8255f8.png) を張る基底ベクトルになる。<br>

> 【Memo】 線形手法とカーネル法、再生核ヒルベルト空間の関係<br>
> この線形手法で用いられている手法は、カーネル法で用いられる手法と同じ。<br>
> 特徴空間 ![image](https://user-images.githubusercontent.com/25688193/51073372-468b4480-16b3-11e9-8a09-b40e8c8255f8.png) を再生核ヒルベルト空間 H の言葉で、より厳密・より一般的に言い換え出来きそう。<br>

<br>

次に、このようなパラメータに関しての線形関数で表現された関数近似器に、最急降下法を適用することを考える。即ち、<br>
![image](https://user-images.githubusercontent.com/25688193/51073641-87855800-16b7-11e9-8f2a-037e8a6aae17.png)<br>

従って、最急降下法によるパラメータの更新式<br>
![image](https://user-images.githubusercontent.com/25688193/51073478-a0d8d500-16b4-11e9-832c-b017557b0f2c.png)<br>
は、特徴ベクトルを用いた、以下のような簡単な形へと帰着される。<br>
![image](https://user-images.githubusercontent.com/25688193/51073481-c36aee00-16b4-11e9-9dee-cdc60265281c.png)<br>

更に、この特徴空間内でのパラメータ ![image](https://user-images.githubusercontent.com/25688193/51073376-5f93f580-16b3-11e9-9a60-5dc8c2bfc4b3.png) に関しての線形関数 ![image](https://user-images.githubusercontent.com/25688193/51073590-80aa1580-16b6-11e9-81c7-fe2ea8e193a3.png) は、線形なので、局所的最適解 ![image](https://user-images.githubusercontent.com/25688193/51073599-a33c2e80-16b6-11e9-9786-c0ad577ec34d.png) は一意に定まり、それ故に大域的最適解となる。<br>
従って、最急降下法のように局所的最適解の存在やその近傍への収束を保証する手法ならば、この局所的最適解の収束性が、自動的に大域的最適解への収束を保証することになる。<br>

<br>

ここで、この特徴写像 ![image](https://user-images.githubusercontent.com/25688193/51073412-d7fab680-16b3-11e9-8cf9-36cba4d57552.png) の具体的な形としては、例えば、以下のようなRBFカーネルが用いられる。<br>
![image](https://user-images.githubusercontent.com/25688193/51081246-91966d80-172e-11e9-98c0-de768cc1e02e.png)<br>


<a id="最急降下型Sarsa（関数近似手法のSarsaへの適用）"></a>

### ◎ 最急降下型Sarsa（関数近似手法のSarsaへの適用）
行動価値関数 Q(s,a)  に対しての、最急降下法によるパラメーターの更新式は、状態価値関数のときと同様にして、以下のようになる。<br>
![image](https://user-images.githubusercontent.com/25688193/51081259-cb677400-172e-11e9-9a5e-700df986705a.png)<br>

この更新式は、前方観測的見方の式になっているが、後方観測的見方の式に書き換えると、状態価値関数のときと同様にして、<br>
![image](https://user-images.githubusercontent.com/25688193/51085665-f0d09e00-177f-11e9-8de4-4a33ef4d4ac2.png)<br>

この後方観測的見方のパラメーターの更新式は、Sarsa(λ) に対して、関数近似と最急降下法手法を適用したものになっているので、最急降下型 Sarsa(λ) という。（線形手法も適用したものは、線形最急降下型Sarsa）<br>
※ 厳密には、最急降下型 Sarsa(λ) の価値推定プロセス部分に相当。<br>

制御手法としての完全な最急降下型 Sarsa(λ) アルゴリズムを得るためには、この価値推定のプロセスだけでなく、方策評価・方策改善のプロセスも織り込む必要がある。<br>
しかしながら、このような作業は、行動集合 A が連続空間、或いは、離散的であっても要素数（＝行動数）が膨大な離散集合である場合は、困難となる。（現時点では、はっくりとした解法は提案されていない？）<br>
一方で、行動集合 A が離散的で、要素数（＝行動数）がそれほど大きくない場合においては、従来のテーブル方式でのSarsa(λ)と同じ方策評価・方策改善のプロセスが利用できる。<br>

これらの事項をまとめると、まず、最急降下型 Sarsa(λ) 価値推定アルゴリズムは、以下のようなアルゴリズムとなる。<br>

![image](https://user-images.githubusercontent.com/25688193/51085813-b7009700-1781-11e9-8796-7c917df350cc.png)<br>

次に、方策評価・方策改善のプロセスも織り込んだ、完全な最急降下型 Sarsa(λ) は、以下のようなアルゴリズムとなる。<br>

> 記載中...

<!--
<a id="最急降下型Q学習（関数近似手法のQ学習への適用）"></a>

### ◎ 最急降下型Q学習（関数近似手法のQ学習への適用）
先の最急降下型Sarsa(λ)において、Sarsa でのTD誤差の式を<br>
![image](https://user-images.githubusercontent.com/25688193/51086352-65f4a100-1789-11e9-9942-73205ad4ded4.png)<br>
Q学習でのTD誤差の式<br>
![image](https://user-images.githubusercontent.com/25688193/51086387-ab18d300-1789-11e9-801b-a672cf8e5822.png)<br>
に置き換えたものを、最急降下型Q学習という？<br>
即ち、最急降下型Q学習での、後方観測的見方でのパラメーターの更新式は、以下のようになる？<br>
![image](https://user-images.githubusercontent.com/25688193/51086395-d1d70980-1789-11e9-9f96-a4d0ef59998c.png)<br>

> 記載中...
-->

<a id="モデルとプランニング"></a>

## ■ モデルとプランニング（各種古典的強化学習の統一的見解）
これまで見てきた動的計画法、モンテカルロ法・TD法といった各種古典的強化学習手法は、モデルの有無や、プランニングという観点から、統一的に再解釈できる。<br>

- モデル：<br>
    まず、強化学習の文脈で、「環境のモデル」とは、環境に対するマルコフ決定過程 ![image](https://user-images.githubusercontent.com/25688193/51228297-3f0db900-199b-11e9-8b56-effdcf199b90.png) そのものであり、エージェントに対して状態行動対 (s,a)∈S×A が与えられたときに、結果として生じる、次の状態 s′ と報酬 r′ の予想は、このモデル（＝マルコフ決定過程）によって作り出される。<br>
    <br>
    ここで、各種強化学習手法は、この環境のモデルを考慮するか否かで種類分けすることが出来る。<br>
    - モデルベースの強化学習手法 [Model-Based RL]<br>
        ![image](https://user-images.githubusercontent.com/25688193/51236945-c4529700-19b6-11e9-8d4f-bf93f1316557.png)<br>
        行動を実行して環境から、経験を取得→それを元に環境のモデルを推定→その環境のモデル元に価値関数の推定や、最適方策の推定を行う強化学習手法。<br>

    - モデルフリーの強化学習手法 [Model-Free RL]<br>
        ![image](https://user-images.githubusercontent.com/25688193/51237003-e1876580-19b6-11e9-8bee-ba627014c394.png)<br>
        環境のモデルを考慮せず、実際の環境から、価値関数の推定や、最適方策の推定を行う強化学習手法。<br>
    
    例えば、動的計画法は、環境のモデルであるマルコフ決定過程 ![image](https://user-images.githubusercontent.com/25688193/51228297-3f0db900-199b-11e9-8b56-effdcf199b90.png) における状態遷移確率 ![image](https://user-images.githubusercontent.com/25688193/51228334-7714fc00-199b-11e9-8cb9-0e53bdfaf353.png) と報酬関数 ![image](https://user-images.githubusercontent.com/25688193/51228343-885e0880-199b-11e9-9d71-c8d9167d77d8.png) とから、次の状態 s' と報酬 r' が生成し、それらから価値関数や最適方策を推定するので、モデル有りの強化学習手法 [Model-Based RL] である？<br>
    一方、モンテカルロ法は、環境のモデルであるマルコフ決定過程 ![image](https://user-images.githubusercontent.com/25688193/51228297-3f0db900-199b-11e9-8b56-effdcf199b90.png) からではなく、実際の環境から経験的にサンプリングされる試行を利用して、価値関数や最適方策の推定を行うので、モデル無しの強化学習手法 [Model-Free RL] である？<br>

    モデルベース強化学習手法が、モデルフリー強化学習手法に比べて有利となるケースには、以下のようなケースが考えられる。<br>
    - 経験から直接的に価値関数を推定することが難しいようなタスク<br>
        ⇒ 例えば、チュスでは、状態数が膨大に存在し、現在の状態から、その状態での勝率といった、価値関数（に準ずるもの）を、経験により直接的に推定することは困難である。<br>
        一方、モデル有り強化学習手法では、モデルがチェスのゲームルールそのものになるため、モデルを用いたプランニング手法が有効となる？<br>
    - 教師有り学習<br>
        > 記載中...

- プランニング：<br>
    ![image](https://user-images.githubusercontent.com/25688193/51236558-ccf69d80-19b5-11e9-9561-71b7e9e3fa7b.png)<br>
	強化学習の文脈で、「プランニング」とは、上図のように、モデル（＝マルコフ決定過程）から、最終的には、最適な行動方策を推定することを意味しているが、<br>
	その途中過程では、モデルからシミュレーション上の経験を作り出し、それを元に次状態や次行動を予想して、最適価値関数を推定し、最終的には、最適な行動方策を推定するプロセスが含まれている。<br>

<br>

このようなモデルとプランニングのアーキテクチャは、下図に示したのように、これまで見てきた各種強化学習手法によく似た共通する構造をもっている。<br>
そして、この強化学習手法とプランニングとのよく似た構造が、強化学習手法とプランニング手法との間で、多くの考え方やアルゴリズムをやり取り出来ることに繋がる。<br>

![image](https://user-images.githubusercontent.com/25688193/51237051-011e8e00-19b7-11e9-98fa-8e3fe0ec7e8c.png)<br>

両者の相違点は、プランニングでは、モデルから生成した”シミュレーション上の経験”を用いるのに対して、
強化学習手法では、”環境が作り出した実際の経験”を用いる点である。<br>

- 【参考サイト】<br>
    - [強化学習理論の基礎4](https://qiita.com/YN6127yn/items/cb6047540a39b3bf15d7)


<a id="Dyna,Dyna-Q"></a>

### ◎ Dyna,Dyna-Q
Dyna は、モデルフリー強化学習手法とモデルベース強化学習手法を組み合わせたアルゴリズムである。<br>
より詳細には、実際の環境から生成されたサンプルを用いて、モデルを学習し、実際の環境から生成されたサンプルと、学習したモデルから生成したサンプルの双方を利用して、推定価値関数や推定最適方策をプランニングする。<br>

更に、Q学習に、この Dyna のアーキテクチャを織り込んだものが、Dyne-Q である。<br>


> 記載中...


<a id="深層学習の構造を取り込んだ強化学習手法"></a>

## ■ 深層学習の構造を取り込んだ強化学習手法（ニューラルネットワークによる関数近似）
> 記載中...

- 【参考サイト】<br>
    - [DQNからRainbowまで 〜深層強化学習の最新動向〜](https://www.slideshare.net/juneokumura/dqnrainbow)<br>
    - [DQNの生い立ち　＋　Deep Q-NetworkをChainerで書いた](https://qiita.com/Ugo-Nama/items/08c6a5f6a571335972d5)<br>
    - [最近のDQN](https://www.slideshare.net/mooopan/dqn-58511529)<br>
    - [DQNの理論説明](https://www.renom.jp/ja/notebooks/tutorial/reinforcement_learning/DQN-theory/notebook.html)
    - [これから強化学習を勉強する人のための「強化学習アルゴリズム・マップ」と、実装例まとめ](https://qiita.com/sugulu/items/3c7d6cbe600d455e853b)
    - [【星の本棚】パーセプトロン](http://yagami12.hatenablog.com/entry/2017/09/17/111935#ID_2)<br>
    - [【星の本棚】畳み込みニューラルネットワーク [CNN :Convolutional Neural Network]](http://yagami12.hatenablog.com/entry/2017/09/17/111935#ID_3)


<a id="多層パーセプトロン（MLP）による価値関数の近似"></a>

### ◎ 多層パーセプトロン（MLP）による価値関数の近似
先の関数近似の手法では、真の価値関数の関数近似を行う関数を、以下のような時間ステップ t に依存したパラメータ ![image](https://user-images.githubusercontent.com/25688193/51438501-73151100-1cf0-11e9-933d-6a9692c3bf66.png) で表現される関数とした。<br>
![image](https://user-images.githubusercontent.com/25688193/51438510-8b852b80-1cf0-11e9-82db-d124c3c1fa59.png)<br>

自然な流れとし、このパラメーター ![image](https://user-images.githubusercontent.com/25688193/51438501-73151100-1cf0-11e9-933d-6a9692c3bf66.png) を、ニューラルネットワークにおける各層間の重みとして採用することで、ニューラルネットワークで表現した関数近似器を採用することが考えられる。<br>

下図は、最も基本的なニューラルネットワークとしての多層パーセプトロンを、価値関数の関数近似器として採用したときの図である。<br>

![image](https://user-images.githubusercontent.com/25688193/51520282-9235af00-1e66-11e9-8e39-08c8cc1b41fe.png)<br>

ここで、上図のように、MLPに強化学習における関数近似手法を適用する際には、多層パーセプトロンへの入力信号・教師信号、及び、多層パーセプトロンからの出力信号は、以下のような設定となる。<br>

![image](https://user-images.githubusercontent.com/25688193/51526103-f7909c80-1e74-11e9-9a4e-cc58e9b3403e.png)<br>

多層パーセプトロン（MLP）では、この教師信号と出力信号との誤差から生じる損失関数の勾配を、逆伝搬させて、各層間の重みを逐次更新させる（＝誤差逆伝搬法）ことになるが、そのことを強化学習における関数近似に適用する場合にどうなるのかを、以下で順を追って見ていく。<br>

まず、MLPを強化学習における関数近似に適用する場合においては、教師信号と出力信号との誤差は、以下のようなQ学習でのTD誤差で表現される。<br>
![image](https://user-images.githubusercontent.com/25688193/51461398-4d941000-1da1-11e9-940c-883def6bbab2.png)<br>

強化学習における関数近似では、一般的に、損失関数として、最小２乗誤差（MSE）を考え、これが最小（※出来れば大域的最適解の意味での最小だが、一般的には、局所的最適解の意味での最小になることが多い）になるように、パラメータ ![image](https://user-images.githubusercontent.com/25688193/51438501-73151100-1cf0-11e9-933d-6a9692c3bf66.png) の学習を行った。<br>
ニューラルネットワークによる関数近似でも同様にして、先のTD誤差から作られる最小２乗誤差を損失関数として採用する。<br>
即ち、<br>
![image](https://user-images.githubusercontent.com/25688193/51462216-6f8e9200-1da3-11e9-8c0b-f253dceafe14.png)<br>

先に見た関数近似の手法では、このパラメータの学習規則であるパラメータの更新式を、最急降下法によって導いた。<br>
ニューラルネットワークによる関数近似でも同様にして、最急降下法によって出力層への重み（＝パラメーター）を更新する。<br>
![image](https://user-images.githubusercontent.com/25688193/51469617-192a4f00-1db5-11e9-9b3a-583f5047e21b.png)<br>

ここで、この損失関数の勾配は、<br>
![image](https://user-images.githubusercontent.com/25688193/51462558-3b67a100-1da4-11e9-87e8-1afc612d9aa9.png)<br>
と書けるので、最急降下法による出力層での重みの更新式は、以下のように書き換えられる。<br>
![image](https://user-images.githubusercontent.com/25688193/51462598-59cd9c80-1da4-11e9-8b70-25a533c2f0fd.png)<br>

そして、この最急降下法による出力層での重みの更新は、誤差逆伝搬で、出力層側から入力層側に向かって順次反映されていく。これにより、出力信号に教師信号（＝TD誤差）を与えるだけで、前段の層を含めたすべての層の重みを更新することが出来る。<br>
※ 誤差逆伝搬では、勾配（＝最急降下法）による重みベクトルの更新を逆方向に伝搬させるために、損失関数の勾配をいわゆる連鎖率の計算で求めていく。<br>
※ この連鎖率での微分計算を効率よく計算していくための、数値解析的な手法として、自動微分が使われる。<br>


> 【Memo】誤差逆伝搬法の詳細計算は、以下のサイトを参考<br>
> - [誤差逆伝播法](http://yagami12.hatenablog.com/entry/2017/09/17/111935#ID_2-2-3)


<a id="NeuralFittedQIteration"></a>

### ◎ Neural Fitted Q Iteration（MLPによる関数近似で学習を安定化させるための工夫）
先のMLPによる関数近似では、各層間の重み θ がそれぞれ別の値のパラメーターであるが故に、パラメーター数が多くて、なかなか学習が収束しないという問題が存在する。<br>

ここで紹介する Neural Fitted Q Iteration は、先のMLPによる関数近似において、学習を安定化させるために以下のような工夫・テクニックを導入した手法である。<br>

- Experience Replay（経験再生）：<br>
    MLP に入力する状態信号は、何も考慮しない場合、あるエピソードにおける開始状態から終了状態に向かっての時系列データ（＝状態の履歴）となるが、このような時系列データは、一般的に、時系列由来の相関の影響を受けてしまっており、互いに独立なデータ [i.i.d] でないことになる。<br>
    この問題を回避するために、このような時系列データではなくて、複数のエピソードにおける様々な時間ステップ t でのデータを、一連の学習用データとして使用する。<br>
    これにより、学習データの偏りを防ぎ、学習を安定化させることが出来る。<br>
    このような学習安定化のためのテクニックを、Experience Replay といい、Neural Fitted Q Iteration では、この Experience Replay のテクニックを利用して、学習を安定化させるための工夫を行っている。<br>
    ※ 後述の Deep Q-Network（DQN）でも、この Experience Relay のテクニックは使用されている。<br>

- オンライン学習ではなくバッチ学習で学習（＝パラメーターの更新）を行う：<br>
    先のMLPによる関数近似では、以下の更新式に従って、（出力層の）重みを更新（＝学習）していた。![image](https://user-images.githubusercontent.com/25688193/51524263-070de680-1e71-11e9-9718-1546909e5d32.png)<br>
    この更新式は、新しい学習用データ (s′,a′,r) が届いた際に、その場で重みを更新するオンライン学習となっている。<br>
    一般的な議論として、オンライン学習は、その場で素早く変化に対応して学習できる代わりに、学習が安定化しずらいというデメリットが存在する。<br>
    Neural Fitted Q Iteration では、先の Experience Replay で生成した一連のデータセットに対して、このデータセット全体で一括に学習を行うというバッチ学習で学習を行うようにする。<br>
    これにより、オンライン学習で学習を進めるよりかは、学習が安定することになる。<br>

> 【Memo】バッチ学習とオンライン学習の違いについては、以下のサイトを参考<br>
> - [バッチ学習 [batch processing] とオンライン学習 [online learning]](http://yagami12.hatenablog.com/entry/2017/09/17/111935#ID_1-5-2)<br>
> - [最急降下法 [gradient descent method] による学習（重みの更新）＜教師あり学習、パッチ学習＞](http://yagami12.hatenablog.com/entry/2017/09/17/111935#ID_2-2-1)<br>
> - [確率的勾配降下法 [stochastic gradient descent method] ＜教師あり学習、オンライン学習＞](http://yagami12.hatenablog.com/entry/2017/09/17/111935#ID_2-2-2)<br>

<br>

以上のような学習安定化のためのテクニックを考慮した Neural Fitted Q Iteration を、アルゴリズムとしてまとめると、以下のようになる。<br>

![image](https://user-images.githubusercontent.com/25688193/51526937-cb761b00-1e76-11e9-91e4-d11bec9fcf01.png)<br>


<a id="DeepQNetwork"></a>

### ◎ Deep Q Network（DQN）
DQNは、先の MLPによる関数近似や Neural Fitted Q-Iteration に対して、学習の安定化のための工夫として、主に以下のような手法やテクニックを導入した手法である。<br>
※ DQN は、単に、DNNで関数近似しただけの手法ではなく、このような学習の成功・安定化テクニックを含めたものであることに注意<br>

- Experience Replay（経験再生）：<br>
    先の Neural Fitted Q-Iteration と同様にして、あるエピソードの状態履歴（＝時系列データ）を、複数エピソードの様々な時間ステップでのデータにシャッフルしたものを、学習用データとして利用する。<br>

- ミニバッチ学習：<br>
    先の Neural Fitted Q-Iteration では、オンライン学習ではなくバッチ学習での重み更新とすることで、学習の安定化を図ったが、DQN では、オンライン学習とバッチ学習の折込案であるミニバッチ学習により、学習を安定化させ、なおかつ、その場での逐次学習を可能にしている。<br>

- CNNの導入：<br>
    ![image](https://user-images.githubusercontent.com/25688193/51535843-fa978700-1e8c-11e9-88f5-4adb1f0cf474.png)<br>
    CNN の導入により、これまでの強化学習手法のように、入力データとして、マス０、マス１、・・・といった各強化学習タスク（迷路探索問題など）固有のエージェントの状態ではなく、強化学習環境の画面の画像データそのものを入力データとすることが出来る。（上図参照）<br>

    更には、CNNの重み共有の構造により、MLPで関数近似するよりパラメーター数を減らすことに繫がって、結果として学習を容易にすることも出来る。<br>
    ※ CNN を導入した DQN のネットワーク構成のアーキテクチャ図は、後述<br>

- 報酬の clipping：<br>
    エージェントの報酬 r の値を、正規化処理として、クリッピングした値にする。<br>
    即ち、+10,-5 のような価値に応じて重み付した値ではなく、正の価値なら＋１、負の価値なら－１、ゼロ価値ならば０に設定する。<br>

- Target network の導入：<br>
    ![image](https://user-images.githubusercontent.com/25688193/51587486-392c5080-1f24-11e9-9636-85cd75ac5566.png)<br>
    先のMLPによる関数近似や Neural Fitted Q Iteration では、行動価値関数の関数近似器の目標値の更新式は、<br>
    ![image](https://user-images.githubusercontent.com/25688193/51587549-6e38a300-1f24-11e9-8e05-74be7f23c67c.png)<br>
    となっていたが、行動価値関数の関数近似値 ![image](https://user-images.githubusercontent.com/25688193/51587595-a04a0500-1f24-11e9-93f9-7ad1fda6371c.png) と ![image](https://user-images.githubusercontent.com/25688193/51587653-ce2f4980-1f24-11e9-86be-23fd5da983a8.png) とは、エピソードの１ステップ間 t→t+1 での行動価値関数であり、互いに時系列に相関のあるものとなっているので、この更新式における、教師信号と出力信号とに、時系列な相関が出来てしまう（上図参考）。<br>
    従って、この更新式で行動価値関数を更新すると、教師信号そのものの値が変化してしまい、本来の教師信号としての役割を果たしていないという問題が存在する。<br>
    <br>
    この問題を解決するための対策として、DQNでは、先の更新式において、前回時間ステップ t での重み θ での行動価値関数 ![image](https://user-images.githubusercontent.com/25688193/51587595-a04a0500-1f24-11e9-93f9-7ad1fda6371c.png) の代わりに、古い時間ステップでの重み ![image](https://user-images.githubusercontent.com/25688193/51587790-501f7280-1f25-11e9-9d41-76b68d926339.png) で固定されたDNN（＝Fixed Target Network という）による関数近似器 ![image](https://user-images.githubusercontent.com/25688193/51587746-2bc39600-1f25-11e9-895f-68e9c44543a6.png) を使用する。<br>
    そして、一定の周期で古い重み ![image](https://user-images.githubusercontent.com/25688193/51587790-501f7280-1f25-11e9-9d41-76b68d926339.png) と最新の重みを同期 θ するようにする。<br>
    即ち、以下のような更新式で、関数近似器を更新する。<br>
    ![image](https://user-images.githubusercontent.com/25688193/51587887-9b398580-1f25-11e9-9d47-8212d59f60be.png)<br>
    これにより、重みが更新されても、教師信号の値は更新させず、教師信号としての本来の役割を果たすことになる。<br>

- 損失関数として、Huber 関数を利用：<br>
    ![image](https://user-images.githubusercontent.com/25688193/51589043-236d5a00-1f29-11e9-83a2-f29d4acd26d4.png)<br>
    先のMLPによる関数近似や Neural Fitted Q Iteration では、損失関数として、（教師信号と出力信号との）最小２乗誤差を採用していたが、この最小２乗誤差では、上図のように、誤差値（＝教師信号ー出力信号）が大きい領域では、損失関数値の勾配値が大きくなりすぎて、学習の不安定化してしまうので、DQNでは、このような勾配爆発の対策として、Huber 関数を損失関数として採用する。<br>
    この Huber 関数は、損失関数の値がある値を超えると、勾配が一定値でクリッピングさせるために、勾配爆発を防ぐことが出来る。（上図参照）<br>


<a id="DQNのネットワーク構成のアーキテクチャ図"></a>

#### ☆ DQN のネットワーク構成のアーキテクチャ図
> 記載中...


---

<a id="参考文献"></a>

## ■ 参考文献

- 強化学習
    - [amazonで詳細を見る](https://www.amazon.co.jp/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-Richard-S-Sutton/dp/4627826613?SubscriptionId=AKIAJMYP6SDQFK6N4QZA&amp&tag=cloudstudy09-22&amp&linkCode=xm2&amp&camp=2025&amp&creative=165953&amp&creativeASIN=4627826613)
    - 強化学習のバイブル本？
    - 数式ではなく、言葉での説明が多い傾向。
    - 説明は丁寧で文書量も多いだが、逆に何がいいたいかの要点が掴みづらいかも。
    - 古い本なので、最新の手法（特にDeep系）は記載していない。
    - 実装コードはなし（疑似アルゴリズムはある）
    - この本を基礎にして、他の文献をあたるのがよさそう。

- 速習 強化学習 ―基礎理論とアルゴリズム―
    - [amazonで詳細を見る](https://www.amazon.co.jp/%E9%80%9F%E7%BF%92-%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-%E2%80%95%E5%9F%BA%E7%A4%8E%E7%90%86%E8%AB%96%E3%81%A8%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0%ク%80%95-Csaba-Szepesvari/dp/4320124227)
    - 強化学習の基礎事項の説明部分が少なく、又、ざっくりとしすぎて、初学には向いていない。
    - 主に、非Deep系の発展的手法の説明が記載しているのは良い。
    - 数式多め
    - 数式での記号の使い方が独特。

- 機械学習スタートアップシリーズ Pythonで学ぶ強化学習 入門から実践まで<br>
    - [amazonで詳細を見る](https://www.amazon.co.jp/dp/4065142989/ref=asc_df_40651429892570349/?tag=jpgo-22&creative=9303&creativeASIN=4065142989&linkCode=df0&hvadid=295704876452&hvpos=1o1&hvnetw=g&hvrand=17341163745073772790&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=1009648&hvtargid=pla-588412770121&th=1&psc=1)<br>
    - 強化学習の初学において、各種古典的種強化学習手法の理解で混乱しやすいポイント（モデルベース手法↔モデルフリー手法、Policyベース↔Valueベースなどの違い）がコンパクトに明確に説明してあり、全体像を再確認するのによい。
    - 古典的手法だけではなく、ニューラルネットワークでの関数近似や、DQN などの発展的内容の説明もある。（こっちが主役）
    - Dyna や遺伝的アルゴリズムによる手法の説明もあり。
    - 各種手法の Python での実装コード（※jupyter notebook を使わず、OOPライクな実装）とその丁寧な説明がある。
    - 理論的な内容の説明があるが、要点だけ、或いは実装コードに準じての説明になっており、理論部分をきちんと理解するには、他の文献を参照しながらでないと厳しいかも？
    - Python での実装コードはあるが、疑似アルゴリズムの記載はない。アルゴリズム自体を理解するには手間がかかるかも？

- つくりながら学ぶ! 深層強化学習 ~PyTorchによる実践プログラミング<br>
    - [amazonで詳細を見る](https://www.amazon.co.jp/dp/4839965625/ref=asc_df_48399656252542427/?tag=jpgo-22&creative=9303&creativeASIN=4839965625&linkCode=df0&hvadid=295723231663&hvpos=1o2&hvnetw=g&hvrand=4934601497265078375&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=1009648&hvtargid=pla-527403650097&th=1&psc=1)<br>
    - 代表的な古典的強化学習手法の jupyter notebook での実装コードとその詳細説明が載っている。（jupter notebook で実装されており、OOP ライクではなく、関数型的なコードで解読はしやすい）
    - 又、深層学習を利用した強化学習手法の jupyter notebook での実装コードとその詳細説明が載っている。（こっちが主役、こっちの実装は OOP ライク）
    - 理論的内容は殆ど記載されておらず、この本だけで理論を理解するには厳しい。


<a id="使用コード"></a>

### ◎ 使用コード

- [GitHub/Yagami360/ReinforcementLearning_Exercises](https://github.com/Yagami360/ReinforcementLearning_Exercises)<br>