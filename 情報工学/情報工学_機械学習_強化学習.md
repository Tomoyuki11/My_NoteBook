# 強化学習 [Re]
機械学習の一種である強化学習について勉強したことをまとめたノート（忘備録）です。随時追記中。<br>


## 目次 [Contents]

1. [概要](#概要)
1. [【補足（外部リンク）】確率過程とマルコフ連鎖](http://yagami12.hatenablog.com/entry/2017/09/17/110406#ID_3)
1. [マルコフ決定過程（MDP）](#マルコフ決定過程（MDP）)
    1. [マルコフ決定過程による強化学習のモデル化](#マルコフ決定過程による強化学習のモデル化)
    1. マルコフ決定過程の例
    1. 価値関数
    1. [定常方策での状態価値関数](#定常方策での状態価値関数)
    1. [行動価値関数](#行動価値関数)
    1. [ベルマン方程式](#ベルマン方程式)
    1. マルコフ決定過程を解くための動的最適化法
1. 補足事項
    1. [【外部リンク】機械学習](http://yagami12.hatenablog.com/entry/2017/09/17/111400)
    1. [【外部リンク】最適化問題](http://yagami12.hatenablog.com/entry/2017/09/17/101739)
    1. [【外部リンク】グラフ理論](http://yagami12.hatenablog.com/entry/2017/09/17/110406)
1. [参考文献](#参考文献)
    1. [使用コード](#使用コード)

---

<a id="概要"></a>

## ■ 概要
> 記載中...

---

<a id="マルコフ決定過程（MDP）"></a>

## ■ マルコフ決定過程（MDP）
強化学習は、概要でみたように、意思決定主体（＝エージェント）と外部環境が相互作用するシステムにおいて、教師なし学習の状況の下、試行錯誤を通じて、目標を示す数値（＝報酬）を最大化するようにシステムを学習制御する枠組みであるが、これをマルコフ決定過程 [MDP:Markov decision process] でモデル化する。<br>

このマルコフ決定過程は、マルコフ性とエルゴード性を持つ確率過程であって、選択可能な行動、及び、報酬を追加したものである。<br>
言い換えると、マルコフ決定過程は、マルコフ連鎖に対して、選択可能な行動、及び、報酬を追加したものである。<br>

- マルコフ性：<br>
    ある状態 ![image](https://user-images.githubusercontent.com/25688193/49641452-85463300-fa53-11e8-9596-7e8b4b082bce.png) から、別の状態 ![image](https://user-images.githubusercontent.com/25688193/49641500-a575f200-fa53-11e8-8467-eaaf3a8e866f.png) に移行する確率が、それ以前の経路 t によらず、現在の状態のみ ![image](https://user-images.githubusercontent.com/25688193/49641452-85463300-fa53-11e8-9596-7e8b4b082bce.png) で決まる性質。マルコフ連鎖は、このマルコフ性を性質を持つ。<br>

- エルゴード性（定常性）：<br>
    任意の初期状態 ![image](https://user-images.githubusercontent.com/25688193/49641553-d5bd9080-fa53-11e8-9c45-e3771976401b.png) からスタートした確率過程が、無限時間経過後には、最初の状態とは無関係になる性質。マルコフ連鎖は、このエルゴード性の性質も合わせ持つ。<br>

<br>

![image](https://user-images.githubusercontent.com/25688193/49641972-208bd800-fa55-11e8-9bce-c325cbe3b59f.png)<br>
![image](https://user-images.githubusercontent.com/25688193/49642034-45804b00-fa55-11e8-8fd8-81b26039f82a.png)<br>

> 【Memo】<br>
> 「速習 強化学習」の本に書いてあるマルコフ決定過程の定義、状態集合、行動集合、遷移確率カーネルの３つの構造の組から定義されてるのに対し、他の文献では、状態集合、行動集合、遷移確率、報酬関数４つの組から定義されてものが多いが、３つの構造の１つの遷移確率カーネルが、状態遷移と報酬関数を与えるから、報酬関数を新たに３つの組とは別に追加しなくても well-defined となる。<br>

- 【参考サイト】<br>
    - [マルコフ決定過程 - Wikipedia](https://ja.wikipedia.org/wiki/%E3%83%9E%E3%83%AB%E3%82%B3%E3%83%95%E6%B1%BA%E5%AE%9A%E9%81%8E%E7%A8%8B)<br>
    - [第３章： 強化学習の基礎理論](http://sysplan.nams.kyushu-u.ac.jp/gen/edu/RL_intro.html#Foundations)<br>


<a id="マルコフ決定過程による強化学習のモデル化"></a>

### ◎ マルコフ決定過程による強化学習のモデル化
次に、このように定義したマルコフ決定過程において、システム（環境）とエージェントは、以下のような相互作用を行うものとして、強化学習をモデル化する。<br>

- （状態と報酬の遷移）<br>
    時間 t における状態 ![image](https://user-images.githubusercontent.com/25688193/49642152-94c67b80-fa55-11e8-9df4-e1277abc3fdf.png) にあるシステムにおいて、エージェントの行動 ![image](https://user-images.githubusercontent.com/25688193/49642191-b0ca1d00-fa55-11e8-9110-0b7f8f4f82b7.png) が実行されると、システムはそれに応じた状態遷移を行うが、このとき、以下のような関係が成り立つものとする。<br>
    ![image](https://user-images.githubusercontent.com/25688193/49642225-cb03fb00-fa55-11e8-925a-3bc9bb8becbe.png)<br>
	即ち、次の状態と報酬の組 ![image](https://user-images.githubusercontent.com/25688193/49642251-da834400-fa55-11e8-8f56-1933d89913cb.png) は、状態と報酬をセットにした遷移確率である遷移確率カーネル ![image](https://user-images.githubusercontent.com/25688193/49642273-ea9b2380-fa55-11e8-86f9-0ce942fdae87.png) の確率分布に従う。<br>
    言い換えると、遷移確率カーネル ![image](https://user-images.githubusercontent.com/25688193/49642273-ea9b2380-fa55-11e8-86f9-0ce942fdae87.png) の確率に従って、次の状態 ![image](https://user-images.githubusercontent.com/25688193/49642302-ff77b700-fa55-11e8-9570-8096cb583d34.png) と報酬 ![image](https://user-images.githubusercontent.com/25688193/49642320-0dc5d300-fa56-11e8-8399-73c5b4c44ed4.png) が定まる。<br>

- （定常性）<br>
    遷移後の次の状態 ![image](https://user-images.githubusercontent.com/25688193/49642302-ff77b700-fa55-11e8-9570-8096cb583d34.png) は、現在の時間 t の状態 ![image](https://user-images.githubusercontent.com/25688193/49642152-94c67b80-fa55-11e8-9df4-e1277abc3fdf.png) と行動 ![image](https://user-images.githubusercontent.com/25688193/49642191-b0ca1d00-fa55-11e8-9110-0b7f8f4f82b7.png) のみに依存し、それ以前の時間 t-1,t-2,... での状態や行動によらない。<br>
    即ち、<br>
    ![image](https://user-images.githubusercontent.com/25688193/49642453-6dbc7980-fa56-11e8-8c29-c852c4c30266.png)<br>
    の関係が成り立つものとする。<br>

- （行動則、政策）<br>
    エージェントは、任意の時間 t での状態と報酬の観測の履歴 ![image](https://user-images.githubusercontent.com/25688193/49642500-9775a080-fa56-11e8-83fc-1978a4876ca6.png) の情報を元に行動することが出来る。<br>
    このような履歴情報を元にして、エージェントが行動を選択するルールを行動則（政策）π といい、この行動則 π は、確率で表現される（＝確率的に決定される）ものとなる。<br>

- （行動則により定まる確率過程）<br>
    そして、エージェントの行動則 π と初期状態 ![image](https://user-images.githubusercontent.com/25688193/49641553-d5bd9080-fa53-11e8-9c45-e3771976401b.png) から、確率的に決定される状態・行動・報酬の組の過程、即ち、確率過程 ![image](https://user-images.githubusercontent.com/25688193/49642598-e0c5f000-fa56-11e8-84a1-33bf822eb4cf.png) が定まる。<br>


エージェントの目的は、これらの相互作用の過程を繰り返しながら、自身の行動則 π により目的関数である割引総和（＝収益）<br>
![image](https://user-images.githubusercontent.com/25688193/49642631-ffc48200-fa56-11e8-9021-8bf4253d6b89.png)<br>
の期待値を最大化することである。即ち、<br>
![image](https://user-images.githubusercontent.com/25688193/49642659-0b17ad80-fa57-11e8-8b2b-0c50d25a037d.png)<br>


<a id="定常方策での状態価値関数"></a>

### ◎ 定常方策での状態価値関数
先に述べたように、強化学習をマルコフ決定過程でモデル化するにあたって、エージェントの目的は、自身の行動則 π により目的関数である割引総和（＝収益）
![image](https://user-images.githubusercontent.com/25688193/49642631-ffc48200-fa56-11e8-9021-8bf4253d6b89.png)<br>
の期待値を最大化することであったが、これを定常方策（＝時間に対して不変な行動則）での収益に限定された状態価値関数の概念を用いて、モデル化することを考える。<br>
![image](https://user-images.githubusercontent.com/25688193/49642950-c2acbf80-fa57-11e8-91eb-3e1e6524953b.png)<br>


<a id="行動価値関数"></a>

### ◎ 行動価値関数
> 記載中...


<a id="ベルマン方程式"></a>

### ◎ ベルマン方程式
> 記載中...

---


<a id="参考文献"></a>

## ■ 参考文献

- 速習 強化学習 ―基礎理論とアルゴリズム―
    - [amazonで詳細を見る](https://www.amazon.co.jp/%E9%80%9F%E7%BF%92-%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-%E2%80%95%E5%9F%BA%E7%A4%8E%E7%90%86%E8%AB%96%E3%81%A8%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0%E2%80%95-Csaba-Szepesvari/dp/4320124227)

- つくりながら学ぶ! 深層強化学習 ~PyTorchによる実践プログラミング<br>
    - [amazonで詳細を見る](https://www.amazon.co.jp/dp/4839965625/ref=asc_df_48399656252542427/?tag=jpgo-22&creative=9303&creativeASIN=4839965625&linkCode=df0&hvadid=295723231663&hvpos=1o2&hvnetw=g&hvrand=4934601497265078375&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=1009648&hvtargid=pla-527403650097&th=1&psc=1)<br>


<a id="使用コード"></a>

### ◎ 使用コード
> 実装中...
