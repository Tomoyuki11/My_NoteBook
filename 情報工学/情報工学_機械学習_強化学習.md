# 強化学習 [Reinforcement Learning]（執筆中）
機械学習の一種である強化学習について勉強したことをまとめたノート（忘備録）です。随時追記中。<br>

- 【参考サイト】
    - [今さら聞けない強化学習（1）：状態価値関数とBellman方程式](https://qiita.com/triwave33/items/5e13e03d4d76b71bc802)
    - [強化学習について学んでみた。（まとめ） - いものやま。](http://yamaimo.hatenablog.jp/entry/2016/01/11/200000)
    - [第３章： 強化学習の基礎理論](http://sysplan.nams.kyushu-u.ac.jp/gen/edu/RL_intro.html#Foundations)<br>
    - [これから強化学習を勉強する人のための「強化学習アルゴリズム・マップ」と、実装例まとめ](https://qiita.com/sugulu/items/3c7d6cbe600d455e853b)
    - [ゼロからDeepまで学ぶ強化学習](https://qiita.com/icoxfog417/items/242439ecd1a477ece312)


## 目次 [Contents]

1. [概要](#概要)
    1. [強化学習の各種アルゴリズムの関係](#強化学習の各種アルゴリズムの関係)
    1. [強化学習で特有の用語説明](#強化学習で特有の用語説明)
1. [強化学習のモデル化](#強化学習のモデル化)
    1. [マルコフ決定過程（MDP）](#マルコフ決定過程（MDP）)
        1. [マルコフ決定過程による強化学習のモデル化](#マルコフ決定過程による強化学習のモデル化)
        1. マルコフ決定過程の例
    1. [価値関数](#価値関数)
    1. [状態価値関数](#状態価値関数)
    1. [行動価値関数](#行動価値関数)
    1. [状態価値関数と行動価値関数の関係](#状態価値関数と行動価値関数の関係)
    1. [ベルマン方程式](#ベルマン方程式)
1. [マルコフ決定過程を解くための動的計画法](#マルコフ決定過程を解くための動的計画法)
    1. [ベルマンの方程式の厳密な解法と反復法](#ベルマンの方程式の厳密な解法と反復法)
    1. [価値反復法 [value iteration]](#価値反復法)
    1. [方策反復法 [policy iteration]](#方策反復法)
        1. 方策勾配法
1. [モンテカルロ法による価値推定](#モンテカルロ法による価値推定)
    1. 初回訪問モンテカルロ法
    1. 逐次訪問モンテカルロ法
1. [TD学習（時間的差分学習）](#TD学習（時間的差分学習）)
    1. TD(0)法
    1. [TD学習の制御問題への適用](#TD学習の制御問題への適用)
        1. [Sarsa（方策オン型TD制御）](#Sarsa)
        1. [Q学習 [Q-learning]（方策オフ型TD制御）](#Q学習)
1. 関数近似による価値推定
    1. ニューラルネットワークでの関数近似による価値推定
1. 深層学習の構造を取り込んだアルゴリズム
    1. DQN [Deep Q-learning]
1. 補足事項
    1. [【外部リンク】機械学習](http://yagami12.hatenablog.com/entry/2017/09/17/111400)
    1. [【外部リンク】最適化問題](http://yagami12.hatenablog.com/entry/2017/09/17/101739)
    1. [【外部リンク】グラフ理論](http://yagami12.hatenablog.com/entry/2017/09/17/110406)
        1. [【補足（外部リンク）】確率過程とマルコフ連鎖](http://yagami12.hatenablog.com/entry/2017/09/17/110406#ID_3)
1. [参考文献](#参考文献)
    1. [使用コード](#使用コード)

---

<!--
<a id="概要"></a>

## ■ 概要
> 記載中...



<a id="強化学習の各種アルゴリズムの関係"></a>

### ◎ 強化学習の各種アルゴリズムの関係
> 記載中...

<a id="強化学習の各種アルゴリズムの関係"></a>

### ◎ 強化学習で特有の用語説明
> 記載中...

-->

<a id="強化学習のモデル化"></a>

## ■ 強化学習のモデル化
![image](https://user-images.githubusercontent.com/25688193/50441622-e9676300-093e-11e9-8179-a4a7deefa11c.png)<br>

上図は、強化学習で取り扱うシステムを図示したものである。<br>
このシステムは、以下のように、エージェントと環境との相互作用でモデル化される。<br>

- エージェントとは、意思決定と学習（※将来に渡る収益の期待値を最大化するような学習）を行う行動主体である。<br>
- 環境とは、エージェントが相互作用を行う対象であり、エージェントの行動 a を反映した上で、エージェントに対して、状態 s と、その状態での報酬 r を与える。<br>
- エージェントの行動 a は、確率分布で表現される行動方策 [policy] ![image](https://user-images.githubusercontent.com/25688193/50441875-081a2980-0940-11e9-9edb-7faff8a50fa5.png) に基づいて選択される。<br>
	ここで、![image](https://user-images.githubusercontent.com/25688193/50441875-081a2980-0940-11e9-9edb-7faff8a50fa5.png) は、![image](https://user-images.githubusercontent.com/25688193/50441923-3d267c00-0940-11e9-997f-f25cd5dd12ca.png) ならば ![image](https://user-images.githubusercontent.com/25688193/50441977-5a5b4a80-0940-11e9-8306-2669589b23e1.png) となる確率を表す。<br>

- エージェントと環境は、継続的に相互作用を行う。<br>
	より詳細には、離散時間 t=0,1,2,... の各々の時間 t おいて、相互作用を行う。<br>
- エージェントがその意思決定目的とする収益は、時間経過 t=0,1,2,.. での時間経過によって、その値が減衰するものとする。<br>
	つまり、時間 t 時点から見て、将来 t+1,t+2,... にわたっての報酬 r の差し引きを考慮した割引収益<br>
	![image](https://user-images.githubusercontent.com/25688193/50442003-752dbf00-0940-11e9-9449-9c2d8391e466.png)<br>
	で考える。<br>
	※ 割引収益で考えるのは、同じ収益値ならば、将来受け取るよりも現時点で受け取るほうがよい値であることの妥当性もあるが、別の理由として、終端状態の存在しない連続タスクにおいて、割引なしの場合の将来にわたっての収益の和が、t→∞ の時間経過により、無限大に発散してしまい、エージェントの目的である将来にわたっての収益の最大化が、数学的に取り扱いずらい問題になってしまうことを防ぐという理由もある。<br>
- そして、エージェントは、時間経過 t=0,1,2,.. での相互作用によって、この将来にわたっての割引収益 ![image](https://user-images.githubusercontent.com/25688193/50442058-a1494000-0940-11e9-9af7-aaa8a2c8b836.png) の期待値 E[R] を最大化するように、自身の行動方策 ![image](https://user-images.githubusercontent.com/25688193/50441875-081a2980-0940-11e9-9edb-7faff8a50fa5.png) を学習しながら、行動する。<br>
	※ 一般的に、強化学習の枠組みにおいて、時間に依存する行動方策 ![image](https://user-images.githubusercontent.com/25688193/50442104-cc339400-0940-11e9-9471-d4864775f324.png) ではなく、時間 t によらない定常方策 ![image](https://user-images.githubusercontent.com/25688193/50442137-e40b1800-0940-11e9-8133-9b607406bb93.png) が用いられる。<br>
	（価値関数の満たすべき方程式を与えるベルマンの方程式は、定常方策 ![image](https://user-images.githubusercontent.com/25688193/50442137-e40b1800-0940-11e9-8133-9b607406bb93.png) を前提とした方程式となっている。）<br>

- ここで、エージェントと環境の相互作用の順序は、以下のようになる。<br>
	① エージェントは、環境から状態![image](https://user-images.githubusercontent.com/25688193/50442183-161c7a00-0941-11e9-95e2-c20d8df60afd.png)を受け取る。（＝エージェントの状態が![image](https://user-images.githubusercontent.com/25688193/50442183-161c7a00-0941-11e9-95e2-c20d8df60afd.png)になる。）<br>
	② エージェントは、現在の状態![image](https://user-images.githubusercontent.com/25688193/50442183-161c7a00-0941-11e9-95e2-c20d8df60afd.png)に基づき、行動方策 ![image](https://user-images.githubusercontent.com/25688193/50441875-081a2980-0940-11e9-9edb-7faff8a50fa5.png) に従って、行動![image](https://user-images.githubusercontent.com/25688193/50442241-58de5200-0941-11e9-8757-a4c9bddb8c28.png)を選択する。<br>
	③ 時間が１ステップ t→t+1 経過後に、エージェントは、エージェントの行動![image](https://user-images.githubusercontent.com/25688193/50442241-58de5200-0941-11e9-8757-a4c9bddb8c28.png)の結果として、報酬 ![image](https://user-images.githubusercontent.com/25688193/50442311-a6f35580-0941-11e9-90f9-2afbb39360c9.png) を受取る。<br>
	※ 時間 t で環境が受け取った行動![image](https://user-images.githubusercontent.com/25688193/50442241-58de5200-0941-11e9-8757-a4c9bddb8c28.png)による即時報酬という意味で、![image](https://user-images.githubusercontent.com/25688193/50442280-84f9d300-0941-11e9-8afb-968821fa5805.png)ではなく ![image](https://user-images.githubusercontent.com/25688193/50442311-a6f35580-0941-11e9-90f9-2afbb39360c9.png) を用いる。<br>

> 【Memo】　エージェントと環境の境界線はどのように設定すべきか？<br>
> このようにモデル化される強化学習のシステムにおいて、実際上のタスクでは、どのようにエージェントと環境との線引を行えばよいのか？という問題は残る。<br>
> これは例えば、ロボット制御タスクにおいては、ロボットを構成するモーターやセンサーなどの物理的デバイスは、その内部の電子回路など自体は、エージェントであるロボット自身からは制御できにので、エージェントの一部というよりも、むしろ環境の一部とみなしたほうが適切であるのでは？と考えられる。<br>
> 同様の例として、人体制御タスクにおいては、人体を構成する筋肉や骨格や感覚器などは、エージェントの一部というよりも、むしろ環境の一部とみなしたほうが適切であるのでは？と考えられる。<br>
> この問題の一般的な基準を与える回答としては、エージェントがに任意に変更出来ないものは、エージェントの外部にあると考え、それらを環境とみなすという解決策が考えられる。<br>

このエージェントと環境の相互作用を、時間や順序を含まて考える場合においては、以下のような遷移図や、バックアップ線図で表現したほうがわかりやすい。<br>

> 記載中...

- エピソード的タスクと連続タスク

- マルコフ性


<a id="マルコフ決定過程（MDP）"></a>

### ◎ マルコフ決定過程（MDP）
> 要書き換え...

強化学習は、先にみたように、意思決定主体（＝エージェント）と外部環境が相互作用するシステムにおいて、教師なし学習の状況の下、試行錯誤を通じて、目標を示す数値（＝報酬）を最大化するようにシステムを学習制御する枠組みであるが、これをマルコフ決定過程 [MDP:Markov decision process] でモデル化する。<br>

このマルコフ決定過程は、マルコフ性とエルゴード性を持つ確率過程であって、選択可能な行動、及び、報酬を追加したものである。<br>
言い換えると、マルコフ決定過程は、マルコフ連鎖に対して、選択可能な行動、及び、報酬を追加したものである。<br>

- マルコフ性：<br>
    ある状態 ![image](https://user-images.githubusercontent.com/25688193/49641452-85463300-fa53-11e8-9596-7e8b4b082bce.png) から、別の状態 ![image](https://user-images.githubusercontent.com/25688193/49641500-a575f200-fa53-11e8-8467-eaaf3a8e866f.png) に移行する確率が、それ以前の経路 t によらず、現在の状態のみ ![image](https://user-images.githubusercontent.com/25688193/49641452-85463300-fa53-11e8-9596-7e8b4b082bce.png) で決まる性質。マルコフ連鎖は、このマルコフ性を性質を持つ。<br>

- エルゴード性（定常性）：<br>
    任意の初期状態 ![image](https://user-images.githubusercontent.com/25688193/49641553-d5bd9080-fa53-11e8-9c45-e3771976401b.png) からスタートした確率過程が、無限時間経過後には、最初の状態とは無関係になる性質。マルコフ連鎖は、このエルゴード性の性質も合わせ持つ。<br>

<br>

![image](https://user-images.githubusercontent.com/25688193/49641972-208bd800-fa55-11e8-9bce-c325cbe3b59f.png)<br>
![image](https://user-images.githubusercontent.com/25688193/49697862-cb3ffa00-fbff-11e8-9cab-5a50d0a77470.png)<br>
![image](https://user-images.githubusercontent.com/25688193/49697644-62578280-fbfd-11e8-82b7-2c931ca73fcc.png)<br>

> 【Memo】<br>
> 「速習 強化学習」の本に書いてあるマルコフ決定過程の定義、状態集合、行動集合、遷移確率カーネルの３つの構造の組から定義されてるのに対し、他の文献では、状態集合、行動集合、遷移確率、報酬関数４つの組から定義されてものが多いが、３つの構造の１つの遷移確率カーネルが、状態遷移と報酬関数を与えるから、報酬関数を新たに３つの組とは別に追加しなくても well-defined となる。<br>

- 【参考サイト】<br>
    - [マルコフ決定過程 - Wikipedia](https://ja.wikipedia.org/wiki/%E3%83%9E%E3%83%AB%E3%82%B3%E3%83%95%E6%B1%BA%E5%AE%9A%E9%81%8E%E7%A8%8B)<br>


<a id="マルコフ決定過程による強化学習のモデル化"></a>

#### ☆ マルコフ決定過程による強化学習のモデル化
> 要書き換え...

次に、このように定義したマルコフ決定過程において、システム（環境）とエージェントは、以下のような相互作用を行うものとして、強化学習をモデル化する。<br>

- （状態と報酬の遷移）<br>
    時間 t における状態 ![image](https://user-images.githubusercontent.com/25688193/49642152-94c67b80-fa55-11e8-9df4-e1277abc3fdf.png) にあるシステムにおいて、エージェントの行動 ![image](https://user-images.githubusercontent.com/25688193/49642191-b0ca1d00-fa55-11e8-9110-0b7f8f4f82b7.png) が実行されると、システムはそれに応じた状態遷移を行うが、このとき、以下のような関係が成り立つものとする。<br>
    ![image](https://user-images.githubusercontent.com/25688193/49642225-cb03fb00-fa55-11e8-925a-3bc9bb8becbe.png)<br>
	即ち、次の状態と報酬の組 ![image](https://user-images.githubusercontent.com/25688193/49642251-da834400-fa55-11e8-8f56-1933d89913cb.png) は、状態と報酬をセットにした遷移確率である遷移確率カーネル ![image](https://user-images.githubusercontent.com/25688193/49642273-ea9b2380-fa55-11e8-86f9-0ce942fdae87.png) の確率分布に従う。<br>
    言い換えると、遷移確率カーネル ![image](https://user-images.githubusercontent.com/25688193/49642273-ea9b2380-fa55-11e8-86f9-0ce942fdae87.png) の確率に従って、次の状態 ![image](https://user-images.githubusercontent.com/25688193/49642302-ff77b700-fa55-11e8-9570-8096cb583d34.png) と報酬 ![image](https://user-images.githubusercontent.com/25688193/49642320-0dc5d300-fa56-11e8-8399-73c5b4c44ed4.png) が定まる。<br>

- （定常性）<br>
    遷移後の次の状態 ![image](https://user-images.githubusercontent.com/25688193/49642302-ff77b700-fa55-11e8-9570-8096cb583d34.png) は、現在の時間 t の状態 ![image](https://user-images.githubusercontent.com/25688193/49642152-94c67b80-fa55-11e8-9df4-e1277abc3fdf.png) と行動 ![image](https://user-images.githubusercontent.com/25688193/49642191-b0ca1d00-fa55-11e8-9110-0b7f8f4f82b7.png) のみに依存し、それ以前の時間 t-1,t-2,... での状態や行動によらない。<br>
    即ち、<br>
    ![image](https://user-images.githubusercontent.com/25688193/49642453-6dbc7980-fa56-11e8-8c29-c852c4c30266.png)<br>
    の関係が成り立つものとする。<br>

- （行動則、政策）<br>
    エージェントは、任意の時間 t での状態と報酬の観測の履歴 ![image](https://user-images.githubusercontent.com/25688193/49642500-9775a080-fa56-11e8-83fc-1978a4876ca6.png) の情報を元に行動することが出来る。<br>
    このような履歴情報を元にして、エージェントが行動を選択するルールを行動則（政策）π といい、この行動則 π は、確率で表現される（＝確率的に決定される）ものとなる。<br>

- （行動則により定まる確率過程）<br>
    そして、エージェントの行動則 π と初期状態 ![image](https://user-images.githubusercontent.com/25688193/49641553-d5bd9080-fa53-11e8-9c45-e3771976401b.png) から、確率的に決定される状態・行動・報酬の組の過程、即ち、確率過程 ![image](https://user-images.githubusercontent.com/25688193/49697224-10602e00-fbf8-11e8-8d5c-9eb374f3866f.png) が定まる。<br>


エージェントの目的は、これらの相互作用の過程を繰り返しながら、自身の行動則 π により目的関数である割引総和（＝収益）<br>
![image](https://user-images.githubusercontent.com/25688193/49642631-ffc48200-fa56-11e8-9021-8bf4253d6b89.png)<br>
の期待値を最大化することである。即ち、<br>
![image](https://user-images.githubusercontent.com/25688193/49642659-0b17ad80-fa57-11e8-8b2b-0c50d25a037d.png)<br>

以上のようなマルコフ決定過程による強化学習のモデル化を、行動則による確率的な行動選択とともに、バックアップ線図で図示すると以下の図のようになる。<br>

![image](https://user-images.githubusercontent.com/25688193/50384146-167c0180-0704-11e9-877c-5c1c664cef7c.png)<br>

ここで、更に、状態遷移確率による状態選択を含める描くと、以下のバックアップ線図のようになる。<br>

![image](https://user-images.githubusercontent.com/25688193/50384158-4f1bdb00-0704-11e9-9652-0e6eae1ede1e.png)<br>


<a id="価値関数"></a>

### ◎ 価値関数
> 記載中...

- 【参照サイト】<br>
    - [今さら聞けない強化学習（1）：状態価値関数とBellman方程式 - Qiita](https://qiita.com/triwave33/items/5e13e03d4d76b71bc802)<br>
    - [今さら聞けない強化学習（3）：行動価値関数とBellman方程式 - Qiita](https://qiita.com/triwave33/items/8966890701169f8cad47)<br>
    - [人工知能概論 第6回 多段決定(2) 強化学習.](https://slidesplayer.net/slide/11477412/)<br>


<a id="状態価値関数"></a>

### ◎ 状態価値関数
先に述べたように、強化学習をマルコフ決定過程でモデル化するにあたって、エージェントの目的は、自身の行動則 π により目的関数である割引総和（＝収益）<br>
![image](https://user-images.githubusercontent.com/25688193/49642631-ffc48200-fa56-11e8-9021-8bf4253d6b89.png)<br>
の期待値を最大化することであったが、これを定常方策（＝時間に対して不変な行動則）での収益に限定された状態価値関数の概念を用いて、モデル化することを考える。<br>

![image](https://user-images.githubusercontent.com/25688193/49914512-de3f1c80-fed4-11e8-9dfc-706e81dba655.png)<br>

![image](https://user-images.githubusercontent.com/25688193/49742237-71acfd80-fcdb-11e8-934b-c5749312da03.png)<br>

ここで、この状態価値関数 ![image](https://user-images.githubusercontent.com/25688193/49739008-2c390200-fcd4-11e8-8c93-0609a67c8915.png) を先のバックアップ線図に合わせて記載すると、上図のようになる。<br>
この図より、状態価値関数の定義にある将来に対する割引総和（＝収益、利得）の期待値 ![image](https://user-images.githubusercontent.com/25688193/49739267-c9943600-fcd4-11e8-9f8a-88ee2de9e046.png) は、緑枠内での報酬 r の平均をとったものになっていることが分かる。<br>


更に、この将来の割引総和に関しての期待値は、上図から分かるように、赤枠部分 ![image](https://user-images.githubusercontent.com/25688193/49739452-44f5e780-fcd5-11e8-935e-dfacc1315420.png) と青枠部分 ![image](https://user-images.githubusercontent.com/25688193/49739483-550dc700-fcd5-11e8-8d70-754fbf5e38d8.png) で各々平均値をとったものに対応しているので、以下の関係が成り立つことが分かる。<br>
![image](https://user-images.githubusercontent.com/25688193/49739922-63101780-fcd6-11e8-88bf-7e291f8e696c.png)<br>
尚、この関係式を上図から直感的にではなく、式変形で示すと以下のようになる。<br>
![image](https://user-images.githubusercontent.com/25688193/49739957-74f1ba80-fcd6-11e8-894a-bd5efa7164ca.png)<br>

ここで、この関係式は、状態価値関数に関しての方程式になっており、（状態価値関数に対しての）ベルマンの方程式という。<br>
そして、このベルマンの方程式は不動点方程式であるので、この方程式を解けば、原理的には不動点としての状態価値関数の最適解が一意に存在することになる（詳細は後述）<br>


<a id="行動価値関数"></a>

### ◎ 行動価値関数
マルコフ決定過程において、価値関数の一種として、定常方策 ![image](https://user-images.githubusercontent.com/25688193/49683687-38726300-fb0c-11e8-8de2-b79447dcff37.png) の元での、状態 ![image](https://user-images.githubusercontent.com/25688193/49683696-4a540600-fb0c-11e8-984c-e6f2eccdeb94.png) にいて行動 ![image](https://user-images.githubusercontent.com/25688193/49683701-62c42080-fb0c-11e8-8690-62119c18f03d.png) を選択する価値を表わす、行動価値関数 ![image](https://user-images.githubusercontent.com/25688193/49683704-72dc0000-fb0c-11e8-841c-0c42230bcc5b.png) なるものを考えることも出来る。<br>

![image](https://user-images.githubusercontent.com/25688193/49914603-4d1c7580-fed5-11e8-810b-5063974a4d23.png)<br>
![image](https://user-images.githubusercontent.com/25688193/49915116-56a6dd00-fed7-11e8-838b-4866ed23e442.png)<br>

ここで、この状態行動関数 ![image](https://user-images.githubusercontent.com/25688193/49683704-72dc0000-fb0c-11e8-841c-0c42230bcc5b.png) を先のバックアップ線図に合わせて記載すると、上図のようになる。<br>
この図より、状態行動関数の定義にある割引総和（＝収益、利得）の期待値 ![image](https://user-images.githubusercontent.com/25688193/49915218-bb623780-fed7-11e8-8daf-fe4499115302.png) は、オレンジ色内での報酬 r の平均をとったものになっていることが分かる。<br>

一般に、状態行動関数 ![image](https://user-images.githubusercontent.com/25688193/49683704-72dc0000-fb0c-11e8-841c-0c42230bcc5b.png) が与えられたときに、この状態行動関数の価値を最大化する行動、即ち、![image](https://user-images.githubusercontent.com/25688193/49915250-e6e52200-fed7-11e8-9675-de40d57af045.png) とするような行動選択は、「状態 s において、行動価値関数 Q に関して greedy（貪欲）な行動である」という。<br>
更に、”任意の” 状態において、Qに関してのグリーディな行動のみを選択する行動方策は、「行動価値関数 Q に関してグリーディな方策である」という。<br>

従って、達成可能な行動価値の最大値である最適行動価値関数 ![image](https://user-images.githubusercontent.com/25688193/49915291-185ded80-fed8-11e8-8eff-72bd73109142.png) においては、![image](https://user-images.githubusercontent.com/25688193/49915291-185ded80-fed8-11e8-8eff-72bd73109142.png) に関してのグリーディな方策は最適であることが分かる。<br>


<a id="状態価値関数と行動価値関数の関係"></a>

### ◎ 状態価値関数と行動価値関数の関係
価値関数には、状態価値関数と行動価値関数の２つが存在するが、これら２つの関数は、以下の式によって、互いに結びつく。<br>

![image](https://user-images.githubusercontent.com/25688193/49915540-87881180-fed9-11e8-938c-68be355dde58.png)<br>

![image](https://user-images.githubusercontent.com/25688193/49915568-a5557680-fed9-11e8-9202-92a1cf3f8286.png)<br>
→ 将来の期待利得である状態価値関数 ![image](https://user-images.githubusercontent.com/25688193/49739008-2c390200-fcd4-11e8-8c93-0609a67c8915.png) は、（現在の期待利得である）行動価値関数 ![image](https://user-images.githubusercontent.com/25688193/49683704-72dc0000-fb0c-11e8-841c-0c42230bcc5b.png) に対して、行動方策に関しての分岐（赤枠部分）で和をとった形で表現できる。<br>

![image](https://user-images.githubusercontent.com/25688193/49915586-b605ec80-fed9-11e8-95fc-960c097e7eea.png)<br>
→ 現在の行動価値関数 ![image](https://user-images.githubusercontent.com/25688193/49683704-72dc0000-fb0c-11e8-841c-0c42230bcc5b.png) の値は、現在の状態 s で行動 a をとって状態 s′ に遷移するときの報酬 r(s,a,s′) に対して、確率分布 ![image](https://user-images.githubusercontent.com/25688193/50039765-09637080-007b-11e9-8694-e28e343c2bb7.png) で表現される遷移後の状態の分岐（青枠部分）に関しての和をとって平均化したもの（＝R）に、状態遷移後の状態価値関数 ![image](https://user-images.githubusercontent.com/25688193/49915737-6673f080-feda-11e8-97e7-a3e6a573cb44.png) の値を加えたものとなる。<br>
尚、状態遷移後の s′ 以降の価値は、状態価値関数 ![image](https://user-images.githubusercontent.com/25688193/49915737-6673f080-feda-11e8-97e7-a3e6a573cb44.png) に含まれるので、これ以降の分岐の和をとる必要はない。<br>


以上の関係式をまとめると、以下のようになる。<br>

![image](https://user-images.githubusercontent.com/25688193/49915748-7e4b7480-feda-11e8-870b-10c0dfb074c3.png)<br>

- （証明略）解釈図より成り立つことが分かる。<br>


<a id="ベルマン方程式"></a>

### ◎ ベルマン方程式
先に見たように、行動価値関数 ![image](https://user-images.githubusercontent.com/25688193/49915291-185ded80-fed8-11e8-8eff-72bd73109142.png) に関してグリーディな方策（＝状態 s において ![image](https://user-images.githubusercontent.com/25688193/49683704-72dc0000-fb0c-11e8-841c-0c42230bcc5b.png) を最大化する行動選択）は最適であるので、最適方策 ![image](https://user-images.githubusercontent.com/25688193/49916370-e18ad600-fedd-11e8-997c-0a1ca74029e4.png) を求めるには、最適行動価値関数 ![image](https://user-images.githubusercontent.com/25688193/49915291-185ded80-fed8-11e8-8eff-72bd73109142.png) の情報、或いは、最適状態価値関数 ![image](https://user-images.githubusercontent.com/25688193/49916303-b0120a80-fedd-11e8-8590-12a538918666.png)・即時報酬 r・遷移確率 P の情報があればよいことになる。<br>

従って、次の問題は、これら最適価値関数 ![image](https://user-images.githubusercontent.com/25688193/49915291-185ded80-fed8-11e8-8eff-72bd73109142.png),![image](https://user-images.githubusercontent.com/25688193/49916303-b0120a80-fedd-11e8-8590-12a538918666.png) を如何にして求めるかということになる。<br>
最適価値関数 ![image](https://user-images.githubusercontent.com/25688193/49915291-185ded80-fed8-11e8-8eff-72bd73109142.png),![image](https://user-images.githubusercontent.com/25688193/49916303-b0120a80-fedd-11e8-8590-12a538918666.png) は、以下のベッセルの方程式と呼ばれる、最適解に関しての不動点方程式から求めることが原理的には可能となる。<br>
※ ”原理的には可能” と表現したのは、ベッセルの方程式が最適価値関数に関しての不動点方程式となっており、収束先の不動点としての最適解を持つことが保証されるが、実際上この方程式を直接解くことは困難であり、代わりに動的計画法（価値反復、方策反復など）の手法で近似解を得るようにすることが多いため。<br>

まずは、最適価値関数のうち、最適状態価値関数 ![image](https://user-images.githubusercontent.com/25688193/49916303-b0120a80-fedd-11e8-8590-12a538918666.png) に対してのベッセルの方程式を考える。<br>

![image](https://user-images.githubusercontent.com/25688193/49916450-3c243200-fede-11e8-80b9-8bd996b0f34f.png)<br>

- （証明略）先の「状態価値関数」の項目を参照<br>

<br>

![image](https://user-images.githubusercontent.com/25688193/49916974-91f9d980-fee0-11e8-9c00-89c0788105a0.png)<br>

- （証明略）先の「状態価値関数」の項目を参照<br>

<br>

同様にして、最適行動価値関数 ![image](https://user-images.githubusercontent.com/25688193/49915291-185ded80-fed8-11e8-8eff-72bd73109142.png) に対してもベッセルの方程式が成り立ち、この不動点方程式の収束解（＝不動点）としての最適行動価値関数の値の存在性が保証される。<br>

![image](https://user-images.githubusercontent.com/25688193/49917004-ad64e480-fee0-11e8-945a-b2e048961947.png)<br>

- （証明略）先の「状態価値関数と行動価値関数の関係」の項目を参照<br>

<br>

- 【参考サイト】<br>
    - [Q-learningの収束性](https://qiita.com/ashigirl966/items/573d533180df49021f28)<br>


<a id="マルコフ決定過程を解くための動的計画法"></a>

## ■ マルコフ決定過程を解くための動的計画法
先のベルマンの方程式<br>
![image](https://user-images.githubusercontent.com/25688193/49993377-d3b27f00-ffc9-11e8-805f-88e6a60730cc.png)<br>
が示す重要な性質は、「ある状態や行動の価値 ![image](https://user-images.githubusercontent.com/25688193/49739008-2c390200-fcd4-11e8-8c93-0609a67c8915.png), ![image](https://user-images.githubusercontent.com/25688193/49683704-72dc0000-fb0c-11e8-841c-0c42230bcc5b.png) は、その後の状態や行動の価値 ![image](https://user-images.githubusercontent.com/25688193/49915737-6673f080-feda-11e8-97e7-a3e6a573cb44.png), ![image](https://user-images.githubusercontent.com/25688193/50039721-0caa2c80-007a-11e9-8f8b-b1700677dc38.png) を用いて表すことが出来る。」という、ブートストラップ性の性質である。<br>
（※機械学習で用られるデータ分割手法の１つであるブートストラップ法と異なるものであることに注意）<br>

このような価値関数のブートストラップ性を利用して、価値関数を反復的に枝分かれをたどりながら逐次計算していく手法を総称して、動的計画法という。<br>
※ この動的計画法で計算可能になるためには、ベルマンの方程式に出てくる状態遷移関数 ![image](https://user-images.githubusercontent.com/25688193/50039765-09637080-007b-11e9-8694-e28e343c2bb7.png) が既知であることが条件になる。言い換えると、エージェントの環境のダイナミクスが既知であることが条件になる。<br>
※ これに対して、環境のダイナミクスが既知でなくとも有効な価値関数の計算手法として、価値推定法（モンテカルロ法による価値推定法など）がある。（詳細は、後述）<br>

ここで、このベルマンの方程式は、価値関数に対しての不動点方程式にもなっており、この不動点方程式が導く不動点としての最適価値関数の存在の一意性は、価値反復法 [value iteration] や方策反復法 [policy iteration] と呼ばれる動的計画法のアルゴリズムで、最適解が得られることの根拠となる。<br>


<a id="ベルマンの方程式の厳密な解法と反復法"></a>

### ◎ ベルマンの方程式の厳密な解法と反復法
まずは、価値反復法や方策反復法などの動的計画法による近似手法を用いず、ベルマンの方程式の式に従って、再帰的に総当たりで価値関数を求めていく方法の問題点を見てみる。<br>

![image](https://user-images.githubusercontent.com/25688193/49993433-f6449800-ffc9-11e8-803f-3682d54f91b6.png)<br>

> 記載中...

反復法は、ベルマンの方程式を満たす価値関数、及びそのときの最適行動方策を、ベルマンの方程式の式にそのままに従って再帰的に総当たり計算するのではなく、今の状態と次の状態の２層間のみの繰り返し計算により近似的に求める動的計画法の一種である。<br>
より詳細には、<br>
- 今の状態 s と次の状態 s' の２層間のみでの計算を、k=0,1,2,... で更新しながら反復する。<br>
- 最適解に近づくまで、この更新処理 k=0,1,2,... を続ける。<br>
- 得られた最適解の近似から、最適行動方針を算出する。<br>
というのが基本的なアイデアである。<br>

![image](https://user-images.githubusercontent.com/25688193/49993714-c21da700-ffca-11e8-95a9-dacfc7538a15.png)<br>

反復法には、それぞれ価値反復法と方策反復法が存在するが、両者の違いは、価値関数の更新方針の違いにある。即ち、<br>

- 価値反復法では、価値関数の更新方針は、グリーディーな方策（＝価値関数をmax化する行動：![image](https://user-images.githubusercontent.com/25688193/49993657-98648000-ffca-11e8-8048-e6e8f69202f2.png)）に基づく更新。<br>
- 方策反復法での価値関数の更新方針は、行動方策に関しての全ての分岐 ![image](https://user-images.githubusercontent.com/25688193/49993632-81259280-ffca-11e8-975b-627395dd1696.png) に基づく更新。<br>
    従って、グリーディーで max のみを選択する価値反復法に比べて、計算量は増加する。<br>

<br>

- 【参考サイト】<br>
    - [今さら聞けない強化学習（5）：状態価値関数近似と方策評価](https://qiita.com/triwave33/items/bed0fd7a2b56ee8e7c29#_reference-085df97779eb2480be0f)<br>


<a id="価値反復法"></a>

### ◎ 価値反復法 [value iteration]
![image](https://user-images.githubusercontent.com/25688193/49993471-12e0d000-ffca-11e8-9ce8-0a94bff54dfc.png)<br>
![image](https://user-images.githubusercontent.com/25688193/49993508-2d1aae00-ffca-11e8-8ca3-efc40c4d07fd.png)<br>
![image](https://user-images.githubusercontent.com/25688193/50040019-7c6ee600-007f-11e9-92cb-a278e6ef6d6d.png)<br>

- 【参考サイト】<br>
    - [強化学習について学んでみた。（その13）](http://yamaimo.hatenablog.jp/entry/2015/09/07/200000)<br>


<a id="方策反復法"></a>

### ◎ 方策反復法 [policy iteration]
方策反復法では、価値反復法とは異なり、価値関数に対してグリーディーな方策 ![image](https://user-images.githubusercontent.com/25688193/49993657-98648000-ffca-11e8-8048-e6e8f69202f2.png) に従って、更新を行うのではなく、行動方策に関しての全ての分岐 ![image](https://user-images.githubusercontent.com/25688193/49993632-81259280-ffca-11e8-975b-627395dd1696.png) の情報に従って、更新を行う。<br>
そのため、グリーディーで max のみを選択する価値反復法に比べて、精度が悪くなることはないが、１ステップあたりの計算量は、価値反復法に比べて増加する。<br>

![image](https://user-images.githubusercontent.com/25688193/49993825-23de1100-ffcb-11e8-8c40-979278c3ed03.png)<br>
![image](https://user-images.githubusercontent.com/25688193/49993867-3a846800-ffcb-11e8-9d25-f375243cda56.png)<br>
![image](https://user-images.githubusercontent.com/25688193/50040026-97d9f100-007f-11e9-845f-a3da07227850.png)<br>

- 【参考サイト】<br>
    - [強化学習について学んでみた。（その12）](http://yamaimo.hatenablog.jp/entry/2015/09/06/200000)<br>


<!--
<a id="価値推定問題"></a>

## ■ 価値推定問題
ここでは、価値推定問題と称して、先の動的計画法（価値反復法、方策反復法など）による価値関数の推定法以外の手法、具体的には、モンテカルロ法による価値推定、及び、TD学習による価値推定などを取り扱う。<br>
-->

<a id="モンテカルロ法による価値推定"></a>

## ■ モンテカルロ法による価値推定
モンテカルロ法とは、数値計算やシミュレーションなどにおいて、ランダムな乱数をサンプリングすることで数値計算（例えば、積分計算など）を行う手法の総称であるが、今考えているマルコフ決定過程における価値関数の推定問題にも応用出来る。<br>

先に見たように、動的計画法と呼ばれる価値関数のブートストラップ性を利用して、反復的に枝分かを辿りながら価値関数を計算していく手法では、ベルマンの方程式に出てくる状態遷移関数 ![image](https://user-images.githubusercontent.com/25688193/50039765-09637080-007b-11e9-8694-e28e343c2bb7.png) が既知でなくてはならないという問題があった。<br>
これに対して、モンテカルロ法による価値関数の推定法では、この状態遷移関数が既知でなくてもよいというメリットが存在する。<br>
※ 但し、収益の分散が大きい場合、モンテカルロ法による価値関数の推定は、信頼度が低くなってしまうという問題がある。<br>

![image](https://user-images.githubusercontent.com/25688193/50039786-85f64f00-007b-11e9-940c-3671484746c4.png)<br>

上図は、例えば、円の面積を求める問題において、動的計画法とモンテカルロ法による手法の違いを示した図である。<br>
この問題は、（円の中：報酬１、円の外：報酬０）という問題設定を行うことでマルコフ決定過程の枠組みでモデル化出来るが、両者の違いは、動的計画法では、円の面積 or 外部の面積で与えられる状態遷移関数を元に、円の内部か外部かの判定を行うのに対し、モンテカルロ法では、状態遷移関数で与えられる円の面積は知らなくとも、ランダムな乱数でサンプリングすることで、結果的に、状態遷移関数で与えられる円の面積に沿った結果を得ることが出来る。<br>

![image](https://user-images.githubusercontent.com/25688193/50376094-a742d680-064b-11e9-93d4-b7dd8eb03e52.png)<br>

- 【参考サイト】<br>
    - [今さら聞けない強化学習（7）：モンテカルロ法で価値推定](https://qiita.com/triwave33/items/0c8833e6b899c26b208e#_reference-523997a713762bb0a83c)<br>


<a id="TD学習（時間的差分学習）"></a>

## ■ TD学習（時間的差分学習）
先のモンテカルロ法では、ブーストラップ性を用いないため、計算量が大きく、又、収益の分散が大きい場合に、モンテカルロ法による価値関数の推定は、信頼度が低くなってしまうという問題が存在する。<br>
一方、動的計画法では、状態遷移確率の形が具体的に与えられていないと、計算が出来ないとという問題が存在する。<br>

この両者（MC法、DP法）の利点を織り込んだ最適価値関数の推定手法として、TD学習と呼ばれる手法が存在する。<br>
具体的には、このTD学習では、状態遷移確率の具体的な形を必要せず、なおかつ、ブートストラップ性を用いて、確率過程の終端まで待たずに、価値関数の更新をオンライ的に逐次行うことが出来る。<br>

まず、オンライン型の推定値の計算の一般的な議論の例として、平均値の計算の取扱いについて見てみる。
報酬 ![image](https://user-images.githubusercontent.com/25688193/50372449-d177a280-0611-11e9-8721-b0385816271b.png) の平均値計算は、以下のようにして計算できる。<br>
![image](https://user-images.githubusercontent.com/25688193/50372454-e0f6eb80-0611-11e9-8b73-1f8778b0e417.png)<br>
この平均値計算式の形式では、新しいデータが追加されるたびに、和の操作を再度実行する必要があるために、計算負荷が大きくなりやすいという問題が存在する。即ち、<br>
![image](https://user-images.githubusercontent.com/25688193/50372466-0d126c80-0612-11e9-86a9-e56599079ada.png)<br>
従って、この式を、前回の平均値と最新のデータから、新しい平均値を算出するという漸近式の表現に書き直すと、<br>
![image](https://user-images.githubusercontent.com/25688193/50372472-2c10fe80-0612-11e9-995c-dec39cf37e0f.png)<br>
即ち、平均値の計算を、漸近式の形で表現すると、以下のような形式となる。<br>

- 新しい推定値 ← 古い推定値＋ステップサイズ ✕ [最新の更新データ ー 古い推定値]<br>

この漸近式の表現では、新しいデータに対して、都度平均値計算をし直す必要はなく、オンライン型の手続きが可能となり、計算負荷が軽減されるといったメリットが存在する。<br>

<br>

次に、この漸近式の表現での価値関数の推定方法を、先の逐次訪問モンテカルロ法による価値推定の方法に適用してみることを考える。<br>
先のモンテカルロ法による価値推定の方法では、各状態 s に対する得られた収益のリスト Returns(s) を平均化したもので価値関数の推定を行っていたが、これを漸近式の形で表現すると、<br>

![image](https://user-images.githubusercontent.com/25688193/50373326-b8c2b900-0620-11e9-8263-32ab0a598a00.png)<br>

この収益 ![image](https://user-images.githubusercontent.com/25688193/50373274-aeec8600-061f-11e9-984b-a741f664ff03.png) を目標値とする漸近式の形で表現した、逐次訪問モンテカルロ法による価値推定手法を、アルファ不変MC法という。<br>
尚、このアルファ不変MC法は、漸近式の形で表現されているが、報酬の平均値である収益 ![image](https://user-images.githubusercontent.com/25688193/50373274-aeec8600-061f-11e9-984b-a741f664ff03.png) がエピソードの完了までまたないと計算できないため、漸近式のメリットであるオンライン型の手続きのメリットは享受できない。<br>

<br>

TD学習では、エピソードの完了までまたないと計算できない収益 ![image](https://user-images.githubusercontent.com/25688193/50373274-aeec8600-061f-11e9-984b-a741f664ff03.png) ではなく、次のステップでの価値関数と報酬の推定値 ![image](https://user-images.githubusercontent.com/25688193/50373355-3be40f00-0621-11e9-8fb4-fa66eb1cf1f2.png) を使用する。<br>
即ち、動的計画法でのベルマンの方程式より、![image](https://user-images.githubusercontent.com/25688193/50373394-e3f9d800-0621-11e9-9f9c-794ad7152dc6.png) の関係が成り立つが、これを枝分かれで平均化した式 ![image](https://user-images.githubusercontent.com/25688193/50373461-dd1f9500-0622-11e9-871b-fa13107b9278.png) を、先の不変MC法の式 ![image](https://user-images.githubusercontent.com/25688193/50373474-f45e8280-0622-11e9-8298-7d58f29bc1bd.png) の収益 ![image](https://user-images.githubusercontent.com/25688193/50373274-aeec8600-061f-11e9-984b-a741f664ff03.png) の項に入力した式<br>

![image](https://user-images.githubusercontent.com/25688193/50381242-4bb52f00-06c5-11e9-9d5c-b68d29ffc8c0.png)<br>

により、価値関数の推定値の更新を行う。<br>
このような直近の価値関数の推定値を用いる最も単純なTD法を、TD(0)法と呼ぶ。<br>
このTD(0)法では、不変MC法と比較すると、![image](https://user-images.githubusercontent.com/25688193/50373489-32f43d00-0623-11e9-9219-a98a40f764a4.png) を目標値として更新を行い、又、不変MC法とは異なり、エピソードの完了まで待たなくとも、次のステップ t+1 まで待つだけでよい。<br>

![image](https://user-images.githubusercontent.com/25688193/50382945-7adf9680-06ed-11e9-9483-fb2dc000de52.png)<br>

> 【Memo】<br>
> - TD法の収束性<br>
>   TD法のアルゴリズムでは、如何なる定常方策 π に対しても、ステップサイズ α が十分小さな値定数であれば、推定値 V(s) が価値関数 ![image](https://user-images.githubusercontent.com/25688193/50376139-6a2b1400-064c-11e9-8438-31d254a79957.png) へ収束されることが保証されている。<br>
>	但し、この収束性の保証は、ほとんどの場合、テーブル形式のTD法（＝テーブルTD(0)法）で適用されるものである。<br>
> - TD法とMC法の収束速度<br>
>   TD法とMC法は、両方とも正しい価値関数の値に収束するが、どちらがより速く収束するのか？という疑問に対する数学的な証明は、現時点では未解決である。<br>
>	しかしながら、例えば、ランダムウォークのような確率的なタスクにおいては、TD法のほうが、アルファ不変MCより速く収束することが知られている。（詳細略）<br>

- 【参考サイト】<br>
    - [今さら聞けない強化学習（9）: TD法の導出](https://qiita.com/triwave33/items/277210c7be4e47c28565)<br>
    - [強化学習について学んでみた。（その18）](http://yamaimo.hatenablog.jp/entry/2015/10/15/200000)


<a id="TD学習の制御問題への適用"></a>

### ◎ TD学習の制御問題への適用
> 記載中...

- 【参考サイト】<br>
    - [今さら聞けない強化学習（10）: SarsaとQ学習の違い](https://qiita.com/triwave33/items/cae48e492769852aa9f1#_reference-745dd3cb44ecaa9c9752)<br>
    - [強化学習について学んでみた。（その19）](http://yamaimo.hatenablog.jp/entry/2015/10/16/200000)


<a id="Sarsa"></a>

### ◎ Sarsa（方策オン型TD制御）
ここでは、TD法を制御問題に適用する方法について見ていく。<br>
TD法を制御問題に適用した手法には、大きく分けて、方策オン型TD制御手法と方策オフ型TD制御手法の２つが存在する。<br>
Sarsa は、このうち方策オン型TD制御手法に分類される手法の１つであり、先のTD法と ε-greedy 手法を組み合わせた手法である。<br>
より具体的には、行動価値関数の推定をTD法によって行い、エピソードの更新（＝行動選択）を ε-greedy な行動方策によって行う手法である。<br>

<br>

まず、行動選択に関する ε-greedy 手法について見ていく。<br>
行動価値関数 Q(s,a) に関してグリーディな行動方策とは、![image](https://user-images.githubusercontent.com/25688193/50381321-74d6bf00-06c7-11e9-9cc1-638686addf68.png) となるような行動 a のことであったが、このグリーディーな行動方策では、常にその時点での最適解を模索するため、局所的最適解に陥る恐れが存在する。<br>
そこで、局所的最適解を脱出して大域的最適解に移動できるように、リスクをとって、一定の確率 ε でランダムな行動をとるよな行動方策を考える。この行動方策を ε-greedy 手法という。<br>

<br>

Sarsa では、行動価値関数の推定をTD法によって行うが、TD法による行動価値関数の更新と推定は、以下のような漸近式で表現されるのであった。<br>
![image](https://user-images.githubusercontent.com/25688193/50381318-583a8700-06c7-11e9-854f-aedaef25bb78.png)<br>

記号の簡単化のため、t の添字なしで書き換えると、以下のようになる。<br>
![image](https://user-images.githubusercontent.com/25688193/50381325-86b86200-06c7-11e9-9caa-e78424eedb99.png)<br>

Sarsa では、この更新式により、推定する行動価値関数の更新を行う。<br>
※ Sarsa というアルゴリズムの名前は、上式の推定式の右辺の変数 s,a,r′,s′,a′ の頭文字を繋げたものに由来する。<br>
そして、エピソードの更新（＝行動選択）は、ε-greedy 手法に従った行動方策によって行う。<br>

以下の図は、Sarsa のバックアップ線図である。<br>

![image](https://user-images.githubusercontent.com/25688193/50384471-5ba23280-0708-11e9-896a-23c09eb02dc6.png)<br>

上図から分かるように、この Sarsa は、ε-greedy な行動方策によってエピソードの更新（＝行動選択）が行われ、それに伴って次の状態 s',a' が定まり、価値関数の更新が行われることになる。<br>
即ち、Sarsa では、実際に進む行動（＝エピソードを更新する行動）と価値関数の推定値の更新に用いる行動が同様となる。<br>
そして、このような制御手法を、方策オン型TD制御という。<br>

<br>

この Sarsa をアルゴリズムとしてまとめると、以下のようになる。<br>

![image](https://user-images.githubusercontent.com/25688193/50385696-d37a5800-071c-11e9-833f-603dd69e3016.png)<br>


<a id="Q学習"></a>

### ◎ Q学習 [Q-learning]（方策オフ型TD制御）
先に見た Sarsa は、エピソードの更新（＝行動選択）と価値関数の更新の双方が、共に同じ ε-greedy な行動方策に従って行われる方策オン型TD制御アルゴリズムであった。<br>

一方、Q学習は、エピソードの更新（＝行動選択）と価値関数の更新の双方が、必ずしも同じ行動方策になるとは限らない方策オン型TD制御アルゴリズムになっている。<br>

<br>

まず、Q学習における、価値関数の推定値の更新式は、以下のようになる。<br>
![image](https://user-images.githubusercontent.com/25688193/50383182-e9265800-06f1-11e9-8981-5b2d5b48e988.png)<br>
この式から分かるように、Sarsa とは異なり、Q学習では、価値関数の値が max となるようなグリーディーな行動選択を一律に行う。<br>
（※ Sarsa では、ε-greedy な行動方策に従って、一定のランダム性を織り込んだ行動選択であった。）<br>
一方、エピソードの更新（＝行動選択）は、Sarsa と同様にして、ε-greedy 手法に従った行動方策によって行う。<br>

<br>

以下の図は、Q学習のバックアップ線図である。<br>

![image](https://user-images.githubusercontent.com/25688193/50384320-94d9a300-0706-11e9-810b-87c9de722cce.png)<br>

上図から分かるように、（推定値の更新式に含まれる項である）![image](https://user-images.githubusercontent.com/25688193/50384493-afad1700-0708-11e9-9600-941f4df96415.png) によるグリーディーな行動選択と、ε-greedy な行動方策によるエピソードの更新（＝行動選択）とは、必ずしも一致するとは限らない。<br>
即ち、Q学習では、実際に進む行動（＝エピソードを更新する行動）と価値関数の推定値の更新に用いる行動が異なる。<br>
そして、このような制御手法を、方策オフ型TD制御という。<br>

<br>

このQ学習をアルゴリズムとしてまとめると、以下のようになる。<br>

![image](https://user-images.githubusercontent.com/25688193/50385712-2e13b400-071d-11e9-8562-1ff059985eb1.png)<br>

> 【Memo】<br>
> - SarsaとQ学習のの収束性の比較<br>
>   行動選択に関しては、SarsaとQ学習は共に ε-greedy によるランダム性をもった行動選択となるが、価値関数の推定値の更新に関しては、Sarsa が ε-greedy ランダム性が更新式に入る一方で、Q学習では、![image](https://user-images.githubusercontent.com/25688193/50384493-afad1700-0708-11e9-9600-941f4df96415.png) のようにランダム性を持たない一律のグリーディーな行動選択となる。<br>
>   従って、ランダム性がない分、Q学習のほうが Sarsa より収束が早い傾向がある。<br>


---

<a id="参考文献"></a>

## ■ 参考文献

- 強化学習
    - [amazonで詳細を見る](https://www.amazon.co.jp/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-Richard-S-Sutton/dp/4627826613?SubscriptionId=AKIAJMYP6SDQFK6N4QZA&amp&tag=cloudstudy09-22&amp&linkCode=xm2&amp&camp=2025&amp&creative=165953&amp&creativeASIN=4627826613)

- 速習 強化学習 ―基礎理論とアルゴリズム―
    - [amazonで詳細を見る](https://www.amazon.co.jp/%E9%80%9F%E7%BF%92-%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-%E2%80%95%E5%9F%BA%E7%A4%8E%E7%90%86%E8%AB%96%E3%81%A8%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0%ク%80%95-Csaba-Szepesvari/dp/4320124227)

- つくりながら学ぶ! 深層強化学習 ~PyTorchによる実践プログラミング<br>
    - [amazonで詳細を見る](https://www.amazon.co.jp/dp/4839965625/ref=asc_df_48399656252542427/?tag=jpgo-22&creative=9303&creativeASIN=4839965625&linkCode=df0&hvadid=295723231663&hvpos=1o2&hvnetw=g&hvrand=4934601497265078375&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=1009648&hvtargid=pla-527403650097&th=1&psc=1)<br>


<a id="使用コード"></a>

### ◎ 使用コード

- [GitHub/Yagami360/ReinforcementLearning_Exercises](https://github.com/Yagami360/ReinforcementLearning_Exercises)<br>