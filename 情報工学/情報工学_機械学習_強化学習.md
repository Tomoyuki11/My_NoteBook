# 強化学習 [Reinforcement Learning]
機械学習の一種である強化学習について勉強したことをまとめたノート（忘備録）です。随時追記中。<br>


## 目次 [Contents]

1. [概要](#概要)
1. [マルコフ決定過程（MDP）](#マルコフ決定過程（MDP）)
    1. [マルコフ決定過程による強化学習のモデル化](#マルコフ決定過程による強化学習のモデル化)
    1. マルコフ決定過程の例
1. [価値関数](#価値関数)
    1. [状態価値関数](#状態価値関数)
    1. [行動価値関数](#行動価値関数)
    1. [状態価値関数と行動価値関数の関係](#状態価値関数と行動価値関数の関係)
1. [ベルマン方程式](#ベルマン方程式)
1. [マルコフ決定過程を解くための動的計画法](#マルコフ決定過程を解くための動的計画法)
    1. [ベルマンの方程式の厳密な解法と反復法](#ベルマンの方程式の厳密な解法と反復法)
    1. [価値反復法 [value iteration]](#価値反復法)
    1. [方策反復法 [policy iteration]](#方策反復法)
        1. 方策勾配法
1. [価値推定問題](#価値推定問題)
    1. [モンテカルロ法による価値推定](#モンテカルロ法による価値推定)
    1. TD学習（時間的差分学習）
        1. Sarsa
        1. Q学習 [Q-learning]
1. 深層学習の構造を取り込んだアルゴリズム
    1. DQN [Deep Q-learning]
1. 補足事項
    1. [【外部リンク】機械学習](http://yagami12.hatenablog.com/entry/2017/09/17/111400)
    1. [【外部リンク】最適化問題](http://yagami12.hatenablog.com/entry/2017/09/17/101739)
    1. [【外部リンク】グラフ理論](http://yagami12.hatenablog.com/entry/2017/09/17/110406)
        1. [【補足（外部リンク）】確率過程とマルコフ連鎖](http://yagami12.hatenablog.com/entry/2017/09/17/110406#ID_3)
1. [参考文献](#参考文献)
    1. [使用コード](#使用コード)

---

<a id="概要"></a>

## ■ 概要
> 記載中...

- 【参考サイト】
    - [これから強化学習を勉強する人のための「強化学習アルゴリズム・マップ」と、実装例まとめ](https://qiita.com/sugulu/items/3c7d6cbe600d455e853b)
    - [ゼロからDeepまで学ぶ強化学習](https://qiita.com/icoxfog417/items/242439ecd1a477ece312)

---

<a id="マルコフ決定過程（MDP）"></a>

## ■ マルコフ決定過程（MDP）
強化学習は、概要でみたように、意思決定主体（＝エージェント）と外部環境が相互作用するシステムにおいて、教師なし学習の状況の下、試行錯誤を通じて、目標を示す数値（＝報酬）を最大化するようにシステムを学習制御する枠組みであるが、これをマルコフ決定過程 [MDP:Markov decision process] でモデル化する。<br>

このマルコフ決定過程は、マルコフ性とエルゴード性を持つ確率過程であって、選択可能な行動、及び、報酬を追加したものである。<br>
言い換えると、マルコフ決定過程は、マルコフ連鎖に対して、選択可能な行動、及び、報酬を追加したものである。<br>

- マルコフ性：<br>
    ある状態 ![image](https://user-images.githubusercontent.com/25688193/49641452-85463300-fa53-11e8-9596-7e8b4b082bce.png) から、別の状態 ![image](https://user-images.githubusercontent.com/25688193/49641500-a575f200-fa53-11e8-8467-eaaf3a8e866f.png) に移行する確率が、それ以前の経路 t によらず、現在の状態のみ ![image](https://user-images.githubusercontent.com/25688193/49641452-85463300-fa53-11e8-9596-7e8b4b082bce.png) で決まる性質。マルコフ連鎖は、このマルコフ性を性質を持つ。<br>

- エルゴード性（定常性）：<br>
    任意の初期状態 ![image](https://user-images.githubusercontent.com/25688193/49641553-d5bd9080-fa53-11e8-9c45-e3771976401b.png) からスタートした確率過程が、無限時間経過後には、最初の状態とは無関係になる性質。マルコフ連鎖は、このエルゴード性の性質も合わせ持つ。<br>

<br>

![image](https://user-images.githubusercontent.com/25688193/49641972-208bd800-fa55-11e8-9bce-c325cbe3b59f.png)<br>
![image](https://user-images.githubusercontent.com/25688193/49697862-cb3ffa00-fbff-11e8-9cab-5a50d0a77470.png)<br>
![image](https://user-images.githubusercontent.com/25688193/49697644-62578280-fbfd-11e8-82b7-2c931ca73fcc.png)<br>

> 【Memo】<br>
> 「速習 強化学習」の本に書いてあるマルコフ決定過程の定義、状態集合、行動集合、遷移確率カーネルの３つの構造の組から定義されてるのに対し、他の文献では、状態集合、行動集合、遷移確率、報酬関数４つの組から定義されてものが多いが、３つの構造の１つの遷移確率カーネルが、状態遷移と報酬関数を与えるから、報酬関数を新たに３つの組とは別に追加しなくても well-defined となる。<br>

- 【参考サイト】<br>
    - [マルコフ決定過程 - Wikipedia](https://ja.wikipedia.org/wiki/%E3%83%9E%E3%83%AB%E3%82%B3%E3%83%95%E6%B1%BA%E5%AE%9A%E9%81%8E%E7%A8%8B)<br>
    - [第３章： 強化学習の基礎理論](http://sysplan.nams.kyushu-u.ac.jp/gen/edu/RL_intro.html#Foundations)<br>


<a id="マルコフ決定過程による強化学習のモデル化"></a>

### ◎ マルコフ決定過程による強化学習のモデル化
次に、このように定義したマルコフ決定過程において、システム（環境）とエージェントは、以下のような相互作用を行うものとして、強化学習をモデル化する。<br>

- （状態と報酬の遷移）<br>
    時間 t における状態 ![image](https://user-images.githubusercontent.com/25688193/49642152-94c67b80-fa55-11e8-9df4-e1277abc3fdf.png) にあるシステムにおいて、エージェントの行動 ![image](https://user-images.githubusercontent.com/25688193/49642191-b0ca1d00-fa55-11e8-9110-0b7f8f4f82b7.png) が実行されると、システムはそれに応じた状態遷移を行うが、このとき、以下のような関係が成り立つものとする。<br>
    ![image](https://user-images.githubusercontent.com/25688193/49642225-cb03fb00-fa55-11e8-925a-3bc9bb8becbe.png)<br>
	即ち、次の状態と報酬の組 ![image](https://user-images.githubusercontent.com/25688193/49642251-da834400-fa55-11e8-8f56-1933d89913cb.png) は、状態と報酬をセットにした遷移確率である遷移確率カーネル ![image](https://user-images.githubusercontent.com/25688193/49642273-ea9b2380-fa55-11e8-86f9-0ce942fdae87.png) の確率分布に従う。<br>
    言い換えると、遷移確率カーネル ![image](https://user-images.githubusercontent.com/25688193/49642273-ea9b2380-fa55-11e8-86f9-0ce942fdae87.png) の確率に従って、次の状態 ![image](https://user-images.githubusercontent.com/25688193/49642302-ff77b700-fa55-11e8-9570-8096cb583d34.png) と報酬 ![image](https://user-images.githubusercontent.com/25688193/49642320-0dc5d300-fa56-11e8-8399-73c5b4c44ed4.png) が定まる。<br>

- （定常性）<br>
    遷移後の次の状態 ![image](https://user-images.githubusercontent.com/25688193/49642302-ff77b700-fa55-11e8-9570-8096cb583d34.png) は、現在の時間 t の状態 ![image](https://user-images.githubusercontent.com/25688193/49642152-94c67b80-fa55-11e8-9df4-e1277abc3fdf.png) と行動 ![image](https://user-images.githubusercontent.com/25688193/49642191-b0ca1d00-fa55-11e8-9110-0b7f8f4f82b7.png) のみに依存し、それ以前の時間 t-1,t-2,... での状態や行動によらない。<br>
    即ち、<br>
    ![image](https://user-images.githubusercontent.com/25688193/49642453-6dbc7980-fa56-11e8-8c29-c852c4c30266.png)<br>
    の関係が成り立つものとする。<br>

- （行動則、政策）<br>
    エージェントは、任意の時間 t での状態と報酬の観測の履歴 ![image](https://user-images.githubusercontent.com/25688193/49642500-9775a080-fa56-11e8-83fc-1978a4876ca6.png) の情報を元に行動することが出来る。<br>
    このような履歴情報を元にして、エージェントが行動を選択するルールを行動則（政策）π といい、この行動則 π は、確率で表現される（＝確率的に決定される）ものとなる。<br>

- （行動則により定まる確率過程）<br>
    そして、エージェントの行動則 π と初期状態 ![image](https://user-images.githubusercontent.com/25688193/49641553-d5bd9080-fa53-11e8-9c45-e3771976401b.png) から、確率的に決定される状態・行動・報酬の組の過程、即ち、確率過程 ![image](https://user-images.githubusercontent.com/25688193/49697224-10602e00-fbf8-11e8-8d5c-9eb374f3866f.png) が定まる。<br>


エージェントの目的は、これらの相互作用の過程を繰り返しながら、自身の行動則 π により目的関数である割引総和（＝収益）<br>
![image](https://user-images.githubusercontent.com/25688193/49642631-ffc48200-fa56-11e8-9021-8bf4253d6b89.png)<br>
の期待値を最大化することである。即ち、<br>
![image](https://user-images.githubusercontent.com/25688193/49642659-0b17ad80-fa57-11e8-8b2b-0c50d25a037d.png)<br>

以上のようなマルコフ決定過程による強化学習のモデル化を、行動則による確率的な行動選択とともに、状態遷移図で図示すると以下の図のようになる。<br>

![image](https://user-images.githubusercontent.com/25688193/49737885-4a513300-fcd1-11e8-961b-7e79abd0ebe4.png)<br>

ここで、更に、状態遷移確率による状態選択を含める描くと、以下の図のようになる。<br>

![image](https://user-images.githubusercontent.com/25688193/49738112-e54a0d00-fcd1-11e8-9998-736f8b5dc43e.png)<br>


<a id="価値関数"></a>

## ■ 価値関数
> 記載中...

- 【参照サイト】<br>
    - [今さら聞けない強化学習（1）：状態価値関数とBellman方程式 - Qiita](https://qiita.com/triwave33/items/5e13e03d4d76b71bc802)<br>
    - [今さら聞けない強化学習（3）：行動価値関数とBellman方程式 - Qiita](https://qiita.com/triwave33/items/8966890701169f8cad47)<br>
    - [人工知能概論 第6回 多段決定(2) 強化学習.](https://slidesplayer.net/slide/11477412/)<br>


<a id="状態価値関数"></a>

### ◎ 状態価値関数
先に述べたように、強化学習をマルコフ決定過程でモデル化するにあたって、エージェントの目的は、自身の行動則 π により目的関数である割引総和（＝収益）<br>
![image](https://user-images.githubusercontent.com/25688193/49642631-ffc48200-fa56-11e8-9021-8bf4253d6b89.png)<br>
の期待値を最大化することであったが、これを定常方策（＝時間に対して不変な行動則）での収益に限定された状態価値関数の概念を用いて、モデル化することを考える。<br>

![image](https://user-images.githubusercontent.com/25688193/49914512-de3f1c80-fed4-11e8-9dfc-706e81dba655.png)<br>

![image](https://user-images.githubusercontent.com/25688193/49742237-71acfd80-fcdb-11e8-934b-c5749312da03.png)<br>

ここで、この状態価値関数 ![image](https://user-images.githubusercontent.com/25688193/49739008-2c390200-fcd4-11e8-8c93-0609a67c8915.png) を先の状態遷移図に合わせて記載すると、上図のようになる。<br>
この図より、状態価値関数の定義にある将来に対する割引総和（＝収益、利得）の期待値 ![image](https://user-images.githubusercontent.com/25688193/49739267-c9943600-fcd4-11e8-9f8a-88ee2de9e046.png) は、緑枠内での報酬 r の平均をとったものになっていることが分かる。<br>


更に、この将来の割引総和に関しての期待値は、上図から分かるように、赤枠部分 ![image](https://user-images.githubusercontent.com/25688193/49739452-44f5e780-fcd5-11e8-935e-dfacc1315420.png) と青枠部分 ![image](https://user-images.githubusercontent.com/25688193/49739483-550dc700-fcd5-11e8-8d70-754fbf5e38d8.png) で各々平均値をとったものに対応しているので、以下の関係が成り立つことが分かる。<br>
![image](https://user-images.githubusercontent.com/25688193/49739922-63101780-fcd6-11e8-88bf-7e291f8e696c.png)<br>
尚、この関係式を上図から直感的にではなく、式変形で示すと以下のようになる。<br>
![image](https://user-images.githubusercontent.com/25688193/49739957-74f1ba80-fcd6-11e8-894a-bd5efa7164ca.png)<br>

ここで、この関係式は、状態価値関数に関しての方程式になっており、（状態価値関数に対しての）ベルマンの方程式という。<br>
そして、このベルマンの方程式は不動点方程式であるので、この方程式を解けば、原理的には不動点としての状態価値関数の最適解が一意に存在することになる（詳細は後述）<br>


<a id="行動価値関数"></a>

### ◎ 行動価値関数
マルコフ決定過程において、価値関数の一種として、定常方策 ![image](https://user-images.githubusercontent.com/25688193/49683687-38726300-fb0c-11e8-8de2-b79447dcff37.png) の元での、状態 ![image](https://user-images.githubusercontent.com/25688193/49683696-4a540600-fb0c-11e8-984c-e6f2eccdeb94.png) にいて行動 ![image](https://user-images.githubusercontent.com/25688193/49683701-62c42080-fb0c-11e8-8690-62119c18f03d.png) を選択する価値を表わす、行動価値関数 ![image](https://user-images.githubusercontent.com/25688193/49683704-72dc0000-fb0c-11e8-841c-0c42230bcc5b.png) なるものを考えることも出来る。<br>

![image](https://user-images.githubusercontent.com/25688193/49914603-4d1c7580-fed5-11e8-810b-5063974a4d23.png)<br>
![image](https://user-images.githubusercontent.com/25688193/49915116-56a6dd00-fed7-11e8-838b-4866ed23e442.png)<br>

ここで、この状態行動関数 ![image](https://user-images.githubusercontent.com/25688193/49683704-72dc0000-fb0c-11e8-841c-0c42230bcc5b.png) を先の状態遷移図に合わせて記載すると、上図のようになる。<br>
この図より、状態行動関数の定義にある割引総和（＝収益、利得）の期待値 ![image](https://user-images.githubusercontent.com/25688193/49915218-bb623780-fed7-11e8-8daf-fe4499115302.png) は、オレンジ色内での報酬 r の平均をとったものになっていることが分かる。<br>

一般に、状態行動関数 ![image](https://user-images.githubusercontent.com/25688193/49683704-72dc0000-fb0c-11e8-841c-0c42230bcc5b.png) が与えられたときに、この状態行動関数の価値を最大化する行動、即ち、![image](https://user-images.githubusercontent.com/25688193/49915250-e6e52200-fed7-11e8-9675-de40d57af045.png) とするような行動選択は、「状態 s において、行動価値関数 Q に関して greedy（貪欲）な行動である」という。<br>
更に、”任意の” 状態において、Qに関してのグリーディな行動のみを選択する行動方策は、「行動価値関数 Q に関してグリーディな方策である」という。<br>

従って、達成可能な行動価値の最大値である最適行動価値関数 ![image](https://user-images.githubusercontent.com/25688193/49915291-185ded80-fed8-11e8-8eff-72bd73109142.png) においては、![image](https://user-images.githubusercontent.com/25688193/49915291-185ded80-fed8-11e8-8eff-72bd73109142.png) に関してのグリーディな方策は最適であることが分かる。<br>


<a id="状態価値関数と行動価値関数の関係"></a>

### ◎ 状態価値関数と行動価値関数の関係
価値関数には、状態価値関数と行動価値関数の２つが存在するが、これら２つの関数は、以下の式によって、互いに結びつく。<br>

![image](https://user-images.githubusercontent.com/25688193/49915540-87881180-fed9-11e8-938c-68be355dde58.png)<br>

![image](https://user-images.githubusercontent.com/25688193/49915568-a5557680-fed9-11e8-9202-92a1cf3f8286.png)<br>
→ 将来の期待利得である状態価値関数 ![image](https://user-images.githubusercontent.com/25688193/49739008-2c390200-fcd4-11e8-8c93-0609a67c8915.png) は、（現在の期待利得である）行動価値関数 ![image](https://user-images.githubusercontent.com/25688193/49683704-72dc0000-fb0c-11e8-841c-0c42230bcc5b.png) に対して、行動方策に関しての分岐（赤枠部分）で和をとった形で表現できる。<br>

![image](https://user-images.githubusercontent.com/25688193/49915586-b605ec80-fed9-11e8-95fc-960c097e7eea.png)<br>
→ 現在の行動価値関数 ![image](https://user-images.githubusercontent.com/25688193/49683704-72dc0000-fb0c-11e8-841c-0c42230bcc5b.png) の値は、現在の状態 s で行動 a をとって状態 s′ に遷移するときの報酬 r(s,a,s′) に対して、確率分布 ![image](https://user-images.githubusercontent.com/25688193/50039765-09637080-007b-11e9-8694-e28e343c2bb7.png) で表現される遷移後の状態の分岐（青枠部分）に関しての和をとって平均化したもの（＝R）に、状態遷移後の状態価値関数 ![image](https://user-images.githubusercontent.com/25688193/49915737-6673f080-feda-11e8-97e7-a3e6a573cb44.png) の値を加えたものとなる。<br>
尚、状態遷移後の s′ 以降の価値は、状態価値関数 ![image](https://user-images.githubusercontent.com/25688193/49915737-6673f080-feda-11e8-97e7-a3e6a573cb44.png) に含まれるので、これ以降の分岐の和をとる必要はない。<br>


以上の関係式をまとめると、以下のようになる。<br>

![image](https://user-images.githubusercontent.com/25688193/49915748-7e4b7480-feda-11e8-870b-10c0dfb074c3.png)<br>

- （証明略）解釈図より成り立つことが分かる。<br>


<a id="ベルマン方程式"></a>

## ■ ベルマン方程式
先に見たように、行動価値関数 ![image](https://user-images.githubusercontent.com/25688193/49915291-185ded80-fed8-11e8-8eff-72bd73109142.png) に関してグリーディな方策（＝状態 s において ![image](https://user-images.githubusercontent.com/25688193/49683704-72dc0000-fb0c-11e8-841c-0c42230bcc5b.png) を最大化する行動選択）は最適であるので、最適方策 ![image](https://user-images.githubusercontent.com/25688193/49916370-e18ad600-fedd-11e8-997c-0a1ca74029e4.png) を求めるには、最適行動価値関数 ![image](https://user-images.githubusercontent.com/25688193/49915291-185ded80-fed8-11e8-8eff-72bd73109142.png) の情報、或いは、最適状態価値関数 ![image](https://user-images.githubusercontent.com/25688193/49916303-b0120a80-fedd-11e8-8590-12a538918666.png)・即時報酬 r・遷移確率 P の情報があればよいことになる。<br>

従って、次の問題は、これら最適価値関数 ![image](https://user-images.githubusercontent.com/25688193/49915291-185ded80-fed8-11e8-8eff-72bd73109142.png),![image](https://user-images.githubusercontent.com/25688193/49916303-b0120a80-fedd-11e8-8590-12a538918666.png) を如何にして求めるかということになる。<br>
最適価値関数 ![image](https://user-images.githubusercontent.com/25688193/49915291-185ded80-fed8-11e8-8eff-72bd73109142.png),![image](https://user-images.githubusercontent.com/25688193/49916303-b0120a80-fedd-11e8-8590-12a538918666.png) は、以下のベッセルの方程式と呼ばれる、最適解に関しての不動点方程式から求めることが原理的には可能となる。<br>
※ ”原理的には可能” と表現したのは、ベッセルの方程式が最適価値関数に関しての不動点方程式となっており、収束先の不動点としての最適解を持つことが保証されるが、実際上この方程式を直接解くことは困難であり、代わりに動的計画法（価値反復、方策反復など）の手法で近似解を得るようにすることが多いため。<br>

まずは、最適価値関数のうち、最適状態価値関数 ![image](https://user-images.githubusercontent.com/25688193/49916303-b0120a80-fedd-11e8-8590-12a538918666.png) に対してのベッセルの方程式を考える。<br>

![image](https://user-images.githubusercontent.com/25688193/49916450-3c243200-fede-11e8-80b9-8bd996b0f34f.png)<br>

- （証明略）先の「状態価値関数」の項目を参照<br>

<br>

![image](https://user-images.githubusercontent.com/25688193/49916974-91f9d980-fee0-11e8-9c00-89c0788105a0.png)<br>

- （証明略）先の「状態価値関数」の項目を参照<br>

<br>

同様にして、最適行動価値関数 ![image](https://user-images.githubusercontent.com/25688193/49915291-185ded80-fed8-11e8-8eff-72bd73109142.png) に対してもベッセルの方程式が成り立ち、この不動点方程式の収束解（＝不動点）としての最適行動価値関数の値の存在性が保証される。<br>

![image](https://user-images.githubusercontent.com/25688193/49917004-ad64e480-fee0-11e8-945a-b2e048961947.png)<br>

- （証明略）先の「状態価値関数と行動価値関数の関係」の項目を参照<br>

<br>

- 【参考サイト】<br>
    - [Q-learningの収束性](https://qiita.com/ashigirl966/items/573d533180df49021f28)<br>


<a id="マルコフ決定過程を解くための動的計画法"></a>

## ■ マルコフ決定過程を解くための動的計画法
先のベルマンの方程式<br>
![image](https://user-images.githubusercontent.com/25688193/49993377-d3b27f00-ffc9-11e8-805f-88e6a60730cc.png)<br>
が示す重要な性質は、「ある状態や行動の価値 ![image](https://user-images.githubusercontent.com/25688193/49739008-2c390200-fcd4-11e8-8c93-0609a67c8915.png), ![image](https://user-images.githubusercontent.com/25688193/49683704-72dc0000-fb0c-11e8-841c-0c42230bcc5b.png) は、その後の状態や行動の価値 ![image](https://user-images.githubusercontent.com/25688193/49915737-6673f080-feda-11e8-97e7-a3e6a573cb44.png), ![image](https://user-images.githubusercontent.com/25688193/50039721-0caa2c80-007a-11e9-8f8b-b1700677dc38.png) を用いて表すことが出来る。」という、ブートストラップ性の性質である。<br>
（※機械学習で用られるデータ分割手法の１つであるブートストラップ法と異なるものであることに注意）<br>

このような価値関数のブートストラップ性を利用して、価値関数を反復的に枝分かれをたどりながら逐次計算していく手法を総称して、動的計画法という。<br>
※ この動的計画法で計算可能になるためには、ベルマンの方程式に出てくる状態遷移関数 ![image](https://user-images.githubusercontent.com/25688193/50039765-09637080-007b-11e9-8694-e28e343c2bb7.png) が既知であることが条件になる。言い換えると、エージェントの環境のダイナミクスが既知であることが条件になる。<br>
※ これに対して、環境のダイナミクスが既知でなくとも有効な価値関数の計算手法として、価値推定法（モンテカルロ法による価値推定法など）がある。（詳細は、後述）<br>

ここで、このベルマンの方程式は、価値関数に対しての不動点方程式にもなっており、この不動点方程式が導く不動点としての最適価値関数の存在の一意性は、価値反復法 [value iteration] や方策反復法 [policy iteration] と呼ばれる動的計画法のアルゴリズムで、最適解が得られることの根拠となる。<br>


<a id="ベルマンの方程式の厳密な解法と反復法"></a>

### ◎ ベルマンの方程式の厳密な解法と反復法
まずは、価値反復法や方策反復法などの動的計画法による近似手法を用いず、ベルマンの方程式の式に従って、再帰的に総当たりで価値関数を求めていく方法の問題点を見てみる。<br>

![image](https://user-images.githubusercontent.com/25688193/49993433-f6449800-ffc9-11e8-803f-3682d54f91b6.png)<br>

> 記載中...

反復法は、ベルマンの方程式を満たす価値関数、及びそのときの最適行動方策を、ベルマンの方程式の式にそのままに従って再帰的に総当たり計算するのではなく、今の状態と次の状態の２層間のみの繰り返し計算により近似的に求める動的計画法の一種である。<br>
より詳細には、<br>
- 今の状態 s と次の状態 s' の２層間のみでの計算を、k=0,1,2,... で更新しながら反復する。<br>
- 最適解に近づくまで、この更新処理 k=0,1,2,... を続ける。<br>
- 得られた最適解の近似から、最適行動方針を算出する。<br>
というのが基本的なアイデアである。<br>

![image](https://user-images.githubusercontent.com/25688193/49993714-c21da700-ffca-11e8-95a9-dacfc7538a15.png)<br>

反復法には、それぞれ価値反復法と方策反復法が存在するが、両者の違いは、価値関数の更新方針の違いにある。即ち、<br>

- 価値反復法では、価値関数の更新方針は、グリーディーな方策（＝価値関数をmax化する行動：![image](https://user-images.githubusercontent.com/25688193/49993657-98648000-ffca-11e8-8048-e6e8f69202f2.png)）に基づく更新。<br>
- 方策反復法での価値関数の更新方針は、行動方策に関しての全ての分岐 ![image](https://user-images.githubusercontent.com/25688193/49993632-81259280-ffca-11e8-975b-627395dd1696.png) に基づく更新。<br>
    従って、グリーディーで max のみを選択する価値反復法に比べて、計算量は増加する。<br>

<br>

- 【参考サイト】<br>
    - [今さら聞けない強化学習（5）：状態価値関数近似と方策評価](https://qiita.com/triwave33/items/bed0fd7a2b56ee8e7c29#_reference-085df97779eb2480be0f)<br>


<a id="価値反復法"></a>

### ◎ 価値反復法 [value iteration]
![image](https://user-images.githubusercontent.com/25688193/49993471-12e0d000-ffca-11e8-9ce8-0a94bff54dfc.png)<br>
![image](https://user-images.githubusercontent.com/25688193/49993508-2d1aae00-ffca-11e8-8ca3-efc40c4d07fd.png)<br>
![image](https://user-images.githubusercontent.com/25688193/50040019-7c6ee600-007f-11e9-92cb-a278e6ef6d6d.png)<br>

- 【参考サイト】<br>
    - [強化学習について学んでみた。（その13）](http://yamaimo.hatenablog.jp/entry/2015/09/07/200000)<br>


<a id="方策反復法"></a>

### ◎ 方策反復法 [policy iteration]
方策反復法では、価値反復法とは異なり、価値関数に対してグリーディーな方策 ![image](https://user-images.githubusercontent.com/25688193/49993657-98648000-ffca-11e8-8048-e6e8f69202f2.png) に従って、更新を行うのではなく、行動方策に関しての全ての分岐 ![image](https://user-images.githubusercontent.com/25688193/49993632-81259280-ffca-11e8-975b-627395dd1696.png) の情報に従って、更新を行う。<br>
そのため、グリーディーで max のみを選択する価値反復法に比べて、精度が悪くなることはないが、１ステップあたりの計算量は、価値反復法に比べて増加する。<br>

![image](https://user-images.githubusercontent.com/25688193/49993825-23de1100-ffcb-11e8-8c40-979278c3ed03.png)<br>
![image](https://user-images.githubusercontent.com/25688193/49993867-3a846800-ffcb-11e8-9d25-f375243cda56.png)<br>
![image](https://user-images.githubusercontent.com/25688193/50040026-97d9f100-007f-11e9-845f-a3da07227850.png)<br>

- 【参考サイト】<br>
    - [強化学習について学んでみた。（その12）](http://yamaimo.hatenablog.jp/entry/2015/09/06/200000)<br>


<a id="価値推定問題"></a>

## ■ 価値推定問題
ここでは、価値推定問題と称して、先の動的計画法（価値反復法、方策反復法など）による価値関数の推定法以外の手法、具体的には、モンテカルロ法による価値推定、及び、TD学習による価値推定などを取り扱う。<br>


<a id="モンテカルロ法による価値推定"></a>

### ◎ モンテカルロ法による価値推定
モンテカルロ法とは、数値計算やシミュレーションなどにおいて、ランダムな乱数をサンプリングすることで数値計算（例えば、積分計算など）を行う手法の総称であるが、今考えているマルコフ決定過程における価値関数の推定問題にも応用出来る。<br>

先に見たように、動的計画法と呼ばれる価値関数のブートストラップ性を利用して、反復的に枝分かを辿りながら価値関数を計算していく手法では、ベルマンの方程式に出てくる状態遷移関数 ![image](https://user-images.githubusercontent.com/25688193/50039765-09637080-007b-11e9-8694-e28e343c2bb7.png) が既知でなくてはならないという問題があった。<br>
これに対して、モンテカルロ法による価値関数の推定法では、この状態遷移関数が既知でなくてもよいというメリットが存在する。<br>
※ 但し、収益の分散が大きい場合、モンテカルロ法による価値関数の推定は、信頼度が低くなってしまうという問題がある。<br>

![image](https://user-images.githubusercontent.com/25688193/50039786-85f64f00-007b-11e9-940c-3671484746c4.png)<br>

上図は、例えば、円の面積を求める問題において、動的計画法とモンテカルロ法による手法の違いを示した図である。<br>
この問題は、（円の中：報酬１、円の外：報酬０）という問題設定を行うことでマルコフ決定過程の枠組みでモデル化出来るが、両者の違いは、動的計画法では、円の面積 or 外部の面積で与えられる状態遷移関数を元に、円の内部か外部かの判定を行うのに対し、モンテカルロ法では、状態遷移関数で与えられる円の面積は知らなくとも、ランダムな乱数でサンプリングすることで、結果的に、状態遷移関数で与えられる円の面積に沿った結果を得ることが出来る。<br>

![image](https://user-images.githubusercontent.com/25688193/50040177-e6888a80-0081-11e9-87e5-e2411855d07e.png)<br>
![image](https://user-images.githubusercontent.com/25688193/50040044-b93add00-007f-11e9-982f-43d9ded63f7b.png)<br>
![image](https://user-images.githubusercontent.com/25688193/50040103-5bf35b80-0080-11e9-8682-1a33850fcaf8.png)<br>

- 【参考サイト】<br>
    - [今さら聞けない強化学習（7）：モンテカルロ法で価値推定](https://qiita.com/triwave33/items/0c8833e6b899c26b208e#_reference-523997a713762bb0a83c)<br>


---

<a id="参考文献"></a>

## ■ 参考文献

- 速習 強化学習 ―基礎理論とアルゴリズム―
    - [amazonで詳細を見る](https://www.amazon.co.jp/%E9%80%9F%E7%BF%92-%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-%E2%80%95%E5%9F%BA%E7%A4%8E%E7%90%86%E8%AB%96%E3%81%A8%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0%ク%80%95-Csaba-Szepesvari/dp/4320124227)

- つくりながら学ぶ! 深層強化学習 ~PyTorchによる実践プログラミング<br>
    - [amazonで詳細を見る](https://www.amazon.co.jp/dp/4839965625/ref=asc_df_48399656252542427/?tag=jpgo-22&creative=9303&creativeASIN=4839965625&linkCode=df0&hvadid=295723231663&hvpos=1o2&hvnetw=g&hvrand=4934601497265078375&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=1009648&hvtargid=pla-527403650097&th=1&psc=1)<br>


<a id="使用コード"></a>

### ◎ 使用コード
> 実装中...
